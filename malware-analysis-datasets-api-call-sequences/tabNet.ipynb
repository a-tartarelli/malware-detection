{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>071e8c3f8922e186e57548cd4c703a5d</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33f8e6d08a6aae939f25a8e0d63dd523</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b68abd064e975e1c6d5f25e748663076</td>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72049be7bd30ea61297ea624ae198067</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>228</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c9b3700a77facf29172f32df6bc77f48</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  \\\n",
       "0  071e8c3f8922e186e57548cd4c703a5d  112  274  158  215  274  158  215  298   \n",
       "1  33f8e6d08a6aae939f25a8e0d63dd523   82  208  187  208  172  117  172  117   \n",
       "2  b68abd064e975e1c6d5f25e748663076   16  110  240  117  240  117  240  117   \n",
       "3  72049be7bd30ea61297ea624ae198067   82  208  187  208  172  117  172  117   \n",
       "4  c9b3700a77facf29172f32df6bc77f48   82  240  117  240  117  240  117  240   \n",
       "\n",
       "   t_8  ...  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  malware  \n",
       "0   76  ...    71   297   135   171   215    35   208    56    71        1  \n",
       "1  172  ...    81   240   117    71   297   135   171   215    35        1  \n",
       "2  240  ...    65   112   123    65   112   123    65   113   112        1  \n",
       "3  172  ...   208   302   208   302   187   208   302   228   302        1  \n",
       "4  117  ...   209   260    40   209   260   141   260   141   260        1  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_csv('../dataset/malwareAnalysisDatasetsApiCallSequences.csv')\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for each column\n",
    "features = features.dropna(axis=0)\n",
    "features = features.drop('hash', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are the values we want to predict\n",
    "labels = features['malware']\n",
    "\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features = features.drop('malware', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create a scaler object\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "features_std = std_scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.26843 |  0:00:01s\n",
      "epoch 1  | loss: 0.10069 |  0:00:02s\n",
      "epoch 2  | loss: 0.08823 |  0:00:04s\n",
      "epoch 3  | loss: 0.08499 |  0:00:05s\n",
      "epoch 4  | loss: 0.08633 |  0:00:06s\n",
      "epoch 5  | loss: 0.08312 |  0:00:07s\n",
      "epoch 6  | loss: 0.08217 |  0:00:09s\n",
      "epoch 7  | loss: 0.08481 |  0:00:10s\n",
      "epoch 8  | loss: 0.08122 |  0:00:11s\n",
      "epoch 9  | loss: 0.07788 |  0:00:12s\n",
      "epoch 10 | loss: 0.07497 |  0:00:14s\n",
      "epoch 11 | loss: 0.07591 |  0:00:15s\n",
      "epoch 12 | loss: 0.07518 |  0:00:16s\n",
      "epoch 13 | loss: 0.07628 |  0:00:18s\n",
      "epoch 14 | loss: 0.07406 |  0:00:19s\n",
      "epoch 15 | loss: 0.0697  |  0:00:20s\n",
      "epoch 16 | loss: 0.07016 |  0:00:21s\n",
      "epoch 17 | loss: 0.06842 |  0:00:23s\n",
      "epoch 18 | loss: 0.07073 |  0:00:24s\n",
      "epoch 19 | loss: 0.06734 |  0:00:25s\n",
      "epoch 20 | loss: 0.06594 |  0:00:26s\n",
      "epoch 21 | loss: 0.06366 |  0:00:28s\n",
      "epoch 22 | loss: 0.05941 |  0:00:29s\n",
      "epoch 23 | loss: 0.05932 |  0:00:30s\n",
      "epoch 24 | loss: 0.05706 |  0:00:31s\n",
      "epoch 25 | loss: 0.05631 |  0:00:32s\n",
      "epoch 26 | loss: 0.05478 |  0:00:34s\n",
      "epoch 27 | loss: 0.05425 |  0:00:35s\n",
      "epoch 28 | loss: 0.05844 |  0:00:36s\n",
      "epoch 29 | loss: 0.06006 |  0:00:37s\n",
      "epoch 30 | loss: 0.05702 |  0:00:39s\n",
      "epoch 31 | loss: 0.05833 |  0:00:40s\n",
      "epoch 32 | loss: 0.05515 |  0:00:41s\n",
      "epoch 33 | loss: 0.0522  |  0:00:43s\n",
      "epoch 34 | loss: 0.05286 |  0:00:44s\n",
      "epoch 35 | loss: 0.05019 |  0:00:46s\n",
      "epoch 36 | loss: 0.05096 |  0:00:48s\n",
      "epoch 37 | loss: 0.04777 |  0:00:49s\n",
      "epoch 38 | loss: 0.05121 |  0:00:51s\n",
      "epoch 39 | loss: 0.05767 |  0:00:52s\n",
      "epoch 40 | loss: 0.05694 |  0:00:54s\n",
      "epoch 41 | loss: 0.05952 |  0:00:56s\n",
      "epoch 42 | loss: 0.05185 |  0:00:58s\n",
      "epoch 43 | loss: 0.0488  |  0:00:59s\n",
      "epoch 44 | loss: 0.04738 |  0:01:00s\n",
      "epoch 45 | loss: 0.04515 |  0:01:02s\n",
      "epoch 46 | loss: 0.04142 |  0:01:04s\n",
      "epoch 47 | loss: 0.04034 |  0:01:05s\n",
      "epoch 48 | loss: 0.04439 |  0:01:07s\n",
      "epoch 49 | loss: 0.04191 |  0:01:09s\n",
      "epoch 50 | loss: 0.04461 |  0:01:11s\n",
      "epoch 51 | loss: 0.03831 |  0:01:12s\n",
      "epoch 52 | loss: 0.03785 |  0:01:13s\n",
      "epoch 53 | loss: 0.0345  |  0:01:15s\n",
      "epoch 54 | loss: 0.03351 |  0:01:16s\n",
      "epoch 55 | loss: 0.03406 |  0:01:18s\n",
      "epoch 56 | loss: 0.03254 |  0:01:19s\n",
      "epoch 57 | loss: 0.03318 |  0:01:20s\n",
      "epoch 58 | loss: 0.0317  |  0:01:22s\n",
      "epoch 59 | loss: 0.03075 |  0:01:23s\n",
      "epoch 60 | loss: 0.03098 |  0:01:24s\n",
      "epoch 61 | loss: 0.03069 |  0:01:26s\n",
      "epoch 62 | loss: 0.02999 |  0:01:27s\n",
      "epoch 63 | loss: 0.02884 |  0:01:28s\n",
      "epoch 64 | loss: 0.03087 |  0:01:29s\n",
      "epoch 65 | loss: 0.03808 |  0:01:31s\n",
      "epoch 66 | loss: 0.03145 |  0:01:32s\n",
      "epoch 67 | loss: 0.03798 |  0:01:33s\n",
      "epoch 68 | loss: 0.03612 |  0:01:34s\n",
      "epoch 69 | loss: 0.03253 |  0:01:36s\n",
      "epoch 70 | loss: 0.0339  |  0:01:37s\n",
      "epoch 71 | loss: 0.03614 |  0:01:38s\n",
      "epoch 72 | loss: 0.03594 |  0:01:39s\n",
      "epoch 73 | loss: 0.0372  |  0:01:41s\n",
      "epoch 74 | loss: 0.04558 |  0:01:42s\n",
      "epoch 75 | loss: 0.03897 |  0:01:43s\n",
      "epoch 76 | loss: 0.03601 |  0:01:44s\n",
      "epoch 77 | loss: 0.03246 |  0:01:46s\n",
      "epoch 78 | loss: 0.03543 |  0:01:47s\n",
      "epoch 79 | loss: 0.04496 |  0:01:48s\n",
      "epoch 80 | loss: 0.03845 |  0:01:50s\n",
      "epoch 81 | loss: 0.03214 |  0:01:51s\n",
      "epoch 82 | loss: 0.03107 |  0:01:52s\n",
      "epoch 83 | loss: 0.02869 |  0:01:53s\n",
      "epoch 84 | loss: 0.02829 |  0:01:55s\n",
      "epoch 85 | loss: 0.02855 |  0:01:56s\n",
      "epoch 86 | loss: 0.03309 |  0:01:57s\n",
      "epoch 87 | loss: 0.03109 |  0:01:59s\n",
      "epoch 88 | loss: 0.0277  |  0:02:00s\n",
      "epoch 89 | loss: 0.02907 |  0:02:01s\n",
      "epoch 90 | loss: 0.02712 |  0:02:02s\n",
      "epoch 91 | loss: 0.03062 |  0:02:04s\n",
      "epoch 92 | loss: 0.0334  |  0:02:05s\n",
      "epoch 93 | loss: 0.02805 |  0:02:06s\n",
      "epoch 94 | loss: 0.02989 |  0:02:08s\n",
      "epoch 95 | loss: 0.02649 |  0:02:09s\n",
      "epoch 96 | loss: 0.02699 |  0:02:10s\n",
      "epoch 97 | loss: 0.02559 |  0:02:12s\n",
      "epoch 98 | loss: 0.02535 |  0:02:13s\n",
      "epoch 99 | loss: 0.02551 |  0:02:14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.26966 |  0:00:01s\n",
      "epoch 1  | loss: 0.09943 |  0:00:02s\n",
      "epoch 2  | loss: 0.0917  |  0:00:03s\n",
      "epoch 3  | loss: 0.09059 |  0:00:05s\n",
      "epoch 4  | loss: 0.08523 |  0:00:06s\n",
      "epoch 5  | loss: 0.08809 |  0:00:07s\n",
      "epoch 6  | loss: 0.08482 |  0:00:09s\n",
      "epoch 7  | loss: 0.08207 |  0:00:10s\n",
      "epoch 8  | loss: 0.08027 |  0:00:11s\n",
      "epoch 9  | loss: 0.08136 |  0:00:13s\n",
      "epoch 10 | loss: 0.07979 |  0:00:14s\n",
      "epoch 11 | loss: 0.07843 |  0:00:15s\n",
      "epoch 12 | loss: 0.07671 |  0:00:16s\n",
      "epoch 13 | loss: 0.07483 |  0:00:18s\n",
      "epoch 14 | loss: 0.07653 |  0:00:19s\n",
      "epoch 15 | loss: 0.08156 |  0:00:21s\n",
      "epoch 16 | loss: 0.07544 |  0:00:22s\n",
      "epoch 17 | loss: 0.07351 |  0:00:23s\n",
      "epoch 18 | loss: 0.07119 |  0:00:24s\n",
      "epoch 19 | loss: 0.0696  |  0:00:26s\n",
      "epoch 20 | loss: 0.06805 |  0:00:27s\n",
      "epoch 21 | loss: 0.06798 |  0:00:28s\n",
      "epoch 22 | loss: 0.06614 |  0:00:30s\n",
      "epoch 23 | loss: 0.06524 |  0:00:31s\n",
      "epoch 24 | loss: 0.06532 |  0:00:32s\n",
      "epoch 25 | loss: 0.06284 |  0:00:34s\n",
      "epoch 26 | loss: 0.06076 |  0:00:35s\n",
      "epoch 27 | loss: 0.06168 |  0:00:36s\n",
      "epoch 28 | loss: 0.06836 |  0:00:37s\n",
      "epoch 29 | loss: 0.06433 |  0:00:39s\n",
      "epoch 30 | loss: 0.05956 |  0:00:40s\n",
      "epoch 31 | loss: 0.05742 |  0:00:41s\n",
      "epoch 32 | loss: 0.05375 |  0:00:43s\n",
      "epoch 33 | loss: 0.0516  |  0:00:44s\n",
      "epoch 34 | loss: 0.04984 |  0:00:45s\n",
      "epoch 35 | loss: 0.05058 |  0:00:47s\n",
      "epoch 36 | loss: 0.04818 |  0:00:48s\n",
      "epoch 37 | loss: 0.04964 |  0:00:49s\n",
      "epoch 38 | loss: 0.05544 |  0:00:51s\n",
      "epoch 39 | loss: 0.05035 |  0:00:52s\n",
      "epoch 40 | loss: 0.04641 |  0:00:53s\n",
      "epoch 41 | loss: 0.0538  |  0:00:54s\n",
      "epoch 42 | loss: 0.04824 |  0:00:56s\n",
      "epoch 43 | loss: 0.0489  |  0:00:57s\n",
      "epoch 44 | loss: 0.04518 |  0:00:58s\n",
      "epoch 45 | loss: 0.04293 |  0:01:00s\n",
      "epoch 46 | loss: 0.04058 |  0:01:01s\n",
      "epoch 47 | loss: 0.03982 |  0:01:02s\n",
      "epoch 48 | loss: 0.0418  |  0:01:03s\n",
      "epoch 49 | loss: 0.04142 |  0:01:05s\n",
      "epoch 50 | loss: 0.03957 |  0:01:06s\n",
      "epoch 51 | loss: 0.03768 |  0:01:08s\n",
      "epoch 52 | loss: 0.03494 |  0:01:09s\n",
      "epoch 53 | loss: 0.04019 |  0:01:11s\n",
      "epoch 54 | loss: 0.0445  |  0:01:12s\n",
      "epoch 55 | loss: 0.03861 |  0:01:14s\n",
      "epoch 56 | loss: 0.03494 |  0:01:16s\n",
      "epoch 57 | loss: 0.03316 |  0:01:17s\n",
      "epoch 58 | loss: 0.03399 |  0:01:19s\n",
      "epoch 59 | loss: 0.03337 |  0:01:20s\n",
      "epoch 60 | loss: 0.03306 |  0:01:22s\n",
      "epoch 61 | loss: 0.03292 |  0:01:23s\n",
      "epoch 62 | loss: 0.02928 |  0:01:25s\n",
      "epoch 63 | loss: 0.03093 |  0:01:26s\n",
      "epoch 64 | loss: 0.03306 |  0:01:28s\n",
      "epoch 65 | loss: 0.03061 |  0:01:30s\n",
      "epoch 66 | loss: 0.03165 |  0:01:31s\n",
      "epoch 67 | loss: 0.03193 |  0:01:33s\n",
      "epoch 68 | loss: 0.0308  |  0:01:34s\n",
      "epoch 69 | loss: 0.0335  |  0:01:35s\n",
      "epoch 70 | loss: 0.03127 |  0:01:37s\n",
      "epoch 71 | loss: 0.03112 |  0:01:39s\n",
      "epoch 72 | loss: 0.02959 |  0:01:40s\n",
      "epoch 73 | loss: 0.02772 |  0:01:41s\n",
      "epoch 74 | loss: 0.02842 |  0:01:43s\n",
      "epoch 75 | loss: 0.02471 |  0:01:44s\n",
      "epoch 76 | loss: 0.02538 |  0:01:46s\n",
      "epoch 77 | loss: 0.02562 |  0:01:47s\n",
      "epoch 78 | loss: 0.03321 |  0:01:49s\n",
      "epoch 79 | loss: 0.02964 |  0:01:50s\n",
      "epoch 80 | loss: 0.02835 |  0:01:52s\n",
      "epoch 81 | loss: 0.02533 |  0:01:53s\n",
      "epoch 82 | loss: 0.02438 |  0:01:54s\n",
      "epoch 83 | loss: 0.02481 |  0:01:56s\n",
      "epoch 84 | loss: 0.02346 |  0:01:57s\n",
      "epoch 85 | loss: 0.0221  |  0:01:58s\n",
      "epoch 86 | loss: 0.02231 |  0:02:00s\n",
      "epoch 87 | loss: 0.02317 |  0:02:01s\n",
      "epoch 88 | loss: 0.02168 |  0:02:03s\n",
      "epoch 89 | loss: 0.02335 |  0:02:04s\n",
      "epoch 90 | loss: 0.02326 |  0:02:05s\n",
      "epoch 91 | loss: 0.02237 |  0:02:07s\n",
      "epoch 92 | loss: 0.02115 |  0:02:08s\n",
      "epoch 93 | loss: 0.02098 |  0:02:10s\n",
      "epoch 94 | loss: 0.02231 |  0:02:11s\n",
      "epoch 95 | loss: 0.02421 |  0:02:12s\n",
      "epoch 96 | loss: 0.02337 |  0:02:14s\n",
      "epoch 97 | loss: 0.02476 |  0:02:15s\n",
      "epoch 98 | loss: 0.02468 |  0:02:16s\n",
      "epoch 99 | loss: 0.02136 |  0:02:18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.27195 |  0:00:01s\n",
      "epoch 1  | loss: 0.09801 |  0:00:02s\n",
      "epoch 2  | loss: 0.08986 |  0:00:04s\n",
      "epoch 3  | loss: 0.08678 |  0:00:05s\n",
      "epoch 4  | loss: 0.08994 |  0:00:06s\n",
      "epoch 5  | loss: 0.08775 |  0:00:08s\n",
      "epoch 6  | loss: 0.08445 |  0:00:09s\n",
      "epoch 7  | loss: 0.08228 |  0:00:11s\n",
      "epoch 8  | loss: 0.07944 |  0:00:12s\n",
      "epoch 9  | loss: 0.07953 |  0:00:13s\n",
      "epoch 10 | loss: 0.08024 |  0:00:15s\n",
      "epoch 11 | loss: 0.08059 |  0:00:16s\n",
      "epoch 12 | loss: 0.07672 |  0:00:18s\n",
      "epoch 13 | loss: 0.07812 |  0:00:19s\n",
      "epoch 14 | loss: 0.07631 |  0:00:20s\n",
      "epoch 15 | loss: 0.07494 |  0:00:22s\n",
      "epoch 16 | loss: 0.07152 |  0:00:23s\n",
      "epoch 17 | loss: 0.0693  |  0:00:24s\n",
      "epoch 18 | loss: 0.06921 |  0:00:26s\n",
      "epoch 19 | loss: 0.06723 |  0:00:27s\n",
      "epoch 20 | loss: 0.06739 |  0:00:28s\n",
      "epoch 21 | loss: 0.06657 |  0:00:30s\n",
      "epoch 22 | loss: 0.06605 |  0:00:31s\n",
      "epoch 23 | loss: 0.06548 |  0:00:33s\n",
      "epoch 24 | loss: 0.06646 |  0:00:34s\n",
      "epoch 25 | loss: 0.06593 |  0:00:35s\n",
      "epoch 26 | loss: 0.06355 |  0:00:37s\n",
      "epoch 27 | loss: 0.06427 |  0:00:38s\n",
      "epoch 28 | loss: 0.06357 |  0:00:40s\n",
      "epoch 29 | loss: 0.06644 |  0:00:41s\n",
      "epoch 30 | loss: 0.06055 |  0:00:42s\n",
      "epoch 31 | loss: 0.06178 |  0:00:44s\n",
      "epoch 32 | loss: 0.06044 |  0:00:45s\n",
      "epoch 33 | loss: 0.05753 |  0:00:47s\n",
      "epoch 34 | loss: 0.0566  |  0:00:48s\n",
      "epoch 35 | loss: 0.05628 |  0:00:49s\n",
      "epoch 36 | loss: 0.05701 |  0:00:51s\n",
      "epoch 37 | loss: 0.05686 |  0:00:52s\n",
      "epoch 38 | loss: 0.05308 |  0:00:54s\n",
      "epoch 39 | loss: 0.05564 |  0:00:55s\n",
      "epoch 40 | loss: 0.05509 |  0:00:56s\n",
      "epoch 41 | loss: 0.05364 |  0:00:58s\n",
      "epoch 42 | loss: 0.05076 |  0:00:59s\n",
      "epoch 43 | loss: 0.04828 |  0:01:00s\n",
      "epoch 44 | loss: 0.04594 |  0:01:02s\n",
      "epoch 45 | loss: 0.04376 |  0:01:03s\n",
      "epoch 46 | loss: 0.04808 |  0:01:04s\n",
      "epoch 47 | loss: 0.05004 |  0:01:06s\n",
      "epoch 48 | loss: 0.0455  |  0:01:07s\n",
      "epoch 49 | loss: 0.04475 |  0:01:08s\n",
      "epoch 50 | loss: 0.04804 |  0:01:10s\n",
      "epoch 51 | loss: 0.04872 |  0:01:11s\n",
      "epoch 52 | loss: 0.04642 |  0:01:13s\n",
      "epoch 53 | loss: 0.0439  |  0:01:14s\n",
      "epoch 54 | loss: 0.04417 |  0:01:15s\n",
      "epoch 55 | loss: 0.04753 |  0:01:17s\n",
      "epoch 56 | loss: 0.0454  |  0:01:18s\n",
      "epoch 57 | loss: 0.04621 |  0:01:20s\n",
      "epoch 58 | loss: 0.05683 |  0:01:21s\n",
      "epoch 59 | loss: 0.0553  |  0:01:22s\n",
      "epoch 60 | loss: 0.05004 |  0:01:24s\n",
      "epoch 61 | loss: 0.04741 |  0:01:25s\n",
      "epoch 62 | loss: 0.04488 |  0:01:27s\n",
      "epoch 63 | loss: 0.04537 |  0:01:28s\n",
      "epoch 64 | loss: 0.04389 |  0:01:29s\n",
      "epoch 65 | loss: 0.04583 |  0:01:31s\n",
      "epoch 66 | loss: 0.04005 |  0:01:32s\n",
      "epoch 67 | loss: 0.04635 |  0:01:33s\n",
      "epoch 68 | loss: 0.04911 |  0:01:35s\n",
      "epoch 69 | loss: 0.05199 |  0:01:36s\n",
      "epoch 70 | loss: 0.04463 |  0:01:38s\n",
      "epoch 71 | loss: 0.04311 |  0:01:39s\n",
      "epoch 72 | loss: 0.04274 |  0:01:40s\n",
      "epoch 73 | loss: 0.03907 |  0:01:42s\n",
      "epoch 74 | loss: 0.03635 |  0:01:43s\n",
      "epoch 75 | loss: 0.03601 |  0:01:44s\n",
      "epoch 76 | loss: 0.03411 |  0:01:46s\n",
      "epoch 77 | loss: 0.03244 |  0:01:47s\n",
      "epoch 78 | loss: 0.03379 |  0:01:49s\n",
      "epoch 79 | loss: 0.0308  |  0:01:50s\n",
      "epoch 80 | loss: 0.03735 |  0:01:51s\n",
      "epoch 81 | loss: 0.03453 |  0:01:53s\n",
      "epoch 82 | loss: 0.03126 |  0:01:54s\n",
      "epoch 83 | loss: 0.0311  |  0:01:55s\n",
      "epoch 84 | loss: 0.02886 |  0:01:57s\n",
      "epoch 85 | loss: 0.02658 |  0:01:58s\n",
      "epoch 86 | loss: 0.02698 |  0:02:00s\n",
      "epoch 87 | loss: 0.0277  |  0:02:01s\n",
      "epoch 88 | loss: 0.02708 |  0:02:02s\n",
      "epoch 89 | loss: 0.02577 |  0:02:04s\n",
      "epoch 90 | loss: 0.02477 |  0:02:05s\n",
      "epoch 91 | loss: 0.02308 |  0:02:06s\n",
      "epoch 92 | loss: 0.02507 |  0:02:08s\n",
      "epoch 93 | loss: 0.02675 |  0:02:09s\n",
      "epoch 94 | loss: 0.02753 |  0:02:10s\n",
      "epoch 95 | loss: 0.02348 |  0:02:12s\n",
      "epoch 96 | loss: 0.02468 |  0:02:13s\n",
      "epoch 97 | loss: 0.03188 |  0:02:15s\n",
      "epoch 98 | loss: 0.02422 |  0:02:16s\n",
      "epoch 99 | loss: 0.02482 |  0:02:17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.28495 |  0:00:01s\n",
      "epoch 1  | loss: 0.10197 |  0:00:02s\n",
      "epoch 2  | loss: 0.09207 |  0:00:04s\n",
      "epoch 3  | loss: 0.08748 |  0:00:05s\n",
      "epoch 4  | loss: 0.08391 |  0:00:06s\n",
      "epoch 5  | loss: 0.08072 |  0:00:08s\n",
      "epoch 6  | loss: 0.08064 |  0:00:09s\n",
      "epoch 7  | loss: 0.07907 |  0:00:10s\n",
      "epoch 8  | loss: 0.0761  |  0:00:12s\n",
      "epoch 9  | loss: 0.07518 |  0:00:13s\n",
      "epoch 10 | loss: 0.07629 |  0:00:14s\n",
      "epoch 11 | loss: 0.07611 |  0:00:16s\n",
      "epoch 12 | loss: 0.07191 |  0:00:17s\n",
      "epoch 13 | loss: 0.07108 |  0:00:19s\n",
      "epoch 14 | loss: 0.06953 |  0:00:20s\n",
      "epoch 15 | loss: 0.06721 |  0:00:21s\n",
      "epoch 16 | loss: 0.06546 |  0:00:23s\n",
      "epoch 17 | loss: 0.06472 |  0:00:24s\n",
      "epoch 18 | loss: 0.06258 |  0:00:25s\n",
      "epoch 19 | loss: 0.06062 |  0:00:27s\n",
      "epoch 20 | loss: 0.05893 |  0:00:28s\n",
      "epoch 21 | loss: 0.05708 |  0:00:30s\n",
      "epoch 22 | loss: 0.05619 |  0:00:31s\n",
      "epoch 23 | loss: 0.05499 |  0:00:32s\n",
      "epoch 24 | loss: 0.05189 |  0:00:34s\n",
      "epoch 25 | loss: 0.05189 |  0:00:35s\n",
      "epoch 26 | loss: 0.05159 |  0:00:36s\n",
      "epoch 27 | loss: 0.05127 |  0:00:38s\n",
      "epoch 28 | loss: 0.05912 |  0:00:39s\n",
      "epoch 29 | loss: 0.0662  |  0:00:41s\n",
      "epoch 30 | loss: 0.06055 |  0:00:42s\n",
      "epoch 31 | loss: 0.05538 |  0:00:43s\n",
      "epoch 32 | loss: 0.05395 |  0:00:45s\n",
      "epoch 33 | loss: 0.05117 |  0:00:46s\n",
      "epoch 34 | loss: 0.04939 |  0:00:47s\n",
      "epoch 35 | loss: 0.04755 |  0:00:49s\n",
      "epoch 36 | loss: 0.045   |  0:00:50s\n",
      "epoch 37 | loss: 0.04331 |  0:00:52s\n",
      "epoch 38 | loss: 0.0425  |  0:00:53s\n",
      "epoch 39 | loss: 0.0433  |  0:00:54s\n",
      "epoch 40 | loss: 0.04141 |  0:00:56s\n",
      "epoch 41 | loss: 0.03739 |  0:00:57s\n",
      "epoch 42 | loss: 0.03658 |  0:00:59s\n",
      "epoch 43 | loss: 0.03731 |  0:01:01s\n",
      "epoch 44 | loss: 0.03553 |  0:01:02s\n",
      "epoch 45 | loss: 0.03669 |  0:01:03s\n",
      "epoch 46 | loss: 0.04093 |  0:01:05s\n",
      "epoch 47 | loss: 0.04716 |  0:01:06s\n",
      "epoch 48 | loss: 0.04141 |  0:01:08s\n",
      "epoch 49 | loss: 0.03802 |  0:01:09s\n",
      "epoch 50 | loss: 0.03853 |  0:01:11s\n",
      "epoch 51 | loss: 0.0391  |  0:01:12s\n",
      "epoch 52 | loss: 0.03632 |  0:01:14s\n",
      "epoch 53 | loss: 0.03522 |  0:01:15s\n",
      "epoch 54 | loss: 0.03256 |  0:01:16s\n",
      "epoch 55 | loss: 0.03192 |  0:01:18s\n",
      "epoch 56 | loss: 0.03819 |  0:01:20s\n",
      "epoch 57 | loss: 0.03754 |  0:01:21s\n",
      "epoch 58 | loss: 0.03564 |  0:01:23s\n",
      "epoch 59 | loss: 0.03155 |  0:01:24s\n",
      "epoch 60 | loss: 0.03109 |  0:01:25s\n",
      "epoch 61 | loss: 0.02998 |  0:01:27s\n",
      "epoch 62 | loss: 0.0282  |  0:01:28s\n",
      "epoch 63 | loss: 0.02727 |  0:01:30s\n",
      "epoch 64 | loss: 0.03215 |  0:01:31s\n",
      "epoch 65 | loss: 0.04797 |  0:01:33s\n",
      "epoch 66 | loss: 0.04152 |  0:01:34s\n",
      "epoch 67 | loss: 0.03765 |  0:01:36s\n",
      "epoch 68 | loss: 0.03802 |  0:01:38s\n",
      "epoch 69 | loss: 0.03405 |  0:01:39s\n",
      "epoch 70 | loss: 0.03153 |  0:01:41s\n",
      "epoch 71 | loss: 0.02984 |  0:01:42s\n",
      "epoch 72 | loss: 0.03038 |  0:01:43s\n",
      "epoch 73 | loss: 0.02769 |  0:01:45s\n",
      "epoch 74 | loss: 0.02674 |  0:01:46s\n",
      "epoch 75 | loss: 0.02505 |  0:01:48s\n",
      "epoch 76 | loss: 0.02478 |  0:01:49s\n",
      "epoch 77 | loss: 0.02532 |  0:01:50s\n",
      "epoch 78 | loss: 0.02528 |  0:01:52s\n",
      "epoch 79 | loss: 0.02403 |  0:01:53s\n",
      "epoch 80 | loss: 0.02315 |  0:01:54s\n",
      "epoch 81 | loss: 0.02319 |  0:01:56s\n",
      "epoch 82 | loss: 0.02398 |  0:01:57s\n",
      "epoch 83 | loss: 0.02515 |  0:01:58s\n",
      "epoch 84 | loss: 0.0295  |  0:02:00s\n",
      "epoch 85 | loss: 0.03099 |  0:02:01s\n",
      "epoch 86 | loss: 0.02573 |  0:02:02s\n",
      "epoch 87 | loss: 0.02519 |  0:02:04s\n",
      "epoch 88 | loss: 0.02304 |  0:02:05s\n",
      "epoch 89 | loss: 0.02459 |  0:02:06s\n",
      "epoch 90 | loss: 0.02311 |  0:02:08s\n",
      "epoch 91 | loss: 0.02024 |  0:02:10s\n",
      "epoch 92 | loss: 0.02135 |  0:02:11s\n",
      "epoch 93 | loss: 0.02146 |  0:02:13s\n",
      "epoch 94 | loss: 0.01882 |  0:02:14s\n",
      "epoch 95 | loss: 0.01966 |  0:02:16s\n",
      "epoch 96 | loss: 0.0265  |  0:02:17s\n",
      "epoch 97 | loss: 0.03106 |  0:02:18s\n",
      "epoch 98 | loss: 0.0276  |  0:02:19s\n",
      "epoch 99 | loss: 0.02093 |  0:02:21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.26591 |  0:00:01s\n",
      "epoch 1  | loss: 0.10175 |  0:00:03s\n",
      "epoch 2  | loss: 0.0928  |  0:00:05s\n",
      "epoch 3  | loss: 0.0913  |  0:00:06s\n",
      "epoch 4  | loss: 0.09056 |  0:00:08s\n",
      "epoch 5  | loss: 0.0899  |  0:00:10s\n",
      "epoch 6  | loss: 0.0971  |  0:00:11s\n",
      "epoch 7  | loss: 0.09131 |  0:00:13s\n",
      "epoch 8  | loss: 0.08616 |  0:00:14s\n",
      "epoch 9  | loss: 0.08297 |  0:00:15s\n",
      "epoch 10 | loss: 0.08448 |  0:00:17s\n",
      "epoch 11 | loss: 0.08431 |  0:00:18s\n",
      "epoch 12 | loss: 0.08187 |  0:00:19s\n",
      "epoch 13 | loss: 0.08177 |  0:00:21s\n",
      "epoch 14 | loss: 0.08812 |  0:00:22s\n",
      "epoch 15 | loss: 0.08236 |  0:00:23s\n",
      "epoch 16 | loss: 0.08434 |  0:00:25s\n",
      "epoch 17 | loss: 0.08842 |  0:00:26s\n",
      "epoch 18 | loss: 0.08753 |  0:00:27s\n",
      "epoch 19 | loss: 0.08178 |  0:00:29s\n",
      "epoch 20 | loss: 0.08328 |  0:00:30s\n",
      "epoch 21 | loss: 0.07975 |  0:00:32s\n",
      "epoch 22 | loss: 0.08461 |  0:00:33s\n",
      "epoch 23 | loss: 0.08547 |  0:00:34s\n",
      "epoch 24 | loss: 0.08114 |  0:00:36s\n",
      "epoch 25 | loss: 0.07658 |  0:00:37s\n",
      "epoch 26 | loss: 0.07759 |  0:00:39s\n",
      "epoch 27 | loss: 0.07393 |  0:00:40s\n",
      "epoch 28 | loss: 0.07274 |  0:00:41s\n",
      "epoch 29 | loss: 0.07206 |  0:00:43s\n",
      "epoch 30 | loss: 0.07529 |  0:00:44s\n",
      "epoch 31 | loss: 0.07196 |  0:00:45s\n",
      "epoch 32 | loss: 0.06998 |  0:00:47s\n",
      "epoch 33 | loss: 0.07418 |  0:00:48s\n",
      "epoch 34 | loss: 0.07544 |  0:00:50s\n",
      "epoch 35 | loss: 0.07099 |  0:00:51s\n",
      "epoch 36 | loss: 0.06777 |  0:00:52s\n",
      "epoch 37 | loss: 0.06629 |  0:00:54s\n",
      "epoch 38 | loss: 0.06447 |  0:00:55s\n",
      "epoch 39 | loss: 0.0659  |  0:00:56s\n",
      "epoch 40 | loss: 0.06537 |  0:00:58s\n",
      "epoch 41 | loss: 0.06421 |  0:00:59s\n",
      "epoch 42 | loss: 0.06154 |  0:01:01s\n",
      "epoch 43 | loss: 0.06405 |  0:01:02s\n",
      "epoch 44 | loss: 0.06418 |  0:01:03s\n",
      "epoch 45 | loss: 0.06652 |  0:01:05s\n",
      "epoch 46 | loss: 0.0664  |  0:01:06s\n",
      "epoch 47 | loss: 0.06791 |  0:01:07s\n",
      "epoch 48 | loss: 0.0698  |  0:01:09s\n",
      "epoch 49 | loss: 0.06983 |  0:01:10s\n",
      "epoch 50 | loss: 0.06545 |  0:01:12s\n",
      "epoch 51 | loss: 0.06481 |  0:01:13s\n",
      "epoch 52 | loss: 0.06193 |  0:01:15s\n",
      "epoch 53 | loss: 0.06208 |  0:01:16s\n",
      "epoch 54 | loss: 0.05913 |  0:01:17s\n",
      "epoch 55 | loss: 0.05574 |  0:01:19s\n",
      "epoch 56 | loss: 0.05303 |  0:01:20s\n",
      "epoch 57 | loss: 0.05619 |  0:01:21s\n",
      "epoch 58 | loss: 0.05812 |  0:01:23s\n",
      "epoch 59 | loss: 0.05318 |  0:01:24s\n",
      "epoch 60 | loss: 0.05126 |  0:01:25s\n",
      "epoch 61 | loss: 0.05104 |  0:01:27s\n",
      "epoch 62 | loss: 0.05076 |  0:01:28s\n",
      "epoch 63 | loss: 0.04909 |  0:01:30s\n",
      "epoch 64 | loss: 0.04635 |  0:01:32s\n",
      "epoch 65 | loss: 0.04718 |  0:01:34s\n",
      "epoch 66 | loss: 0.04565 |  0:01:36s\n",
      "epoch 67 | loss: 0.04584 |  0:01:37s\n",
      "epoch 68 | loss: 0.047   |  0:01:39s\n",
      "epoch 69 | loss: 0.04697 |  0:01:41s\n",
      "epoch 70 | loss: 0.0456  |  0:01:42s\n",
      "epoch 71 | loss: 0.04629 |  0:01:44s\n",
      "epoch 72 | loss: 0.05076 |  0:01:45s\n",
      "epoch 73 | loss: 0.04631 |  0:01:47s\n",
      "epoch 74 | loss: 0.04365 |  0:01:49s\n",
      "epoch 75 | loss: 0.04338 |  0:01:50s\n",
      "epoch 76 | loss: 0.04175 |  0:01:52s\n",
      "epoch 77 | loss: 0.0413  |  0:01:54s\n",
      "epoch 78 | loss: 0.04091 |  0:01:55s\n",
      "epoch 79 | loss: 0.04423 |  0:01:57s\n",
      "epoch 80 | loss: 0.04223 |  0:01:59s\n",
      "epoch 81 | loss: 0.03952 |  0:02:00s\n",
      "epoch 82 | loss: 0.03795 |  0:02:02s\n",
      "epoch 83 | loss: 0.03868 |  0:02:04s\n",
      "epoch 84 | loss: 0.03692 |  0:02:05s\n",
      "epoch 85 | loss: 0.03569 |  0:02:07s\n",
      "epoch 86 | loss: 0.03597 |  0:02:09s\n",
      "epoch 87 | loss: 0.03618 |  0:02:10s\n",
      "epoch 88 | loss: 0.04107 |  0:02:12s\n",
      "epoch 89 | loss: 0.03684 |  0:02:14s\n",
      "epoch 90 | loss: 0.03836 |  0:02:16s\n",
      "epoch 91 | loss: 0.03524 |  0:02:17s\n",
      "epoch 92 | loss: 0.03609 |  0:02:19s\n",
      "epoch 93 | loss: 0.03321 |  0:02:20s\n",
      "epoch 94 | loss: 0.03301 |  0:02:22s\n",
      "epoch 95 | loss: 0.03204 |  0:02:23s\n",
      "epoch 96 | loss: 0.03178 |  0:02:25s\n",
      "epoch 97 | loss: 0.02982 |  0:02:27s\n",
      "epoch 98 | loss: 0.02867 |  0:02:28s\n",
      "epoch 99 | loss: 0.03005 |  0:02:30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29062 |  0:00:01s\n",
      "epoch 1  | loss: 0.10934 |  0:00:03s\n",
      "epoch 2  | loss: 0.09527 |  0:00:04s\n",
      "epoch 3  | loss: 0.09196 |  0:00:06s\n",
      "epoch 4  | loss: 0.08987 |  0:00:08s\n",
      "epoch 5  | loss: 0.08711 |  0:00:09s\n",
      "epoch 6  | loss: 0.09315 |  0:00:11s\n",
      "epoch 7  | loss: 0.08571 |  0:00:12s\n",
      "epoch 8  | loss: 0.08173 |  0:00:14s\n",
      "epoch 9  | loss: 0.07972 |  0:00:16s\n",
      "epoch 10 | loss: 0.07803 |  0:00:17s\n",
      "epoch 11 | loss: 0.07967 |  0:00:19s\n",
      "epoch 12 | loss: 0.07865 |  0:00:21s\n",
      "epoch 13 | loss: 0.07756 |  0:00:22s\n",
      "epoch 14 | loss: 0.07415 |  0:00:24s\n",
      "epoch 15 | loss: 0.07617 |  0:00:25s\n",
      "epoch 16 | loss: 0.0742  |  0:00:27s\n",
      "epoch 17 | loss: 0.07725 |  0:00:28s\n",
      "epoch 18 | loss: 0.07623 |  0:00:30s\n",
      "epoch 19 | loss: 0.07225 |  0:00:32s\n",
      "epoch 20 | loss: 0.07129 |  0:00:33s\n",
      "epoch 21 | loss: 0.07241 |  0:00:35s\n",
      "epoch 22 | loss: 0.06973 |  0:00:36s\n",
      "epoch 23 | loss: 0.06784 |  0:00:38s\n",
      "epoch 24 | loss: 0.06418 |  0:00:40s\n",
      "epoch 25 | loss: 0.06331 |  0:00:41s\n",
      "epoch 26 | loss: 0.06248 |  0:00:43s\n",
      "epoch 27 | loss: 0.06165 |  0:00:44s\n",
      "epoch 28 | loss: 0.06162 |  0:00:46s\n",
      "epoch 29 | loss: 0.05985 |  0:00:48s\n",
      "epoch 30 | loss: 0.05735 |  0:00:50s\n",
      "epoch 31 | loss: 0.05516 |  0:00:51s\n",
      "epoch 32 | loss: 0.0559  |  0:00:53s\n",
      "epoch 33 | loss: 0.05437 |  0:00:55s\n",
      "epoch 34 | loss: 0.05536 |  0:00:56s\n",
      "epoch 35 | loss: 0.05605 |  0:00:58s\n",
      "epoch 36 | loss: 0.05391 |  0:00:59s\n",
      "epoch 37 | loss: 0.05321 |  0:01:01s\n",
      "epoch 38 | loss: 0.05232 |  0:01:03s\n",
      "epoch 39 | loss: 0.04919 |  0:01:04s\n",
      "epoch 40 | loss: 0.04613 |  0:01:06s\n",
      "epoch 41 | loss: 0.05384 |  0:01:08s\n",
      "epoch 42 | loss: 0.05027 |  0:01:09s\n",
      "epoch 43 | loss: 0.05689 |  0:01:11s\n",
      "epoch 44 | loss: 0.05622 |  0:01:13s\n",
      "epoch 45 | loss: 0.0515  |  0:01:14s\n",
      "epoch 46 | loss: 0.05595 |  0:01:16s\n",
      "epoch 47 | loss: 0.05415 |  0:01:18s\n",
      "epoch 48 | loss: 0.04826 |  0:01:19s\n",
      "epoch 49 | loss: 0.04721 |  0:01:21s\n",
      "epoch 50 | loss: 0.0441  |  0:01:23s\n",
      "epoch 51 | loss: 0.04122 |  0:01:24s\n",
      "epoch 52 | loss: 0.03883 |  0:01:26s\n",
      "epoch 53 | loss: 0.03918 |  0:01:27s\n",
      "epoch 54 | loss: 0.0392  |  0:01:29s\n",
      "epoch 55 | loss: 0.04474 |  0:01:31s\n",
      "epoch 56 | loss: 0.0397  |  0:01:32s\n",
      "epoch 57 | loss: 0.03985 |  0:01:34s\n",
      "epoch 58 | loss: 0.03736 |  0:01:36s\n",
      "epoch 59 | loss: 0.03758 |  0:01:37s\n",
      "epoch 60 | loss: 0.04352 |  0:01:39s\n",
      "epoch 61 | loss: 0.04039 |  0:01:40s\n",
      "epoch 62 | loss: 0.03729 |  0:01:42s\n",
      "epoch 63 | loss: 0.03602 |  0:01:44s\n",
      "epoch 64 | loss: 0.03442 |  0:01:45s\n",
      "epoch 65 | loss: 0.0343  |  0:01:47s\n",
      "epoch 66 | loss: 0.03476 |  0:01:48s\n",
      "epoch 67 | loss: 0.03606 |  0:01:50s\n",
      "epoch 68 | loss: 0.03471 |  0:01:52s\n",
      "epoch 69 | loss: 0.03214 |  0:01:53s\n",
      "epoch 70 | loss: 0.02936 |  0:01:55s\n",
      "epoch 71 | loss: 0.02963 |  0:01:57s\n",
      "epoch 72 | loss: 0.03726 |  0:01:58s\n",
      "epoch 73 | loss: 0.0413  |  0:02:00s\n",
      "epoch 74 | loss: 0.033   |  0:02:01s\n",
      "epoch 75 | loss: 0.03314 |  0:02:03s\n",
      "epoch 76 | loss: 0.0345  |  0:02:05s\n",
      "epoch 77 | loss: 0.03185 |  0:02:06s\n",
      "epoch 78 | loss: 0.03235 |  0:02:08s\n",
      "epoch 79 | loss: 0.03475 |  0:02:10s\n",
      "epoch 80 | loss: 0.03041 |  0:02:11s\n",
      "epoch 81 | loss: 0.02747 |  0:02:13s\n",
      "epoch 82 | loss: 0.02598 |  0:02:15s\n",
      "epoch 83 | loss: 0.02507 |  0:02:16s\n",
      "epoch 84 | loss: 0.02357 |  0:02:18s\n",
      "epoch 85 | loss: 0.02645 |  0:02:20s\n",
      "epoch 86 | loss: 0.02814 |  0:02:21s\n",
      "epoch 87 | loss: 0.03039 |  0:02:23s\n",
      "epoch 88 | loss: 0.02722 |  0:02:24s\n",
      "epoch 89 | loss: 0.02521 |  0:02:26s\n",
      "epoch 90 | loss: 0.03955 |  0:02:27s\n",
      "epoch 91 | loss: 0.03341 |  0:02:29s\n",
      "epoch 92 | loss: 0.03018 |  0:02:31s\n",
      "epoch 93 | loss: 0.02896 |  0:02:32s\n",
      "epoch 94 | loss: 0.0281  |  0:02:34s\n",
      "epoch 95 | loss: 0.0243  |  0:02:36s\n",
      "epoch 96 | loss: 0.02252 |  0:02:37s\n",
      "epoch 97 | loss: 0.02285 |  0:02:39s\n",
      "epoch 98 | loss: 0.02045 |  0:02:40s\n",
      "epoch 99 | loss: 0.01959 |  0:02:42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.28073 |  0:00:01s\n",
      "epoch 1  | loss: 0.10531 |  0:00:03s\n",
      "epoch 2  | loss: 0.09505 |  0:00:05s\n",
      "epoch 3  | loss: 0.0904  |  0:00:06s\n",
      "epoch 4  | loss: 0.08933 |  0:00:08s\n",
      "epoch 5  | loss: 0.08535 |  0:00:10s\n",
      "epoch 6  | loss: 0.08631 |  0:00:11s\n",
      "epoch 7  | loss: 0.0844  |  0:00:13s\n",
      "epoch 8  | loss: 0.08142 |  0:00:15s\n",
      "epoch 9  | loss: 0.07739 |  0:00:16s\n",
      "epoch 10 | loss: 0.07569 |  0:00:18s\n",
      "epoch 11 | loss: 0.07841 |  0:00:20s\n",
      "epoch 12 | loss: 0.07674 |  0:00:21s\n",
      "epoch 13 | loss: 0.07339 |  0:00:23s\n",
      "epoch 14 | loss: 0.07205 |  0:00:24s\n",
      "epoch 15 | loss: 0.07005 |  0:00:26s\n",
      "epoch 16 | loss: 0.06991 |  0:00:28s\n",
      "epoch 17 | loss: 0.07055 |  0:00:29s\n",
      "epoch 18 | loss: 0.06697 |  0:00:31s\n",
      "epoch 19 | loss: 0.06403 |  0:00:33s\n",
      "epoch 20 | loss: 0.06187 |  0:00:34s\n",
      "epoch 21 | loss: 0.07128 |  0:00:36s\n",
      "epoch 22 | loss: 0.07277 |  0:00:38s\n",
      "epoch 23 | loss: 0.06865 |  0:00:39s\n",
      "epoch 24 | loss: 0.06678 |  0:00:41s\n",
      "epoch 25 | loss: 0.06549 |  0:00:43s\n",
      "epoch 26 | loss: 0.06228 |  0:00:44s\n",
      "epoch 27 | loss: 0.05932 |  0:00:46s\n",
      "epoch 28 | loss: 0.05852 |  0:00:48s\n",
      "epoch 29 | loss: 0.05599 |  0:00:50s\n",
      "epoch 30 | loss: 0.05632 |  0:00:51s\n",
      "epoch 31 | loss: 0.05465 |  0:00:53s\n",
      "epoch 32 | loss: 0.05481 |  0:00:55s\n",
      "epoch 33 | loss: 0.05254 |  0:00:56s\n",
      "epoch 34 | loss: 0.0556  |  0:00:58s\n",
      "epoch 35 | loss: 0.05218 |  0:00:59s\n",
      "epoch 36 | loss: 0.05024 |  0:01:01s\n",
      "epoch 37 | loss: 0.04849 |  0:01:02s\n",
      "epoch 38 | loss: 0.04661 |  0:01:04s\n",
      "epoch 39 | loss: 0.04722 |  0:01:06s\n",
      "epoch 40 | loss: 0.05144 |  0:01:07s\n",
      "epoch 41 | loss: 0.05122 |  0:01:09s\n",
      "epoch 42 | loss: 0.04774 |  0:01:10s\n",
      "epoch 43 | loss: 0.04546 |  0:01:12s\n",
      "epoch 44 | loss: 0.04566 |  0:01:14s\n",
      "epoch 45 | loss: 0.04662 |  0:01:15s\n",
      "epoch 46 | loss: 0.04303 |  0:01:17s\n",
      "epoch 47 | loss: 0.04439 |  0:01:19s\n",
      "epoch 48 | loss: 0.04229 |  0:01:20s\n",
      "epoch 49 | loss: 0.04063 |  0:01:22s\n",
      "epoch 50 | loss: 0.04056 |  0:01:24s\n",
      "epoch 51 | loss: 0.03874 |  0:01:25s\n",
      "epoch 52 | loss: 0.04376 |  0:01:27s\n",
      "epoch 53 | loss: 0.04047 |  0:01:29s\n",
      "epoch 54 | loss: 0.03978 |  0:01:30s\n",
      "epoch 55 | loss: 0.03741 |  0:01:32s\n",
      "epoch 56 | loss: 0.03664 |  0:01:34s\n",
      "epoch 57 | loss: 0.03563 |  0:01:35s\n",
      "epoch 58 | loss: 0.03349 |  0:01:37s\n",
      "epoch 59 | loss: 0.03391 |  0:01:38s\n",
      "epoch 60 | loss: 0.0314  |  0:01:40s\n",
      "epoch 61 | loss: 0.03115 |  0:01:42s\n",
      "epoch 62 | loss: 0.03111 |  0:01:44s\n",
      "epoch 63 | loss: 0.03293 |  0:01:45s\n",
      "epoch 64 | loss: 0.0371  |  0:01:47s\n",
      "epoch 65 | loss: 0.03483 |  0:01:48s\n",
      "epoch 66 | loss: 0.03258 |  0:01:50s\n",
      "epoch 67 | loss: 0.02904 |  0:01:51s\n",
      "epoch 68 | loss: 0.03218 |  0:01:53s\n",
      "epoch 69 | loss: 0.03189 |  0:01:55s\n",
      "epoch 70 | loss: 0.0288  |  0:01:56s\n",
      "epoch 71 | loss: 0.02821 |  0:01:58s\n",
      "epoch 72 | loss: 0.02748 |  0:02:00s\n",
      "epoch 73 | loss: 0.02501 |  0:02:01s\n",
      "epoch 74 | loss: 0.02356 |  0:02:03s\n",
      "epoch 75 | loss: 0.02631 |  0:02:04s\n",
      "epoch 76 | loss: 0.02722 |  0:02:06s\n",
      "epoch 77 | loss: 0.02696 |  0:02:07s\n",
      "epoch 78 | loss: 0.0241  |  0:02:09s\n",
      "epoch 79 | loss: 0.02362 |  0:02:10s\n",
      "epoch 80 | loss: 0.02816 |  0:02:12s\n",
      "epoch 81 | loss: 0.02491 |  0:02:13s\n",
      "epoch 82 | loss: 0.02366 |  0:02:15s\n",
      "epoch 83 | loss: 0.02357 |  0:02:16s\n",
      "epoch 84 | loss: 0.02415 |  0:02:18s\n",
      "epoch 85 | loss: 0.02519 |  0:02:20s\n",
      "epoch 86 | loss: 0.02482 |  0:02:21s\n",
      "epoch 87 | loss: 0.02561 |  0:02:23s\n",
      "epoch 88 | loss: 0.02472 |  0:02:25s\n",
      "epoch 89 | loss: 0.02135 |  0:02:26s\n",
      "epoch 90 | loss: 0.0194  |  0:02:28s\n",
      "epoch 91 | loss: 0.01908 |  0:02:30s\n",
      "epoch 92 | loss: 0.01853 |  0:02:31s\n",
      "epoch 93 | loss: 0.01963 |  0:02:33s\n",
      "epoch 94 | loss: 0.01957 |  0:02:35s\n",
      "epoch 95 | loss: 0.02267 |  0:02:36s\n",
      "epoch 96 | loss: 0.02114 |  0:02:38s\n",
      "epoch 97 | loss: 0.02117 |  0:02:39s\n",
      "epoch 98 | loss: 0.01793 |  0:02:41s\n",
      "epoch 99 | loss: 0.01978 |  0:02:43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.26885 |  0:00:01s\n",
      "epoch 1  | loss: 0.09946 |  0:00:03s\n",
      "epoch 2  | loss: 0.09031 |  0:00:05s\n",
      "epoch 3  | loss: 0.08831 |  0:00:06s\n",
      "epoch 4  | loss: 0.08617 |  0:00:08s\n",
      "epoch 5  | loss: 0.08591 |  0:00:10s\n",
      "epoch 6  | loss: 0.08264 |  0:00:11s\n",
      "epoch 7  | loss: 0.08151 |  0:00:13s\n",
      "epoch 8  | loss: 0.08047 |  0:00:15s\n",
      "epoch 9  | loss: 0.07749 |  0:00:17s\n",
      "epoch 10 | loss: 0.07798 |  0:00:18s\n",
      "epoch 11 | loss: 0.07783 |  0:00:20s\n",
      "epoch 12 | loss: 0.07469 |  0:00:21s\n",
      "epoch 13 | loss: 0.07259 |  0:00:23s\n",
      "epoch 14 | loss: 0.06964 |  0:00:25s\n",
      "epoch 15 | loss: 0.06841 |  0:00:28s\n",
      "epoch 16 | loss: 0.06381 |  0:00:29s\n",
      "epoch 17 | loss: 0.06245 |  0:00:31s\n",
      "epoch 18 | loss: 0.06258 |  0:00:33s\n",
      "epoch 19 | loss: 0.0601  |  0:00:35s\n",
      "epoch 20 | loss: 0.05819 |  0:00:37s\n",
      "epoch 21 | loss: 0.05994 |  0:00:38s\n",
      "epoch 22 | loss: 0.05695 |  0:00:40s\n",
      "epoch 23 | loss: 0.05661 |  0:00:42s\n",
      "epoch 24 | loss: 0.05508 |  0:00:44s\n",
      "epoch 25 | loss: 0.05728 |  0:00:46s\n",
      "epoch 26 | loss: 0.05629 |  0:00:48s\n",
      "epoch 27 | loss: 0.05292 |  0:00:50s\n",
      "epoch 28 | loss: 0.05636 |  0:00:52s\n",
      "epoch 29 | loss: 0.05389 |  0:00:54s\n",
      "epoch 30 | loss: 0.05135 |  0:00:56s\n",
      "epoch 31 | loss: 0.05255 |  0:00:58s\n",
      "epoch 32 | loss: 0.05203 |  0:01:00s\n",
      "epoch 33 | loss: 0.0515  |  0:01:02s\n",
      "epoch 34 | loss: 0.04924 |  0:01:04s\n",
      "epoch 35 | loss: 0.04878 |  0:01:06s\n",
      "epoch 36 | loss: 0.04651 |  0:01:08s\n",
      "epoch 37 | loss: 0.05187 |  0:01:09s\n",
      "epoch 38 | loss: 0.0557  |  0:01:11s\n",
      "epoch 39 | loss: 0.05103 |  0:01:13s\n",
      "epoch 40 | loss: 0.04764 |  0:01:15s\n",
      "epoch 41 | loss: 0.05146 |  0:01:17s\n",
      "epoch 42 | loss: 0.05147 |  0:01:19s\n",
      "epoch 43 | loss: 0.0489  |  0:01:21s\n",
      "epoch 44 | loss: 0.0528  |  0:01:23s\n",
      "epoch 45 | loss: 0.05835 |  0:01:25s\n",
      "epoch 46 | loss: 0.05172 |  0:01:27s\n",
      "epoch 47 | loss: 0.0489  |  0:01:29s\n",
      "epoch 48 | loss: 0.04721 |  0:01:31s\n",
      "epoch 49 | loss: 0.05101 |  0:01:33s\n",
      "epoch 50 | loss: 0.04692 |  0:01:35s\n",
      "epoch 51 | loss: 0.05304 |  0:01:37s\n",
      "epoch 52 | loss: 0.04648 |  0:01:39s\n",
      "epoch 53 | loss: 0.04388 |  0:01:41s\n",
      "epoch 54 | loss: 0.04259 |  0:01:43s\n",
      "epoch 55 | loss: 0.04047 |  0:01:45s\n",
      "epoch 56 | loss: 0.03848 |  0:01:47s\n",
      "epoch 57 | loss: 0.04108 |  0:01:49s\n",
      "epoch 58 | loss: 0.04433 |  0:01:51s\n",
      "epoch 59 | loss: 0.04252 |  0:01:54s\n",
      "epoch 60 | loss: 0.03963 |  0:01:56s\n",
      "epoch 61 | loss: 0.037   |  0:01:59s\n",
      "epoch 62 | loss: 0.03559 |  0:02:01s\n",
      "epoch 63 | loss: 0.03853 |  0:02:03s\n",
      "epoch 64 | loss: 0.04814 |  0:02:05s\n",
      "epoch 65 | loss: 0.04364 |  0:02:07s\n",
      "epoch 66 | loss: 0.03769 |  0:02:08s\n",
      "epoch 67 | loss: 0.03631 |  0:02:11s\n",
      "epoch 68 | loss: 0.04305 |  0:02:12s\n",
      "epoch 69 | loss: 0.04362 |  0:02:14s\n",
      "epoch 70 | loss: 0.04114 |  0:02:16s\n",
      "epoch 71 | loss: 0.0424  |  0:02:18s\n",
      "epoch 72 | loss: 0.04057 |  0:02:21s\n",
      "epoch 73 | loss: 0.0391  |  0:02:23s\n",
      "epoch 74 | loss: 0.03507 |  0:02:25s\n",
      "epoch 75 | loss: 0.0322  |  0:02:27s\n",
      "epoch 76 | loss: 0.03814 |  0:02:29s\n",
      "epoch 77 | loss: 0.03813 |  0:02:31s\n",
      "epoch 78 | loss: 0.04615 |  0:02:33s\n",
      "epoch 79 | loss: 0.03843 |  0:02:35s\n",
      "epoch 80 | loss: 0.03431 |  0:02:37s\n",
      "epoch 81 | loss: 0.03427 |  0:02:39s\n",
      "epoch 82 | loss: 0.03465 |  0:02:41s\n",
      "epoch 83 | loss: 0.0342  |  0:02:43s\n",
      "epoch 84 | loss: 0.03353 |  0:02:45s\n",
      "epoch 85 | loss: 0.03177 |  0:02:48s\n",
      "epoch 86 | loss: 0.02919 |  0:02:50s\n",
      "epoch 87 | loss: 0.02855 |  0:02:52s\n",
      "epoch 88 | loss: 0.02865 |  0:02:53s\n",
      "epoch 89 | loss: 0.03215 |  0:02:55s\n",
      "epoch 90 | loss: 0.03649 |  0:02:57s\n",
      "epoch 91 | loss: 0.04736 |  0:02:59s\n",
      "epoch 92 | loss: 0.03903 |  0:03:01s\n",
      "epoch 93 | loss: 0.03486 |  0:03:02s\n",
      "epoch 94 | loss: 0.03149 |  0:03:04s\n",
      "epoch 95 | loss: 0.03199 |  0:03:06s\n",
      "epoch 96 | loss: 0.03157 |  0:03:08s\n",
      "epoch 97 | loss: 0.03459 |  0:03:09s\n",
      "epoch 98 | loss: 0.03361 |  0:03:11s\n",
      "epoch 99 | loss: 0.04963 |  0:03:13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.27514 |  0:00:01s\n",
      "epoch 1  | loss: 0.11096 |  0:00:03s\n",
      "epoch 2  | loss: 0.0905  |  0:00:04s\n",
      "epoch 3  | loss: 0.08622 |  0:00:06s\n",
      "epoch 4  | loss: 0.08285 |  0:00:07s\n",
      "epoch 5  | loss: 0.07924 |  0:00:09s\n",
      "epoch 6  | loss: 0.07866 |  0:00:10s\n",
      "epoch 7  | loss: 0.07467 |  0:00:12s\n",
      "epoch 8  | loss: 0.07264 |  0:00:14s\n",
      "epoch 9  | loss: 0.07163 |  0:00:15s\n",
      "epoch 10 | loss: 0.07093 |  0:00:17s\n",
      "epoch 11 | loss: 0.06847 |  0:00:19s\n",
      "epoch 12 | loss: 0.06588 |  0:00:20s\n",
      "epoch 13 | loss: 0.0649  |  0:00:21s\n",
      "epoch 14 | loss: 0.06214 |  0:00:23s\n",
      "epoch 15 | loss: 0.06175 |  0:00:24s\n",
      "epoch 16 | loss: 0.06026 |  0:00:26s\n",
      "epoch 17 | loss: 0.0601  |  0:00:27s\n",
      "epoch 18 | loss: 0.05849 |  0:00:29s\n",
      "epoch 19 | loss: 0.05847 |  0:00:30s\n",
      "epoch 20 | loss: 0.05822 |  0:00:32s\n",
      "epoch 21 | loss: 0.0582  |  0:00:33s\n",
      "epoch 22 | loss: 0.05445 |  0:00:35s\n",
      "epoch 23 | loss: 0.0555  |  0:00:37s\n",
      "epoch 24 | loss: 0.05566 |  0:00:38s\n",
      "epoch 25 | loss: 0.05702 |  0:00:40s\n",
      "epoch 26 | loss: 0.05279 |  0:00:41s\n",
      "epoch 27 | loss: 0.05404 |  0:00:43s\n",
      "epoch 28 | loss: 0.06014 |  0:00:44s\n",
      "epoch 29 | loss: 0.06322 |  0:00:46s\n",
      "epoch 30 | loss: 0.06249 |  0:00:47s\n",
      "epoch 31 | loss: 0.06242 |  0:00:49s\n",
      "epoch 32 | loss: 0.05699 |  0:00:51s\n",
      "epoch 33 | loss: 0.05541 |  0:00:52s\n",
      "epoch 34 | loss: 0.05105 |  0:00:54s\n",
      "epoch 35 | loss: 0.04662 |  0:00:55s\n",
      "epoch 36 | loss: 0.04828 |  0:00:57s\n",
      "epoch 37 | loss: 0.04456 |  0:00:58s\n",
      "epoch 38 | loss: 0.04985 |  0:00:59s\n",
      "epoch 39 | loss: 0.0513  |  0:01:01s\n",
      "epoch 40 | loss: 0.05072 |  0:01:03s\n",
      "epoch 41 | loss: 0.05017 |  0:01:04s\n",
      "epoch 42 | loss: 0.05147 |  0:01:06s\n",
      "epoch 43 | loss: 0.04708 |  0:01:07s\n",
      "epoch 44 | loss: 0.04346 |  0:01:09s\n",
      "epoch 45 | loss: 0.04096 |  0:01:10s\n",
      "epoch 46 | loss: 0.04079 |  0:01:12s\n",
      "epoch 47 | loss: 0.03744 |  0:01:14s\n",
      "epoch 48 | loss: 0.05244 |  0:01:15s\n",
      "epoch 49 | loss: 0.0456  |  0:01:17s\n",
      "epoch 50 | loss: 0.04248 |  0:01:18s\n",
      "epoch 51 | loss: 0.04284 |  0:01:20s\n",
      "epoch 52 | loss: 0.03957 |  0:01:21s\n",
      "epoch 53 | loss: 0.03722 |  0:01:23s\n",
      "epoch 54 | loss: 0.03487 |  0:01:24s\n",
      "epoch 55 | loss: 0.03411 |  0:01:26s\n",
      "epoch 56 | loss: 0.0327  |  0:01:28s\n",
      "epoch 57 | loss: 0.03288 |  0:01:29s\n",
      "epoch 58 | loss: 0.03713 |  0:01:31s\n",
      "epoch 59 | loss: 0.03813 |  0:01:33s\n",
      "epoch 60 | loss: 0.03832 |  0:01:34s\n",
      "epoch 61 | loss: 0.05194 |  0:01:36s\n",
      "epoch 62 | loss: 0.04869 |  0:01:38s\n",
      "epoch 63 | loss: 0.04335 |  0:01:39s\n",
      "epoch 64 | loss: 0.03853 |  0:01:41s\n",
      "epoch 65 | loss: 0.03588 |  0:01:42s\n",
      "epoch 66 | loss: 0.03444 |  0:01:44s\n",
      "epoch 67 | loss: 0.035   |  0:01:45s\n",
      "epoch 68 | loss: 0.03111 |  0:01:47s\n",
      "epoch 69 | loss: 0.03654 |  0:01:48s\n",
      "epoch 70 | loss: 0.03901 |  0:01:50s\n",
      "epoch 71 | loss: 0.03774 |  0:01:52s\n",
      "epoch 72 | loss: 0.0349  |  0:01:53s\n",
      "epoch 73 | loss: 0.03507 |  0:01:55s\n",
      "epoch 74 | loss: 0.03223 |  0:01:56s\n",
      "epoch 75 | loss: 0.03229 |  0:01:59s\n",
      "epoch 76 | loss: 0.02947 |  0:02:00s\n",
      "epoch 77 | loss: 0.0279  |  0:02:02s\n",
      "epoch 78 | loss: 0.02897 |  0:02:04s\n",
      "epoch 79 | loss: 0.02786 |  0:02:05s\n",
      "epoch 80 | loss: 0.0268  |  0:02:07s\n",
      "epoch 81 | loss: 0.02547 |  0:02:10s\n",
      "epoch 82 | loss: 0.02357 |  0:02:13s\n",
      "epoch 83 | loss: 0.02413 |  0:02:15s\n",
      "epoch 84 | loss: 0.02479 |  0:02:18s\n",
      "epoch 85 | loss: 0.02625 |  0:02:21s\n",
      "epoch 86 | loss: 0.02476 |  0:02:23s\n",
      "epoch 87 | loss: 0.02566 |  0:02:26s\n",
      "epoch 88 | loss: 0.02462 |  0:02:28s\n",
      "epoch 89 | loss: 0.02255 |  0:02:31s\n",
      "epoch 90 | loss: 0.0259  |  0:02:33s\n",
      "epoch 91 | loss: 0.02525 |  0:02:35s\n",
      "epoch 92 | loss: 0.02384 |  0:02:36s\n",
      "epoch 93 | loss: 0.02465 |  0:02:39s\n",
      "epoch 94 | loss: 0.02279 |  0:02:42s\n",
      "epoch 95 | loss: 0.02191 |  0:02:44s\n",
      "epoch 96 | loss: 0.02345 |  0:02:46s\n",
      "epoch 97 | loss: 0.0212  |  0:02:48s\n",
      "epoch 98 | loss: 0.0271  |  0:02:50s\n",
      "epoch 99 | loss: 0.02506 |  0:02:52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.28346 |  0:00:02s\n",
      "epoch 1  | loss: 0.1047  |  0:00:04s\n",
      "epoch 2  | loss: 0.09067 |  0:00:06s\n",
      "epoch 3  | loss: 0.08686 |  0:00:08s\n",
      "epoch 4  | loss: 0.08227 |  0:00:10s\n",
      "epoch 5  | loss: 0.07929 |  0:00:12s\n",
      "epoch 6  | loss: 0.08166 |  0:00:14s\n",
      "epoch 7  | loss: 0.08419 |  0:00:16s\n",
      "epoch 8  | loss: 0.07814 |  0:00:18s\n",
      "epoch 9  | loss: 0.07458 |  0:00:20s\n",
      "epoch 10 | loss: 0.0736  |  0:00:22s\n",
      "epoch 11 | loss: 0.07116 |  0:00:24s\n",
      "epoch 12 | loss: 0.07101 |  0:00:26s\n",
      "epoch 13 | loss: 0.07467 |  0:00:28s\n",
      "epoch 14 | loss: 0.07088 |  0:00:30s\n",
      "epoch 15 | loss: 0.06909 |  0:00:32s\n",
      "epoch 16 | loss: 0.0733  |  0:00:34s\n",
      "epoch 17 | loss: 0.07079 |  0:00:36s\n",
      "epoch 18 | loss: 0.07079 |  0:00:38s\n",
      "epoch 19 | loss: 0.06971 |  0:00:40s\n",
      "epoch 20 | loss: 0.07141 |  0:00:43s\n",
      "epoch 21 | loss: 0.07851 |  0:00:45s\n",
      "epoch 22 | loss: 0.07127 |  0:00:48s\n",
      "epoch 23 | loss: 0.07479 |  0:00:50s\n",
      "epoch 24 | loss: 0.07152 |  0:00:52s\n",
      "epoch 25 | loss: 0.07018 |  0:00:54s\n",
      "epoch 26 | loss: 0.06876 |  0:00:56s\n",
      "epoch 27 | loss: 0.06698 |  0:00:58s\n",
      "epoch 28 | loss: 0.06679 |  0:01:00s\n",
      "epoch 29 | loss: 0.06614 |  0:01:02s\n",
      "epoch 30 | loss: 0.06638 |  0:01:04s\n",
      "epoch 31 | loss: 0.06677 |  0:01:06s\n",
      "epoch 32 | loss: 0.06329 |  0:01:08s\n",
      "epoch 33 | loss: 0.06223 |  0:01:10s\n",
      "epoch 34 | loss: 0.06451 |  0:01:12s\n",
      "epoch 35 | loss: 0.06386 |  0:01:14s\n",
      "epoch 36 | loss: 0.06149 |  0:01:16s\n",
      "epoch 37 | loss: 0.06159 |  0:01:18s\n",
      "epoch 38 | loss: 0.06021 |  0:01:20s\n",
      "epoch 39 | loss: 0.05908 |  0:01:22s\n",
      "epoch 40 | loss: 0.05961 |  0:01:24s\n",
      "epoch 41 | loss: 0.05634 |  0:01:26s\n",
      "epoch 42 | loss: 0.05555 |  0:01:28s\n",
      "epoch 43 | loss: 0.0557  |  0:01:30s\n",
      "epoch 44 | loss: 0.05717 |  0:01:32s\n",
      "epoch 45 | loss: 0.05754 |  0:01:34s\n",
      "epoch 46 | loss: 0.05643 |  0:01:36s\n",
      "epoch 47 | loss: 0.05976 |  0:01:38s\n",
      "epoch 48 | loss: 0.05585 |  0:01:40s\n",
      "epoch 49 | loss: 0.05672 |  0:01:41s\n",
      "epoch 50 | loss: 0.05624 |  0:01:43s\n",
      "epoch 51 | loss: 0.05691 |  0:01:45s\n",
      "epoch 52 | loss: 0.05943 |  0:01:47s\n",
      "epoch 53 | loss: 0.06232 |  0:01:49s\n",
      "epoch 54 | loss: 0.06324 |  0:01:51s\n",
      "epoch 55 | loss: 0.05897 |  0:01:53s\n",
      "epoch 56 | loss: 0.06023 |  0:01:55s\n",
      "epoch 57 | loss: 0.06129 |  0:01:57s\n",
      "epoch 58 | loss: 0.06406 |  0:01:59s\n",
      "epoch 59 | loss: 0.06078 |  0:02:01s\n",
      "epoch 60 | loss: 0.05992 |  0:02:03s\n",
      "epoch 61 | loss: 0.06086 |  0:02:05s\n",
      "epoch 62 | loss: 0.05714 |  0:02:07s\n",
      "epoch 63 | loss: 0.05731 |  0:02:09s\n",
      "epoch 64 | loss: 0.05555 |  0:02:12s\n",
      "epoch 65 | loss: 0.05598 |  0:02:14s\n",
      "epoch 66 | loss: 0.05496 |  0:02:16s\n",
      "epoch 67 | loss: 0.05455 |  0:02:18s\n",
      "epoch 68 | loss: 0.05237 |  0:02:21s\n",
      "epoch 69 | loss: 0.0522  |  0:02:23s\n",
      "epoch 70 | loss: 0.05307 |  0:02:25s\n",
      "epoch 71 | loss: 0.05283 |  0:02:28s\n",
      "epoch 72 | loss: 0.06004 |  0:02:30s\n",
      "epoch 73 | loss: 0.05826 |  0:02:32s\n",
      "epoch 74 | loss: 0.05552 |  0:02:34s\n",
      "epoch 75 | loss: 0.05331 |  0:02:36s\n",
      "epoch 76 | loss: 0.0527  |  0:02:38s\n",
      "epoch 77 | loss: 0.051   |  0:02:40s\n",
      "epoch 78 | loss: 0.05125 |  0:02:42s\n",
      "epoch 79 | loss: 0.04954 |  0:02:44s\n",
      "epoch 80 | loss: 0.04956 |  0:02:46s\n",
      "epoch 81 | loss: 0.04969 |  0:02:48s\n",
      "epoch 82 | loss: 0.04812 |  0:02:50s\n",
      "epoch 83 | loss: 0.04741 |  0:02:53s\n",
      "epoch 84 | loss: 0.04756 |  0:02:55s\n",
      "epoch 85 | loss: 0.0459  |  0:02:57s\n",
      "epoch 86 | loss: 0.04648 |  0:02:59s\n",
      "epoch 87 | loss: 0.04704 |  0:03:00s\n",
      "epoch 88 | loss: 0.04824 |  0:03:02s\n",
      "epoch 89 | loss: 0.04737 |  0:03:04s\n",
      "epoch 90 | loss: 0.04932 |  0:03:06s\n",
      "epoch 91 | loss: 0.04723 |  0:03:08s\n",
      "epoch 92 | loss: 0.04713 |  0:03:10s\n",
      "epoch 93 | loss: 0.04514 |  0:03:12s\n",
      "epoch 94 | loss: 0.0455  |  0:03:14s\n",
      "epoch 95 | loss: 0.05303 |  0:03:16s\n",
      "epoch 96 | loss: 0.04883 |  0:03:18s\n",
      "epoch 97 | loss: 0.04835 |  0:03:20s\n",
      "epoch 98 | loss: 0.04589 |  0:03:22s\n",
      "epoch 99 | loss: 0.04736 |  0:03:24s\n",
      "precision: 0.872\n",
      "recall: 0.801\n",
      "f1: 0.830\n",
      "auc_roc: 0.801\n"
     ]
    }
   ],
   "source": [
    "from multiscorer import MultiScorer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import average\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# define pipeline\n",
    "steps = [('under', RandomUnderSampler(sampling_strategy=0.03)), ('model', TabNetClassifier())]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "cv = StratifiedKFold(n_splits=10, random_state=44, shuffle=True)\n",
    "\n",
    "scorer = MultiScorer({                                               # Create a MultiScorer instance\n",
    "    'precision': (precision_score, {'average': 'macro'}),              # Param 'average' will be passed to precision_score as kwarg \n",
    "    'recall': (recall_score, {'average': 'macro'}),\n",
    "    'f1': (f1_score, {'average': 'macro'}),\n",
    "    'auc_roc': (roc_auc_score, {'average': 'macro'})\n",
    "})\n",
    "\n",
    "\n",
    "cross_val_score(pipeline, features_std, labels, scoring=scorer, cv=cv)               # Use the function with our socrer. Ignore its result \n",
    "\n",
    "results = scorer.get_results()                                       # Get a dict of lists containing the scores for each metric\n",
    "\n",
    "for metric in results.keys():                                        # Iterate and use the results\n",
    "  print(\"%s: %.3f\" % (metric, average(results[metric])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.27813 |  0:00:02s\n",
      "epoch 1  | loss: 0.11009 |  0:00:04s\n",
      "epoch 2  | loss: 0.10027 |  0:00:06s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_96111/2697974874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'precision_macro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recall_macro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f1_macro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;31m# Apply predict epoch to all eval sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_train_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0msteps_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt_transformers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             M_loss += torch.mean(\n\u001b[1;32m    162\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, priors, processed_feat)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_buffers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_buffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_modules'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m             \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_modules'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# example of evaluating a decision tree with random undersampling\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# define pipeline\n",
    "steps = [('under', RandomUnderSampler(sampling_strategy=0.03)), ('model', TabNetClassifier(device_name='cuda'))]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# evaluate pipeline\n",
    "scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'roc_auc']\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "scores = cross_validate(pipeline, features_std, labels, scoring=scoring, cv=cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8687 precision\n",
      "0.8041 recall\n",
      "0.8320 f1 score\n",
      "0.9348 auc roc\n",
      "131.21145 training time\n",
      "0.19298 predition time\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.4f precision\" % (scores['test_precision_macro'].mean()))\n",
    "print(\"%0.4f recall\" % (scores['test_recall_macro'].mean()))\n",
    "print(\"%0.4f f1 score\" % (scores['test_f1_macro'].mean()))\n",
    "print(\"%0.4f auc roc\" % (scores['test_roc_auc'].mean()))\n",
    "\n",
    "print(\"%0.5f training time\" % (scores['fit_time'].mean()))\n",
    "print(\"%0.5f predition time\" % (scores['score_time'].mean()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
