{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T02:18:02.234513Z",
     "iopub.status.busy": "2023-03-04T02:18:02.233784Z",
     "iopub.status.idle": "2023-03-04T02:18:03.477602Z",
     "shell.execute_reply": "2023-03-04T02:18:03.475896Z",
     "shell.execute_reply.started": "2023-03-04T02:18:02.234448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas is used for data manipulation\n",
    "# Read in data and display first 5 rows\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_df = pd.read_pickle(\"../dataset/apimdsFeuturesExtracted.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('../dataset/apimdsFinalissimo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "listToOneHot = list(df3.columns.values)\n",
    "listToOneHot.remove('malware')\n",
    "\n",
    "malwareColTrain = df3['malware'].copy()\n",
    "df3 = df3.drop('malware', axis = 1)\n",
    "\n",
    "dummyDf = pd.get_dummies(df3, listToOneHot, dummy_na=True)\n",
    "dummyDf.head(15)\n",
    "\n",
    "dummyDf.drop('116_nan', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "malwareColTrain = df3['malware'].copy()\n",
    "df3 = df3.drop('malware', axis = 1)\n",
    "\n",
    "cols= df3.columns\n",
    "\n",
    "for c in cols:\n",
    "   fh = FeatureHasher(n_features=114, input_type='string')\n",
    "   hash_train = fh.transform(df3[[c]].astype(str).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "malwareColTrainPCA = PCA_df['malware'].copy()\n",
    "PCA_df = PCA_df.drop('malware', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "dummyDf = zscore(dummyDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create a scaler object\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "df_std = std_scaler.fit_transform(dummyDf)\n",
    "\n",
    "#from scipy.stats import zscore\n",
    "\n",
    "#dummyDf = dummyDf.apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dummyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_tabnet\n",
      "  Obtaining dependency information for pytorch_tabnet from https://files.pythonhosted.org/packages/0f/92/ed98b89b7cf5661656daa4cc88e578f712eb5eae41b8f46a56c1ece3a895/pytorch_tabnet-4.1.0-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from pytorch_tabnet) (1.21.6)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from pytorch_tabnet) (1.3.0)\n",
      "Requirement already satisfied: scipy>1.4 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from pytorch_tabnet) (1.11.1)\n",
      "Requirement already satisfied: torch>=1.3 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from pytorch_tabnet) (1.11.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from pytorch_tabnet) (4.65.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from scikit_learn>0.21->pytorch_tabnet) (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from scikit_learn>0.21->pytorch_tabnet) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from torch>=1.3->pytorch_tabnet) (4.7.1)\n",
      "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytorch_tabnet\n",
      "Successfully installed pytorch_tabnet-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytorch_tabnet\n",
      "  Obtaining dependency information for pytorch_tabnet from https://files.pythonhosted.org/packages/0f/92/ed98b89b7cf5661656daa4cc88e578f712eb5eae41b8f46a56c1ece3a895/pytorch_tabnet-4.1.0-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3.11/site-packages (from pytorch_tabnet) (1.25.1)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /usr/lib/python3.11/site-packages (from pytorch_tabnet) (1.3.0)\n",
      "Requirement already satisfied: scipy>1.4 in /usr/lib/python3.11/site-packages (from pytorch_tabnet) (1.11.2)\n",
      "Collecting torch>=1.3 (from pytorch_tabnet)\n",
      "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.36 (from pytorch_tabnet)\n",
      "  Obtaining dependency information for tqdm>=4.36 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/lib/python3.11/site-packages (from scikit_learn>0.21->pytorch_tabnet) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/lib/python3.11/site-packages (from scikit_learn>0.21->pytorch_tabnet) (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.11/site-packages (from torch>=1.3->pytorch_tabnet) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/lib/python3.11/site-packages (from torch>=1.3->pytorch_tabnet) (4.7.1)\n",
      "Collecting sympy (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3.11/site-packages (from torch>=1.3->pytorch_tabnet) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0 (from torch>=1.3->pytorch_tabnet)\n",
      "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (68.0.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (0.40.0)\n",
      "Collecting cmake (from triton==2.0.0->torch>=1.3->pytorch_tabnet)\n",
      "  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/2e/51/3a4672a819b4532a378bfefad8f886cfe71057556e0d4eefb64523fd370a/cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: lit in /usr/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.3->pytorch_tabnet) (15.0.7.dev0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3.11/site-packages (from jinja2->torch>=1.3->pytorch_tabnet) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.3->pytorch_tabnet)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs in /usr/lib/python3.11/site-packages (from setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (3.10.0)\n",
      "Requirement already satisfied: jaraco.text in /usr/lib/python3.11/site-packages (from setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (3.11.1)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3.11/site-packages (from setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (10.1.0)\n",
      "Requirement already satisfied: ordered-set in /usr/lib/python3.11/site-packages (from setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (4.1.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.11/site-packages (from setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (23.1)\n",
      "Requirement already satisfied: tomli in /usr/lib/python3.11/site-packages (from setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (2.0.1)\n",
      "Requirement already satisfied: validate-pyproject in /usr/lib/python3.11/site-packages (from setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (0.13.post1.dev0+gb752273.d20230520)\n",
      "Requirement already satisfied: jaraco.functools in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (3.8.1)\n",
      "Requirement already satisfied: jaraco.context>=4.1 in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (4.3.0)\n",
      "Requirement already satisfied: autocommand in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (2.2.2)\n",
      "Requirement already satisfied: inflect in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (7.0.0)\n",
      "Requirement already satisfied: fastjsonschema<=3,>=2.16.2 in /usr/lib/python3.11/site-packages (from validate-pyproject->setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (2.18.0)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /usr/lib/python3.11/site-packages (from inflect->jaraco.text->setuptools->nvidia-cublas-cu11==11.10.3.66->torch>=1.3->pytorch_tabnet) (1.10.9)\n",
      "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, cmake, tqdm, sympy, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, networkx, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, pytorch_tabnet\n",
      "Successfully installed cmake-3.27.2 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pytorch_tabnet-4.1.0 sympy-1.12 torch-2.0.1 tqdm-4.66.1 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_tabnet --break-system-package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Using {}\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessio/.local/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/.local/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/alessio/.local/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "from multiscorer import MultiScorer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import average\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# define pipeline\n",
    "steps = [('under', RandomUnderSampler(sampling_strategy=0.03)), ('model', TabNetClassifier(device_name='cpu', verbose=3))]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "cv = StratifiedKFold(n_splits=10, random_state=44, shuffle=True)\n",
    "\n",
    "scorer = MultiScorer({                                               # Create a MultiScorer instance\n",
    "    'precision': (precision_score, {'average': 'macro'}),              # Param 'average' will be passed to precision_score as kwarg \n",
    "    'recall': (recall_score, {'average': 'macro'}),\n",
    "    'f1': (f1_score, {'average': 'macro'}),\n",
    "    'auc_roc': (roc_auc_score, {'average': 'macro'})\n",
    "})\n",
    "\n",
    "\n",
    "cross_val_score(pipeline, df_std, malwareColTrain, scoring=scorer, cv=cv)               # Use the function with our socrer. Ignore its result \n",
    "\n",
    "results = scorer.get_results()                                       # Get a dict of lists containing the scores for each metric\n",
    "\n",
    "for metric in results.keys():                                        # Iterate and use the results\n",
    "  print(\"%s: %.4f\" % (metric, average(results[metric])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Using {}\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T02:21:26.648062Z",
     "iopub.status.busy": "2023-03-04T02:21:26.646055Z",
     "iopub.status.idle": "2023-03-04T02:21:29.161687Z",
     "shell.execute_reply": "2023-03-04T02:21:29.160325Z",
     "shell.execute_reply.started": "2023-03-04T02:21:26.647956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.039   |  0:00:00s\n",
      "epoch 1  | loss: 0.77143 |  0:00:00s\n",
      "epoch 2  | loss: 0.55877 |  0:00:00s\n",
      "epoch 3  | loss: 0.44797 |  0:00:00s\n",
      "epoch 4  | loss: 0.38976 |  0:00:00s\n",
      "epoch 5  | loss: 0.34343 |  0:00:00s\n",
      "epoch 6  | loss: 0.31869 |  0:00:00s\n",
      "epoch 7  | loss: 0.32083 |  0:00:00s\n",
      "epoch 8  | loss: 0.33398 |  0:00:01s\n",
      "epoch 9  | loss: 0.30638 |  0:00:01s\n",
      "epoch 10 | loss: 0.305   |  0:00:01s\n",
      "epoch 11 | loss: 0.29344 |  0:00:01s\n",
      "epoch 12 | loss: 0.30666 |  0:00:01s\n",
      "epoch 13 | loss: 0.29611 |  0:00:01s\n",
      "epoch 14 | loss: 0.2892  |  0:00:01s\n",
      "epoch 15 | loss: 0.27532 |  0:00:01s\n",
      "epoch 16 | loss: 0.25636 |  0:00:02s\n",
      "epoch 17 | loss: 0.24878 |  0:00:02s\n",
      "epoch 18 | loss: 0.23206 |  0:00:02s\n",
      "epoch 19 | loss: 0.22631 |  0:00:02s\n",
      "epoch 20 | loss: 0.21711 |  0:00:02s\n",
      "epoch 21 | loss: 0.20601 |  0:00:02s\n",
      "epoch 22 | loss: 0.19706 |  0:00:02s\n",
      "epoch 23 | loss: 0.20013 |  0:00:02s\n",
      "epoch 24 | loss: 0.17796 |  0:00:03s\n",
      "epoch 25 | loss: 0.17186 |  0:00:03s\n",
      "epoch 26 | loss: 0.1549  |  0:00:03s\n",
      "epoch 27 | loss: 0.15328 |  0:00:03s\n",
      "epoch 28 | loss: 0.13866 |  0:00:03s\n",
      "epoch 29 | loss: 0.14296 |  0:00:03s\n",
      "epoch 30 | loss: 0.13    |  0:00:03s\n",
      "epoch 31 | loss: 0.1171  |  0:00:03s\n",
      "epoch 32 | loss: 0.11664 |  0:00:03s\n",
      "epoch 33 | loss: 0.11398 |  0:00:04s\n",
      "epoch 34 | loss: 0.10703 |  0:00:04s\n",
      "epoch 35 | loss: 0.10665 |  0:00:04s\n",
      "epoch 36 | loss: 0.09496 |  0:00:04s\n",
      "epoch 37 | loss: 0.08854 |  0:00:04s\n",
      "epoch 38 | loss: 0.08279 |  0:00:04s\n",
      "epoch 39 | loss: 0.09316 |  0:00:04s\n",
      "epoch 40 | loss: 0.08751 |  0:00:04s\n",
      "epoch 41 | loss: 0.08236 |  0:00:05s\n",
      "epoch 42 | loss: 0.08056 |  0:00:05s\n",
      "epoch 43 | loss: 0.06785 |  0:00:05s\n",
      "epoch 44 | loss: 0.06631 |  0:00:05s\n",
      "epoch 45 | loss: 0.06798 |  0:00:05s\n",
      "epoch 46 | loss: 0.06246 |  0:00:05s\n",
      "epoch 47 | loss: 0.06008 |  0:00:05s\n",
      "epoch 48 | loss: 0.06828 |  0:00:05s\n",
      "epoch 49 | loss: 0.06002 |  0:00:05s\n",
      "epoch 50 | loss: 0.0522  |  0:00:06s\n",
      "epoch 51 | loss: 0.05369 |  0:00:06s\n",
      "epoch 52 | loss: 0.05267 |  0:00:06s\n",
      "epoch 53 | loss: 0.048   |  0:00:06s\n",
      "epoch 54 | loss: 0.049   |  0:00:06s\n",
      "epoch 55 | loss: 0.04745 |  0:00:06s\n",
      "epoch 56 | loss: 0.04558 |  0:00:06s\n",
      "epoch 57 | loss: 0.04297 |  0:00:06s\n",
      "epoch 58 | loss: 0.04374 |  0:00:06s\n",
      "epoch 59 | loss: 0.0421  |  0:00:07s\n",
      "epoch 60 | loss: 0.04347 |  0:00:07s\n",
      "epoch 61 | loss: 0.04041 |  0:00:07s\n",
      "epoch 62 | loss: 0.04159 |  0:00:07s\n",
      "epoch 63 | loss: 0.04074 |  0:00:07s\n",
      "epoch 64 | loss: 0.04019 |  0:00:07s\n",
      "epoch 65 | loss: 0.0395  |  0:00:07s\n",
      "epoch 66 | loss: 0.03676 |  0:00:07s\n",
      "epoch 67 | loss: 0.03653 |  0:00:07s\n",
      "epoch 68 | loss: 0.03873 |  0:00:08s\n",
      "epoch 69 | loss: 0.03516 |  0:00:08s\n",
      "epoch 70 | loss: 0.03811 |  0:00:08s\n",
      "epoch 71 | loss: 0.03689 |  0:00:08s\n",
      "epoch 72 | loss: 0.03904 |  0:00:08s\n",
      "epoch 73 | loss: 0.04032 |  0:00:08s\n",
      "epoch 74 | loss: 0.03594 |  0:00:08s\n",
      "epoch 75 | loss: 0.03616 |  0:00:08s\n",
      "epoch 76 | loss: 0.03562 |  0:00:08s\n",
      "epoch 77 | loss: 0.03524 |  0:00:09s\n",
      "epoch 78 | loss: 0.03393 |  0:00:09s\n",
      "epoch 79 | loss: 0.03351 |  0:00:09s\n",
      "epoch 80 | loss: 0.03657 |  0:00:09s\n",
      "epoch 81 | loss: 0.03833 |  0:00:09s\n",
      "epoch 82 | loss: 0.03099 |  0:00:09s\n",
      "epoch 83 | loss: 0.03319 |  0:00:09s\n",
      "epoch 84 | loss: 0.03268 |  0:00:09s\n",
      "epoch 85 | loss: 0.03115 |  0:00:10s\n",
      "epoch 86 | loss: 0.0355  |  0:00:10s\n",
      "epoch 87 | loss: 0.03501 |  0:00:10s\n",
      "epoch 88 | loss: 0.03732 |  0:00:10s\n",
      "epoch 89 | loss: 0.03274 |  0:00:10s\n",
      "epoch 90 | loss: 0.03379 |  0:00:10s\n",
      "epoch 91 | loss: 0.03342 |  0:00:10s\n",
      "epoch 92 | loss: 0.03571 |  0:00:10s\n",
      "epoch 93 | loss: 0.03124 |  0:00:10s\n",
      "epoch 94 | loss: 0.03111 |  0:00:11s\n",
      "epoch 95 | loss: 0.03135 |  0:00:11s\n",
      "epoch 96 | loss: 0.03106 |  0:00:11s\n",
      "epoch 97 | loss: 0.03508 |  0:00:11s\n",
      "epoch 98 | loss: 0.0321  |  0:00:11s\n",
      "epoch 99 | loss: 0.0316  |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.0793  |  0:00:00s\n",
      "epoch 1  | loss: 0.7738  |  0:00:00s\n",
      "epoch 2  | loss: 0.57169 |  0:00:00s\n",
      "epoch 3  | loss: 0.44636 |  0:00:00s\n",
      "epoch 4  | loss: 0.36937 |  0:00:00s\n",
      "epoch 5  | loss: 0.33423 |  0:00:00s\n",
      "epoch 6  | loss: 0.31716 |  0:00:00s\n",
      "epoch 7  | loss: 0.34026 |  0:00:00s\n",
      "epoch 8  | loss: 0.34032 |  0:00:01s\n",
      "epoch 9  | loss: 0.31167 |  0:00:01s\n",
      "epoch 10 | loss: 0.29909 |  0:00:01s\n",
      "epoch 11 | loss: 0.28765 |  0:00:01s\n",
      "epoch 12 | loss: 0.27451 |  0:00:01s\n",
      "epoch 13 | loss: 0.26881 |  0:00:01s\n",
      "epoch 14 | loss: 0.25837 |  0:00:01s\n",
      "epoch 15 | loss: 0.24556 |  0:00:01s\n",
      "epoch 16 | loss: 0.23697 |  0:00:02s\n",
      "epoch 17 | loss: 0.21609 |  0:00:02s\n",
      "epoch 18 | loss: 0.21116 |  0:00:02s\n",
      "epoch 19 | loss: 0.19131 |  0:00:02s\n",
      "epoch 20 | loss: 0.18211 |  0:00:02s\n",
      "epoch 21 | loss: 0.1705  |  0:00:02s\n",
      "epoch 22 | loss: 0.16779 |  0:00:02s\n",
      "epoch 23 | loss: 0.15095 |  0:00:02s\n",
      "epoch 24 | loss: 0.13441 |  0:00:02s\n",
      "epoch 25 | loss: 0.1213  |  0:00:03s\n",
      "epoch 26 | loss: 0.10245 |  0:00:03s\n",
      "epoch 27 | loss: 0.09478 |  0:00:03s\n",
      "epoch 28 | loss: 0.08567 |  0:00:03s\n",
      "epoch 29 | loss: 0.08391 |  0:00:03s\n",
      "epoch 30 | loss: 0.06753 |  0:00:03s\n",
      "epoch 31 | loss: 0.06979 |  0:00:03s\n",
      "epoch 32 | loss: 0.06967 |  0:00:03s\n",
      "epoch 33 | loss: 0.05765 |  0:00:04s\n",
      "epoch 34 | loss: 0.06707 |  0:00:04s\n",
      "epoch 35 | loss: 0.06359 |  0:00:04s\n",
      "epoch 36 | loss: 0.04753 |  0:00:04s\n",
      "epoch 37 | loss: 0.05284 |  0:00:04s\n",
      "epoch 38 | loss: 0.0484  |  0:00:04s\n",
      "epoch 39 | loss: 0.04951 |  0:00:04s\n",
      "epoch 40 | loss: 0.05105 |  0:00:05s\n",
      "epoch 41 | loss: 0.04652 |  0:00:05s\n",
      "epoch 42 | loss: 0.04406 |  0:00:05s\n",
      "epoch 43 | loss: 0.04238 |  0:00:05s\n",
      "epoch 44 | loss: 0.04171 |  0:00:05s\n",
      "epoch 45 | loss: 0.04684 |  0:00:05s\n",
      "epoch 46 | loss: 0.04049 |  0:00:05s\n",
      "epoch 47 | loss: 0.0395  |  0:00:05s\n",
      "epoch 48 | loss: 0.04318 |  0:00:06s\n",
      "epoch 49 | loss: 0.03651 |  0:00:06s\n",
      "epoch 50 | loss: 0.03454 |  0:00:06s\n",
      "epoch 51 | loss: 0.03856 |  0:00:06s\n",
      "epoch 52 | loss: 0.03459 |  0:00:06s\n",
      "epoch 53 | loss: 0.04413 |  0:00:06s\n",
      "epoch 54 | loss: 0.03823 |  0:00:06s\n",
      "epoch 55 | loss: 0.03653 |  0:00:06s\n",
      "epoch 56 | loss: 0.03611 |  0:00:07s\n",
      "epoch 57 | loss: 0.0345  |  0:00:07s\n",
      "epoch 58 | loss: 0.0385  |  0:00:07s\n",
      "epoch 59 | loss: 0.03308 |  0:00:07s\n",
      "epoch 60 | loss: 0.03543 |  0:00:07s\n",
      "epoch 61 | loss: 0.03408 |  0:00:07s\n",
      "epoch 62 | loss: 0.03549 |  0:00:07s\n",
      "epoch 63 | loss: 0.03711 |  0:00:07s\n",
      "epoch 64 | loss: 0.03587 |  0:00:08s\n",
      "epoch 65 | loss: 0.03224 |  0:00:08s\n",
      "epoch 66 | loss: 0.03644 |  0:00:08s\n",
      "epoch 67 | loss: 0.03246 |  0:00:08s\n",
      "epoch 68 | loss: 0.03329 |  0:00:08s\n",
      "epoch 69 | loss: 0.03237 |  0:00:08s\n",
      "epoch 70 | loss: 0.03779 |  0:00:08s\n",
      "epoch 71 | loss: 0.03488 |  0:00:08s\n",
      "epoch 72 | loss: 0.03287 |  0:00:09s\n",
      "epoch 73 | loss: 0.03472 |  0:00:09s\n",
      "epoch 74 | loss: 0.03287 |  0:00:09s\n",
      "epoch 75 | loss: 0.03205 |  0:00:09s\n",
      "epoch 76 | loss: 0.03291 |  0:00:09s\n",
      "epoch 77 | loss: 0.03819 |  0:00:09s\n",
      "epoch 78 | loss: 0.03308 |  0:00:10s\n",
      "epoch 79 | loss: 0.03093 |  0:00:10s\n",
      "epoch 80 | loss: 0.03271 |  0:00:10s\n",
      "epoch 81 | loss: 0.03171 |  0:00:10s\n",
      "epoch 82 | loss: 0.03223 |  0:00:10s\n",
      "epoch 83 | loss: 0.03511 |  0:00:10s\n",
      "epoch 84 | loss: 0.03601 |  0:00:10s\n",
      "epoch 85 | loss: 0.03094 |  0:00:10s\n",
      "epoch 86 | loss: 0.03207 |  0:00:11s\n",
      "epoch 87 | loss: 0.03082 |  0:00:11s\n",
      "epoch 88 | loss: 0.03208 |  0:00:11s\n",
      "epoch 89 | loss: 0.03395 |  0:00:11s\n",
      "epoch 90 | loss: 0.03083 |  0:00:11s\n",
      "epoch 91 | loss: 0.03194 |  0:00:11s\n",
      "epoch 92 | loss: 0.03215 |  0:00:11s\n",
      "epoch 93 | loss: 0.03655 |  0:00:12s\n",
      "epoch 94 | loss: 0.03169 |  0:00:12s\n",
      "epoch 95 | loss: 0.03184 |  0:00:12s\n",
      "epoch 96 | loss: 0.03039 |  0:00:12s\n",
      "epoch 97 | loss: 0.03304 |  0:00:12s\n",
      "epoch 98 | loss: 0.03103 |  0:00:12s\n",
      "epoch 99 | loss: 0.0331  |  0:00:12s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.06992 |  0:00:00s\n",
      "epoch 1  | loss: 0.79332 |  0:00:00s\n",
      "epoch 2  | loss: 0.59171 |  0:00:00s\n",
      "epoch 3  | loss: 0.41922 |  0:00:00s\n",
      "epoch 4  | loss: 0.33578 |  0:00:00s\n",
      "epoch 5  | loss: 0.30741 |  0:00:00s\n",
      "epoch 6  | loss: 0.30221 |  0:00:00s\n",
      "epoch 7  | loss: 0.30082 |  0:00:01s\n",
      "epoch 8  | loss: 0.28697 |  0:00:01s\n",
      "epoch 9  | loss: 0.28561 |  0:00:01s\n",
      "epoch 10 | loss: 0.2864  |  0:00:01s\n",
      "epoch 11 | loss: 0.2838  |  0:00:01s\n",
      "epoch 12 | loss: 0.28163 |  0:00:01s\n",
      "epoch 13 | loss: 0.2755  |  0:00:01s\n",
      "epoch 14 | loss: 0.26461 |  0:00:02s\n",
      "epoch 15 | loss: 0.26886 |  0:00:02s\n",
      "epoch 16 | loss: 0.25634 |  0:00:02s\n",
      "epoch 17 | loss: 0.24389 |  0:00:02s\n",
      "epoch 18 | loss: 0.22813 |  0:00:02s\n",
      "epoch 19 | loss: 0.21918 |  0:00:02s\n",
      "epoch 20 | loss: 0.21357 |  0:00:02s\n",
      "epoch 21 | loss: 0.20894 |  0:00:02s\n",
      "epoch 22 | loss: 0.19241 |  0:00:02s\n",
      "epoch 23 | loss: 0.17515 |  0:00:03s\n",
      "epoch 24 | loss: 0.16091 |  0:00:03s\n",
      "epoch 25 | loss: 0.14894 |  0:00:03s\n",
      "epoch 26 | loss: 0.14839 |  0:00:03s\n",
      "epoch 27 | loss: 0.13135 |  0:00:03s\n",
      "epoch 28 | loss: 0.12128 |  0:00:03s\n",
      "epoch 29 | loss: 0.11523 |  0:00:03s\n",
      "epoch 30 | loss: 0.11331 |  0:00:03s\n",
      "epoch 31 | loss: 0.1106  |  0:00:04s\n",
      "epoch 32 | loss: 0.10367 |  0:00:04s\n",
      "epoch 33 | loss: 0.10017 |  0:00:04s\n",
      "epoch 34 | loss: 0.09169 |  0:00:04s\n",
      "epoch 35 | loss: 0.09713 |  0:00:04s\n",
      "epoch 36 | loss: 0.09301 |  0:00:04s\n",
      "epoch 37 | loss: 0.09313 |  0:00:04s\n",
      "epoch 38 | loss: 0.10729 |  0:00:05s\n",
      "epoch 39 | loss: 0.08819 |  0:00:05s\n",
      "epoch 40 | loss: 0.10043 |  0:00:05s\n",
      "epoch 41 | loss: 0.09134 |  0:00:05s\n",
      "epoch 42 | loss: 0.0909  |  0:00:05s\n",
      "epoch 43 | loss: 0.08772 |  0:00:05s\n",
      "epoch 44 | loss: 0.08613 |  0:00:05s\n",
      "epoch 45 | loss: 0.08537 |  0:00:05s\n",
      "epoch 46 | loss: 0.08441 |  0:00:05s\n",
      "epoch 47 | loss: 0.08191 |  0:00:06s\n",
      "epoch 48 | loss: 0.08013 |  0:00:06s\n",
      "epoch 49 | loss: 0.0845  |  0:00:06s\n",
      "epoch 50 | loss: 0.08253 |  0:00:06s\n",
      "epoch 51 | loss: 0.08179 |  0:00:06s\n",
      "epoch 52 | loss: 0.09087 |  0:00:06s\n",
      "epoch 53 | loss: 0.08486 |  0:00:06s\n",
      "epoch 54 | loss: 0.0817  |  0:00:06s\n",
      "epoch 55 | loss: 0.08643 |  0:00:06s\n",
      "epoch 56 | loss: 0.08111 |  0:00:07s\n",
      "epoch 57 | loss: 0.08141 |  0:00:07s\n",
      "epoch 58 | loss: 0.08811 |  0:00:07s\n",
      "epoch 59 | loss: 0.08046 |  0:00:07s\n",
      "epoch 60 | loss: 0.07619 |  0:00:07s\n",
      "epoch 61 | loss: 0.08257 |  0:00:07s\n",
      "epoch 62 | loss: 0.07234 |  0:00:07s\n",
      "epoch 63 | loss: 0.0674  |  0:00:07s\n",
      "epoch 64 | loss: 0.0622  |  0:00:08s\n",
      "epoch 65 | loss: 0.05561 |  0:00:08s\n",
      "epoch 66 | loss: 0.05106 |  0:00:08s\n",
      "epoch 67 | loss: 0.04735 |  0:00:08s\n",
      "epoch 68 | loss: 0.05489 |  0:00:08s\n",
      "epoch 69 | loss: 0.05344 |  0:00:08s\n",
      "epoch 70 | loss: 0.0644  |  0:00:08s\n",
      "epoch 71 | loss: 0.05568 |  0:00:08s\n",
      "epoch 72 | loss: 0.05868 |  0:00:08s\n",
      "epoch 73 | loss: 0.06591 |  0:00:09s\n",
      "epoch 74 | loss: 0.05382 |  0:00:09s\n",
      "epoch 75 | loss: 0.05677 |  0:00:09s\n",
      "epoch 76 | loss: 0.05335 |  0:00:09s\n",
      "epoch 77 | loss: 0.0496  |  0:00:09s\n",
      "epoch 78 | loss: 0.05478 |  0:00:09s\n",
      "epoch 79 | loss: 0.05218 |  0:00:09s\n",
      "epoch 80 | loss: 0.05079 |  0:00:09s\n",
      "epoch 81 | loss: 0.04665 |  0:00:09s\n",
      "epoch 82 | loss: 0.04897 |  0:00:10s\n",
      "epoch 83 | loss: 0.05129 |  0:00:10s\n",
      "epoch 84 | loss: 0.05281 |  0:00:10s\n",
      "epoch 85 | loss: 0.0513  |  0:00:10s\n",
      "epoch 86 | loss: 0.04494 |  0:00:10s\n",
      "epoch 87 | loss: 0.05332 |  0:00:10s\n",
      "epoch 88 | loss: 0.0481  |  0:00:10s\n",
      "epoch 89 | loss: 0.05153 |  0:00:10s\n",
      "epoch 90 | loss: 0.04649 |  0:00:10s\n",
      "epoch 91 | loss: 0.04897 |  0:00:11s\n",
      "epoch 92 | loss: 0.04762 |  0:00:11s\n",
      "epoch 93 | loss: 0.04314 |  0:00:11s\n",
      "epoch 94 | loss: 0.04476 |  0:00:11s\n",
      "epoch 95 | loss: 0.03965 |  0:00:11s\n",
      "epoch 96 | loss: 0.04018 |  0:00:11s\n",
      "epoch 97 | loss: 0.0394  |  0:00:11s\n",
      "epoch 98 | loss: 0.03931 |  0:00:11s\n",
      "epoch 99 | loss: 0.04241 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.05702 |  0:00:00s\n",
      "epoch 1  | loss: 0.72925 |  0:00:00s\n",
      "epoch 2  | loss: 0.54771 |  0:00:00s\n",
      "epoch 3  | loss: 0.42229 |  0:00:00s\n",
      "epoch 4  | loss: 0.36655 |  0:00:00s\n",
      "epoch 5  | loss: 0.33285 |  0:00:00s\n",
      "epoch 6  | loss: 0.33652 |  0:00:00s\n",
      "epoch 7  | loss: 0.33322 |  0:00:00s\n",
      "epoch 8  | loss: 0.3192  |  0:00:01s\n",
      "epoch 9  | loss: 0.29497 |  0:00:01s\n",
      "epoch 10 | loss: 0.30684 |  0:00:01s\n",
      "epoch 11 | loss: 0.2847  |  0:00:01s\n",
      "epoch 12 | loss: 0.2802  |  0:00:01s\n",
      "epoch 13 | loss: 0.26887 |  0:00:01s\n",
      "epoch 14 | loss: 0.27328 |  0:00:01s\n",
      "epoch 15 | loss: 0.26431 |  0:00:01s\n",
      "epoch 16 | loss: 0.26842 |  0:00:01s\n",
      "epoch 17 | loss: 0.25841 |  0:00:02s\n",
      "epoch 18 | loss: 0.2367  |  0:00:02s\n",
      "epoch 19 | loss: 0.22105 |  0:00:02s\n",
      "epoch 20 | loss: 0.22126 |  0:00:02s\n",
      "epoch 21 | loss: 0.20807 |  0:00:02s\n",
      "epoch 22 | loss: 0.20874 |  0:00:02s\n",
      "epoch 23 | loss: 0.19565 |  0:00:02s\n",
      "epoch 24 | loss: 0.17662 |  0:00:02s\n",
      "epoch 25 | loss: 0.17396 |  0:00:02s\n",
      "epoch 26 | loss: 0.15143 |  0:00:03s\n",
      "epoch 27 | loss: 0.16599 |  0:00:03s\n",
      "epoch 28 | loss: 0.13672 |  0:00:03s\n",
      "epoch 29 | loss: 0.12112 |  0:00:03s\n",
      "epoch 30 | loss: 0.09977 |  0:00:03s\n",
      "epoch 31 | loss: 0.09066 |  0:00:03s\n",
      "epoch 32 | loss: 0.0858  |  0:00:03s\n",
      "epoch 33 | loss: 0.08015 |  0:00:03s\n",
      "epoch 34 | loss: 0.07714 |  0:00:03s\n",
      "epoch 35 | loss: 0.07142 |  0:00:04s\n",
      "epoch 36 | loss: 0.07136 |  0:00:04s\n",
      "epoch 37 | loss: 0.06465 |  0:00:04s\n",
      "epoch 38 | loss: 0.06478 |  0:00:04s\n",
      "epoch 39 | loss: 0.05949 |  0:00:04s\n",
      "epoch 40 | loss: 0.05591 |  0:00:04s\n",
      "epoch 41 | loss: 0.05048 |  0:00:04s\n",
      "epoch 42 | loss: 0.05596 |  0:00:04s\n",
      "epoch 43 | loss: 0.04671 |  0:00:04s\n",
      "epoch 44 | loss: 0.04818 |  0:00:05s\n",
      "epoch 45 | loss: 0.04684 |  0:00:05s\n",
      "epoch 46 | loss: 0.04241 |  0:00:05s\n",
      "epoch 47 | loss: 0.0451  |  0:00:05s\n",
      "epoch 48 | loss: 0.04726 |  0:00:05s\n",
      "epoch 49 | loss: 0.04541 |  0:00:05s\n",
      "epoch 50 | loss: 0.03723 |  0:00:05s\n",
      "epoch 51 | loss: 0.04598 |  0:00:05s\n",
      "epoch 52 | loss: 0.04518 |  0:00:05s\n",
      "epoch 53 | loss: 0.0431  |  0:00:06s\n",
      "epoch 54 | loss: 0.03735 |  0:00:06s\n",
      "epoch 55 | loss: 0.04006 |  0:00:06s\n",
      "epoch 56 | loss: 0.04037 |  0:00:06s\n",
      "epoch 57 | loss: 0.03791 |  0:00:06s\n",
      "epoch 58 | loss: 0.04137 |  0:00:06s\n",
      "epoch 59 | loss: 0.03853 |  0:00:06s\n",
      "epoch 60 | loss: 0.03921 |  0:00:06s\n",
      "epoch 61 | loss: 0.03665 |  0:00:06s\n",
      "epoch 62 | loss: 0.03817 |  0:00:07s\n",
      "epoch 63 | loss: 0.03748 |  0:00:07s\n",
      "epoch 64 | loss: 0.03848 |  0:00:07s\n",
      "epoch 65 | loss: 0.03722 |  0:00:07s\n",
      "epoch 66 | loss: 0.03858 |  0:00:07s\n",
      "epoch 67 | loss: 0.03879 |  0:00:07s\n",
      "epoch 68 | loss: 0.03712 |  0:00:07s\n",
      "epoch 69 | loss: 0.04557 |  0:00:07s\n",
      "epoch 70 | loss: 0.03801 |  0:00:07s\n",
      "epoch 71 | loss: 0.03795 |  0:00:08s\n",
      "epoch 72 | loss: 0.03686 |  0:00:08s\n",
      "epoch 73 | loss: 0.03748 |  0:00:08s\n",
      "epoch 74 | loss: 0.0364  |  0:00:08s\n",
      "epoch 75 | loss: 0.03898 |  0:00:08s\n",
      "epoch 76 | loss: 0.03671 |  0:00:08s\n",
      "epoch 77 | loss: 0.03665 |  0:00:08s\n",
      "epoch 78 | loss: 0.03625 |  0:00:08s\n",
      "epoch 79 | loss: 0.03704 |  0:00:08s\n",
      "epoch 80 | loss: 0.03617 |  0:00:09s\n",
      "epoch 81 | loss: 0.03584 |  0:00:09s\n",
      "epoch 82 | loss: 0.0367  |  0:00:09s\n",
      "epoch 83 | loss: 0.03667 |  0:00:09s\n",
      "epoch 84 | loss: 0.03468 |  0:00:09s\n",
      "epoch 85 | loss: 0.0368  |  0:00:09s\n",
      "epoch 86 | loss: 0.03572 |  0:00:09s\n",
      "epoch 87 | loss: 0.0353  |  0:00:09s\n",
      "epoch 88 | loss: 0.0371  |  0:00:09s\n",
      "epoch 89 | loss: 0.03529 |  0:00:10s\n",
      "epoch 90 | loss: 0.0363  |  0:00:10s\n",
      "epoch 91 | loss: 0.03601 |  0:00:10s\n",
      "epoch 92 | loss: 0.03466 |  0:00:10s\n",
      "epoch 93 | loss: 0.03628 |  0:00:10s\n",
      "epoch 94 | loss: 0.03658 |  0:00:10s\n",
      "epoch 95 | loss: 0.03694 |  0:00:10s\n",
      "epoch 96 | loss: 0.03479 |  0:00:10s\n",
      "epoch 97 | loss: 0.03481 |  0:00:10s\n",
      "epoch 98 | loss: 0.034   |  0:00:11s\n",
      "epoch 99 | loss: 0.03624 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.13489 |  0:00:00s\n",
      "epoch 1  | loss: 0.7278  |  0:00:00s\n",
      "epoch 2  | loss: 0.53324 |  0:00:00s\n",
      "epoch 3  | loss: 0.43667 |  0:00:00s\n",
      "epoch 4  | loss: 0.36249 |  0:00:00s\n",
      "epoch 5  | loss: 0.33105 |  0:00:00s\n",
      "epoch 6  | loss: 0.33949 |  0:00:00s\n",
      "epoch 7  | loss: 0.34333 |  0:00:00s\n",
      "epoch 8  | loss: 0.31739 |  0:00:01s\n",
      "epoch 9  | loss: 0.29729 |  0:00:01s\n",
      "epoch 10 | loss: 0.29833 |  0:00:01s\n",
      "epoch 11 | loss: 0.30033 |  0:00:01s\n",
      "epoch 12 | loss: 0.28229 |  0:00:01s\n",
      "epoch 13 | loss: 0.26691 |  0:00:01s\n",
      "epoch 14 | loss: 0.25254 |  0:00:01s\n",
      "epoch 15 | loss: 0.24827 |  0:00:01s\n",
      "epoch 16 | loss: 0.23914 |  0:00:02s\n",
      "epoch 17 | loss: 0.23436 |  0:00:02s\n",
      "epoch 18 | loss: 0.22783 |  0:00:02s\n",
      "epoch 19 | loss: 0.21792 |  0:00:02s\n",
      "epoch 20 | loss: 0.22369 |  0:00:02s\n",
      "epoch 21 | loss: 0.1882  |  0:00:02s\n",
      "epoch 22 | loss: 0.17626 |  0:00:02s\n",
      "epoch 23 | loss: 0.17756 |  0:00:02s\n",
      "epoch 24 | loss: 0.15915 |  0:00:02s\n",
      "epoch 25 | loss: 0.18121 |  0:00:03s\n",
      "epoch 26 | loss: 0.14666 |  0:00:03s\n",
      "epoch 27 | loss: 0.14222 |  0:00:03s\n",
      "epoch 28 | loss: 0.12485 |  0:00:03s\n",
      "epoch 29 | loss: 0.11451 |  0:00:03s\n",
      "epoch 30 | loss: 0.10798 |  0:00:03s\n",
      "epoch 31 | loss: 0.08821 |  0:00:03s\n",
      "epoch 32 | loss: 0.0842  |  0:00:03s\n",
      "epoch 33 | loss: 0.09093 |  0:00:04s\n",
      "epoch 34 | loss: 0.08073 |  0:00:04s\n",
      "epoch 35 | loss: 0.07288 |  0:00:04s\n",
      "epoch 36 | loss: 0.06567 |  0:00:04s\n",
      "epoch 37 | loss: 0.06719 |  0:00:04s\n",
      "epoch 38 | loss: 0.06158 |  0:00:04s\n",
      "epoch 39 | loss: 0.0654  |  0:00:04s\n",
      "epoch 40 | loss: 0.06622 |  0:00:04s\n",
      "epoch 41 | loss: 0.05944 |  0:00:04s\n",
      "epoch 42 | loss: 0.05859 |  0:00:04s\n",
      "epoch 43 | loss: 0.05894 |  0:00:05s\n",
      "epoch 44 | loss: 0.05129 |  0:00:05s\n",
      "epoch 45 | loss: 0.04926 |  0:00:05s\n",
      "epoch 46 | loss: 0.05195 |  0:00:05s\n",
      "epoch 47 | loss: 0.04876 |  0:00:05s\n",
      "epoch 48 | loss: 0.04882 |  0:00:05s\n",
      "epoch 49 | loss: 0.05253 |  0:00:05s\n",
      "epoch 50 | loss: 0.05372 |  0:00:05s\n",
      "epoch 51 | loss: 0.04504 |  0:00:05s\n",
      "epoch 52 | loss: 0.0486  |  0:00:06s\n",
      "epoch 53 | loss: 0.04802 |  0:00:06s\n",
      "epoch 54 | loss: 0.04873 |  0:00:06s\n",
      "epoch 55 | loss: 0.04773 |  0:00:06s\n",
      "epoch 56 | loss: 0.0456  |  0:00:06s\n",
      "epoch 57 | loss: 0.04594 |  0:00:06s\n",
      "epoch 58 | loss: 0.04348 |  0:00:06s\n",
      "epoch 59 | loss: 0.04187 |  0:00:06s\n",
      "epoch 60 | loss: 0.04019 |  0:00:06s\n",
      "epoch 61 | loss: 0.04349 |  0:00:07s\n",
      "epoch 62 | loss: 0.04385 |  0:00:07s\n",
      "epoch 63 | loss: 0.0459  |  0:00:07s\n",
      "epoch 64 | loss: 0.04802 |  0:00:07s\n",
      "epoch 65 | loss: 0.04283 |  0:00:07s\n",
      "epoch 66 | loss: 0.04202 |  0:00:07s\n",
      "epoch 67 | loss: 0.04293 |  0:00:07s\n",
      "epoch 68 | loss: 0.04401 |  0:00:07s\n",
      "epoch 69 | loss: 0.03987 |  0:00:07s\n",
      "epoch 70 | loss: 0.04306 |  0:00:08s\n",
      "epoch 71 | loss: 0.03984 |  0:00:08s\n",
      "epoch 72 | loss: 0.04099 |  0:00:08s\n",
      "epoch 73 | loss: 0.04158 |  0:00:08s\n",
      "epoch 74 | loss: 0.03629 |  0:00:08s\n",
      "epoch 75 | loss: 0.04037 |  0:00:08s\n",
      "epoch 76 | loss: 0.04172 |  0:00:08s\n",
      "epoch 77 | loss: 0.04072 |  0:00:08s\n",
      "epoch 78 | loss: 0.03697 |  0:00:08s\n",
      "epoch 79 | loss: 0.04338 |  0:00:09s\n",
      "epoch 80 | loss: 0.04625 |  0:00:09s\n",
      "epoch 81 | loss: 0.04077 |  0:00:09s\n",
      "epoch 82 | loss: 0.03837 |  0:00:09s\n",
      "epoch 83 | loss: 0.04045 |  0:00:09s\n",
      "epoch 84 | loss: 0.03764 |  0:00:09s\n",
      "epoch 85 | loss: 0.0396  |  0:00:09s\n",
      "epoch 86 | loss: 0.04083 |  0:00:09s\n",
      "epoch 87 | loss: 0.03725 |  0:00:09s\n",
      "epoch 88 | loss: 0.0373  |  0:00:10s\n",
      "epoch 89 | loss: 0.03621 |  0:00:10s\n",
      "epoch 90 | loss: 0.03749 |  0:00:10s\n",
      "epoch 91 | loss: 0.0376  |  0:00:10s\n",
      "epoch 92 | loss: 0.03685 |  0:00:10s\n",
      "epoch 93 | loss: 0.03828 |  0:00:10s\n",
      "epoch 94 | loss: 0.03774 |  0:00:10s\n",
      "epoch 95 | loss: 0.03772 |  0:00:10s\n",
      "epoch 96 | loss: 0.03841 |  0:00:10s\n",
      "epoch 97 | loss: 0.03717 |  0:00:11s\n",
      "epoch 98 | loss: 0.03797 |  0:00:11s\n",
      "epoch 99 | loss: 0.03602 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.0477  |  0:00:00s\n",
      "epoch 1  | loss: 0.76705 |  0:00:00s\n",
      "epoch 2  | loss: 0.55731 |  0:00:00s\n",
      "epoch 3  | loss: 0.45413 |  0:00:00s\n",
      "epoch 4  | loss: 0.3401  |  0:00:00s\n",
      "epoch 5  | loss: 0.29976 |  0:00:00s\n",
      "epoch 6  | loss: 0.30128 |  0:00:00s\n",
      "epoch 7  | loss: 0.30026 |  0:00:00s\n",
      "epoch 8  | loss: 0.32173 |  0:00:00s\n",
      "epoch 9  | loss: 0.31159 |  0:00:01s\n",
      "epoch 10 | loss: 0.30403 |  0:00:01s\n",
      "epoch 11 | loss: 0.29338 |  0:00:01s\n",
      "epoch 12 | loss: 0.27563 |  0:00:01s\n",
      "epoch 13 | loss: 0.26956 |  0:00:01s\n",
      "epoch 14 | loss: 0.27657 |  0:00:01s\n",
      "epoch 15 | loss: 0.26212 |  0:00:01s\n",
      "epoch 16 | loss: 0.25696 |  0:00:01s\n",
      "epoch 17 | loss: 0.23574 |  0:00:01s\n",
      "epoch 18 | loss: 0.21329 |  0:00:02s\n",
      "epoch 19 | loss: 0.20163 |  0:00:02s\n",
      "epoch 20 | loss: 0.18916 |  0:00:02s\n",
      "epoch 21 | loss: 0.18828 |  0:00:02s\n",
      "epoch 22 | loss: 0.18347 |  0:00:02s\n",
      "epoch 23 | loss: 0.18428 |  0:00:02s\n",
      "epoch 24 | loss: 0.16406 |  0:00:02s\n",
      "epoch 25 | loss: 0.15952 |  0:00:02s\n",
      "epoch 26 | loss: 0.15272 |  0:00:02s\n",
      "epoch 27 | loss: 0.14502 |  0:00:03s\n",
      "epoch 28 | loss: 0.14442 |  0:00:03s\n",
      "epoch 29 | loss: 0.13698 |  0:00:03s\n",
      "epoch 30 | loss: 0.12548 |  0:00:03s\n",
      "epoch 31 | loss: 0.12293 |  0:00:03s\n",
      "epoch 32 | loss: 0.10316 |  0:00:03s\n",
      "epoch 33 | loss: 0.10284 |  0:00:03s\n",
      "epoch 34 | loss: 0.09469 |  0:00:03s\n",
      "epoch 35 | loss: 0.09775 |  0:00:04s\n",
      "epoch 36 | loss: 0.08649 |  0:00:04s\n",
      "epoch 37 | loss: 0.08358 |  0:00:04s\n",
      "epoch 38 | loss: 0.08728 |  0:00:04s\n",
      "epoch 39 | loss: 0.08657 |  0:00:04s\n",
      "epoch 40 | loss: 0.07843 |  0:00:04s\n",
      "epoch 41 | loss: 0.07903 |  0:00:04s\n",
      "epoch 42 | loss: 0.08437 |  0:00:04s\n",
      "epoch 43 | loss: 0.07818 |  0:00:04s\n",
      "epoch 44 | loss: 0.0779  |  0:00:05s\n",
      "epoch 45 | loss: 0.0763  |  0:00:05s\n",
      "epoch 46 | loss: 0.06897 |  0:00:05s\n",
      "epoch 47 | loss: 0.06881 |  0:00:05s\n",
      "epoch 48 | loss: 0.07265 |  0:00:05s\n",
      "epoch 49 | loss: 0.06117 |  0:00:05s\n",
      "epoch 50 | loss: 0.06707 |  0:00:05s\n",
      "epoch 51 | loss: 0.06804 |  0:00:05s\n",
      "epoch 52 | loss: 0.06792 |  0:00:05s\n",
      "epoch 53 | loss: 0.06041 |  0:00:06s\n",
      "epoch 54 | loss: 0.06622 |  0:00:06s\n",
      "epoch 55 | loss: 0.06321 |  0:00:06s\n",
      "epoch 56 | loss: 0.06066 |  0:00:06s\n",
      "epoch 57 | loss: 0.062   |  0:00:06s\n",
      "epoch 58 | loss: 0.05762 |  0:00:06s\n",
      "epoch 59 | loss: 0.05771 |  0:00:06s\n",
      "epoch 60 | loss: 0.05504 |  0:00:06s\n",
      "epoch 61 | loss: 0.04969 |  0:00:06s\n",
      "epoch 62 | loss: 0.05058 |  0:00:07s\n",
      "epoch 63 | loss: 0.05099 |  0:00:07s\n",
      "epoch 64 | loss: 0.04432 |  0:00:07s\n",
      "epoch 65 | loss: 0.04389 |  0:00:07s\n",
      "epoch 66 | loss: 0.04958 |  0:00:07s\n",
      "epoch 67 | loss: 0.04546 |  0:00:07s\n",
      "epoch 68 | loss: 0.04471 |  0:00:07s\n",
      "epoch 69 | loss: 0.04692 |  0:00:07s\n",
      "epoch 70 | loss: 0.0441  |  0:00:07s\n",
      "epoch 71 | loss: 0.04704 |  0:00:08s\n",
      "epoch 72 | loss: 0.04254 |  0:00:08s\n",
      "epoch 73 | loss: 0.04184 |  0:00:08s\n",
      "epoch 74 | loss: 0.0434  |  0:00:08s\n",
      "epoch 75 | loss: 0.04402 |  0:00:08s\n",
      "epoch 76 | loss: 0.03975 |  0:00:08s\n",
      "epoch 77 | loss: 0.0417  |  0:00:08s\n",
      "epoch 78 | loss: 0.04231 |  0:00:08s\n",
      "epoch 79 | loss: 0.03948 |  0:00:08s\n",
      "epoch 80 | loss: 0.03928 |  0:00:09s\n",
      "epoch 81 | loss: 0.03992 |  0:00:09s\n",
      "epoch 82 | loss: 0.0402  |  0:00:09s\n",
      "epoch 83 | loss: 0.03952 |  0:00:09s\n",
      "epoch 84 | loss: 0.03777 |  0:00:09s\n",
      "epoch 85 | loss: 0.0424  |  0:00:09s\n",
      "epoch 86 | loss: 0.04178 |  0:00:09s\n",
      "epoch 87 | loss: 0.03979 |  0:00:09s\n",
      "epoch 88 | loss: 0.03838 |  0:00:09s\n",
      "epoch 89 | loss: 0.03818 |  0:00:10s\n",
      "epoch 90 | loss: 0.03877 |  0:00:10s\n",
      "epoch 91 | loss: 0.03828 |  0:00:10s\n",
      "epoch 92 | loss: 0.03877 |  0:00:10s\n",
      "epoch 93 | loss: 0.04091 |  0:00:10s\n",
      "epoch 94 | loss: 0.0392  |  0:00:10s\n",
      "epoch 95 | loss: 0.0394  |  0:00:10s\n",
      "epoch 96 | loss: 0.03667 |  0:00:10s\n",
      "epoch 97 | loss: 0.03914 |  0:00:10s\n",
      "epoch 98 | loss: 0.03644 |  0:00:11s\n",
      "epoch 99 | loss: 0.03787 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.07283 |  0:00:00s\n",
      "epoch 1  | loss: 0.80636 |  0:00:00s\n",
      "epoch 2  | loss: 0.62657 |  0:00:00s\n",
      "epoch 3  | loss: 0.48953 |  0:00:00s\n",
      "epoch 4  | loss: 0.3937  |  0:00:00s\n",
      "epoch 5  | loss: 0.33372 |  0:00:00s\n",
      "epoch 6  | loss: 0.31021 |  0:00:00s\n",
      "epoch 7  | loss: 0.29738 |  0:00:00s\n",
      "epoch 8  | loss: 0.29495 |  0:00:01s\n",
      "epoch 9  | loss: 0.27804 |  0:00:01s\n",
      "epoch 10 | loss: 0.28219 |  0:00:01s\n",
      "epoch 11 | loss: 0.24864 |  0:00:01s\n",
      "epoch 12 | loss: 0.23628 |  0:00:01s\n",
      "epoch 13 | loss: 0.20414 |  0:00:01s\n",
      "epoch 14 | loss: 0.19231 |  0:00:01s\n",
      "epoch 15 | loss: 0.15546 |  0:00:01s\n",
      "epoch 16 | loss: 0.14423 |  0:00:01s\n",
      "epoch 17 | loss: 0.13286 |  0:00:02s\n",
      "epoch 18 | loss: 0.12684 |  0:00:02s\n",
      "epoch 19 | loss: 0.11457 |  0:00:02s\n",
      "epoch 20 | loss: 0.1033  |  0:00:02s\n",
      "epoch 21 | loss: 0.09396 |  0:00:02s\n",
      "epoch 22 | loss: 0.09398 |  0:00:02s\n",
      "epoch 23 | loss: 0.08546 |  0:00:02s\n",
      "epoch 24 | loss: 0.07958 |  0:00:02s\n",
      "epoch 25 | loss: 0.08265 |  0:00:02s\n",
      "epoch 26 | loss: 0.0835  |  0:00:03s\n",
      "epoch 27 | loss: 0.07849 |  0:00:03s\n",
      "epoch 28 | loss: 0.07119 |  0:00:03s\n",
      "epoch 29 | loss: 0.0608  |  0:00:03s\n",
      "epoch 30 | loss: 0.05749 |  0:00:03s\n",
      "epoch 31 | loss: 0.0547  |  0:00:03s\n",
      "epoch 32 | loss: 0.05452 |  0:00:03s\n",
      "epoch 33 | loss: 0.05458 |  0:00:03s\n",
      "epoch 34 | loss: 0.05994 |  0:00:03s\n",
      "epoch 35 | loss: 0.05121 |  0:00:04s\n",
      "epoch 36 | loss: 0.04457 |  0:00:04s\n",
      "epoch 37 | loss: 0.04321 |  0:00:04s\n",
      "epoch 38 | loss: 0.03815 |  0:00:04s\n",
      "epoch 39 | loss: 0.04622 |  0:00:04s\n",
      "epoch 40 | loss: 0.03971 |  0:00:04s\n",
      "epoch 41 | loss: 0.03928 |  0:00:04s\n",
      "epoch 42 | loss: 0.04191 |  0:00:04s\n",
      "epoch 43 | loss: 0.03812 |  0:00:05s\n",
      "epoch 44 | loss: 0.03511 |  0:00:05s\n",
      "epoch 45 | loss: 0.03759 |  0:00:05s\n",
      "epoch 46 | loss: 0.03719 |  0:00:05s\n",
      "epoch 47 | loss: 0.03564 |  0:00:05s\n",
      "epoch 48 | loss: 0.0356  |  0:00:05s\n",
      "epoch 49 | loss: 0.03621 |  0:00:05s\n",
      "epoch 50 | loss: 0.03663 |  0:00:05s\n",
      "epoch 51 | loss: 0.03467 |  0:00:05s\n",
      "epoch 52 | loss: 0.0373  |  0:00:06s\n",
      "epoch 53 | loss: 0.03248 |  0:00:06s\n",
      "epoch 54 | loss: 0.03568 |  0:00:06s\n",
      "epoch 55 | loss: 0.03766 |  0:00:06s\n",
      "epoch 56 | loss: 0.03219 |  0:00:06s\n",
      "epoch 57 | loss: 0.03713 |  0:00:06s\n",
      "epoch 58 | loss: 0.03365 |  0:00:06s\n",
      "epoch 59 | loss: 0.03357 |  0:00:06s\n",
      "epoch 60 | loss: 0.03346 |  0:00:06s\n",
      "epoch 61 | loss: 0.03351 |  0:00:07s\n",
      "epoch 62 | loss: 0.03172 |  0:00:07s\n",
      "epoch 63 | loss: 0.03288 |  0:00:07s\n",
      "epoch 64 | loss: 0.03378 |  0:00:07s\n",
      "epoch 65 | loss: 0.03335 |  0:00:07s\n",
      "epoch 66 | loss: 0.03228 |  0:00:07s\n",
      "epoch 67 | loss: 0.0332  |  0:00:07s\n",
      "epoch 68 | loss: 0.03499 |  0:00:07s\n",
      "epoch 69 | loss: 0.03303 |  0:00:07s\n",
      "epoch 70 | loss: 0.0388  |  0:00:08s\n",
      "epoch 71 | loss: 0.03452 |  0:00:08s\n",
      "epoch 72 | loss: 0.03369 |  0:00:08s\n",
      "epoch 73 | loss: 0.03783 |  0:00:08s\n",
      "epoch 74 | loss: 0.03232 |  0:00:08s\n",
      "epoch 75 | loss: 0.04055 |  0:00:08s\n",
      "epoch 76 | loss: 0.03278 |  0:00:08s\n",
      "epoch 77 | loss: 0.03449 |  0:00:08s\n",
      "epoch 78 | loss: 0.03186 |  0:00:08s\n",
      "epoch 79 | loss: 0.03368 |  0:00:09s\n",
      "epoch 80 | loss: 0.031   |  0:00:09s\n",
      "epoch 81 | loss: 0.0355  |  0:00:09s\n",
      "epoch 82 | loss: 0.03413 |  0:00:09s\n",
      "epoch 83 | loss: 0.03278 |  0:00:09s\n",
      "epoch 84 | loss: 0.03226 |  0:00:09s\n",
      "epoch 85 | loss: 0.03281 |  0:00:09s\n",
      "epoch 86 | loss: 0.03271 |  0:00:09s\n",
      "epoch 87 | loss: 0.03542 |  0:00:09s\n",
      "epoch 88 | loss: 0.03169 |  0:00:10s\n",
      "epoch 89 | loss: 0.03414 |  0:00:10s\n",
      "epoch 90 | loss: 0.03311 |  0:00:10s\n",
      "epoch 91 | loss: 0.034   |  0:00:10s\n",
      "epoch 92 | loss: 0.03135 |  0:00:10s\n",
      "epoch 93 | loss: 0.03128 |  0:00:10s\n",
      "epoch 94 | loss: 0.03358 |  0:00:10s\n",
      "epoch 95 | loss: 0.03202 |  0:00:10s\n",
      "epoch 96 | loss: 0.032   |  0:00:10s\n",
      "epoch 97 | loss: 0.03079 |  0:00:11s\n",
      "epoch 98 | loss: 0.03121 |  0:00:11s\n",
      "epoch 99 | loss: 0.03066 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.03449 |  0:00:00s\n",
      "epoch 1  | loss: 0.73939 |  0:00:00s\n",
      "epoch 2  | loss: 0.56129 |  0:00:00s\n",
      "epoch 3  | loss: 0.44291 |  0:00:00s\n",
      "epoch 4  | loss: 0.38184 |  0:00:00s\n",
      "epoch 5  | loss: 0.34057 |  0:00:00s\n",
      "epoch 6  | loss: 0.35711 |  0:00:00s\n",
      "epoch 7  | loss: 0.36694 |  0:00:00s\n",
      "epoch 8  | loss: 0.35778 |  0:00:00s\n",
      "epoch 9  | loss: 0.32944 |  0:00:01s\n",
      "epoch 10 | loss: 0.31058 |  0:00:01s\n",
      "epoch 11 | loss: 0.3131  |  0:00:01s\n",
      "epoch 12 | loss: 0.30448 |  0:00:01s\n",
      "epoch 13 | loss: 0.32132 |  0:00:01s\n",
      "epoch 14 | loss: 0.3085  |  0:00:01s\n",
      "epoch 15 | loss: 0.31097 |  0:00:01s\n",
      "epoch 16 | loss: 0.28391 |  0:00:01s\n",
      "epoch 17 | loss: 0.27454 |  0:00:02s\n",
      "epoch 18 | loss: 0.25872 |  0:00:02s\n",
      "epoch 19 | loss: 0.2481  |  0:00:02s\n",
      "epoch 20 | loss: 0.2483  |  0:00:02s\n",
      "epoch 21 | loss: 0.23612 |  0:00:02s\n",
      "epoch 22 | loss: 0.22761 |  0:00:02s\n",
      "epoch 23 | loss: 0.21269 |  0:00:02s\n",
      "epoch 24 | loss: 0.219   |  0:00:02s\n",
      "epoch 25 | loss: 0.18713 |  0:00:02s\n",
      "epoch 26 | loss: 0.18004 |  0:00:03s\n",
      "epoch 27 | loss: 0.17441 |  0:00:03s\n",
      "epoch 28 | loss: 0.16633 |  0:00:03s\n",
      "epoch 29 | loss: 0.14148 |  0:00:03s\n",
      "epoch 30 | loss: 0.12904 |  0:00:03s\n",
      "epoch 31 | loss: 0.12785 |  0:00:03s\n",
      "epoch 32 | loss: 0.1136  |  0:00:03s\n",
      "epoch 33 | loss: 0.09402 |  0:00:03s\n",
      "epoch 34 | loss: 0.09304 |  0:00:03s\n",
      "epoch 35 | loss: 0.08895 |  0:00:04s\n",
      "epoch 36 | loss: 0.07527 |  0:00:04s\n",
      "epoch 37 | loss: 0.07253 |  0:00:04s\n",
      "epoch 38 | loss: 0.07453 |  0:00:04s\n",
      "epoch 39 | loss: 0.07406 |  0:00:04s\n",
      "epoch 40 | loss: 0.05869 |  0:00:04s\n",
      "epoch 41 | loss: 0.05788 |  0:00:04s\n",
      "epoch 42 | loss: 0.05593 |  0:00:04s\n",
      "epoch 43 | loss: 0.05615 |  0:00:04s\n",
      "epoch 44 | loss: 0.05503 |  0:00:05s\n",
      "epoch 45 | loss: 0.05766 |  0:00:05s\n",
      "epoch 46 | loss: 0.05251 |  0:00:05s\n",
      "epoch 47 | loss: 0.05235 |  0:00:05s\n",
      "epoch 48 | loss: 0.04893 |  0:00:05s\n",
      "epoch 49 | loss: 0.04503 |  0:00:05s\n",
      "epoch 50 | loss: 0.05531 |  0:00:05s\n",
      "epoch 51 | loss: 0.05052 |  0:00:05s\n",
      "epoch 52 | loss: 0.04639 |  0:00:06s\n",
      "epoch 53 | loss: 0.04318 |  0:00:06s\n",
      "epoch 54 | loss: 0.04666 |  0:00:06s\n",
      "epoch 55 | loss: 0.04126 |  0:00:06s\n",
      "epoch 56 | loss: 0.04443 |  0:00:06s\n",
      "epoch 57 | loss: 0.04558 |  0:00:06s\n",
      "epoch 58 | loss: 0.04052 |  0:00:06s\n",
      "epoch 59 | loss: 0.04235 |  0:00:06s\n",
      "epoch 60 | loss: 0.04189 |  0:00:06s\n",
      "epoch 61 | loss: 0.04165 |  0:00:07s\n",
      "epoch 62 | loss: 0.04385 |  0:00:07s\n",
      "epoch 63 | loss: 0.04292 |  0:00:07s\n",
      "epoch 64 | loss: 0.03888 |  0:00:07s\n",
      "epoch 65 | loss: 0.04377 |  0:00:07s\n",
      "epoch 66 | loss: 0.0439  |  0:00:07s\n",
      "epoch 67 | loss: 0.04346 |  0:00:07s\n",
      "epoch 68 | loss: 0.04005 |  0:00:07s\n",
      "epoch 69 | loss: 0.03981 |  0:00:07s\n",
      "epoch 70 | loss: 0.04307 |  0:00:08s\n",
      "epoch 71 | loss: 0.04063 |  0:00:08s\n",
      "epoch 72 | loss: 0.0405  |  0:00:08s\n",
      "epoch 73 | loss: 0.03602 |  0:00:08s\n",
      "epoch 74 | loss: 0.04214 |  0:00:08s\n",
      "epoch 75 | loss: 0.0381  |  0:00:08s\n",
      "epoch 76 | loss: 0.03753 |  0:00:08s\n",
      "epoch 77 | loss: 0.03929 |  0:00:08s\n",
      "epoch 78 | loss: 0.03714 |  0:00:08s\n",
      "epoch 79 | loss: 0.03752 |  0:00:08s\n",
      "epoch 80 | loss: 0.04014 |  0:00:09s\n",
      "epoch 81 | loss: 0.03609 |  0:00:09s\n",
      "epoch 82 | loss: 0.0374  |  0:00:09s\n",
      "epoch 83 | loss: 0.03594 |  0:00:09s\n",
      "epoch 84 | loss: 0.03749 |  0:00:09s\n",
      "epoch 85 | loss: 0.04143 |  0:00:09s\n",
      "epoch 86 | loss: 0.0384  |  0:00:09s\n",
      "epoch 87 | loss: 0.03659 |  0:00:09s\n",
      "epoch 88 | loss: 0.04081 |  0:00:09s\n",
      "epoch 89 | loss: 0.03857 |  0:00:10s\n",
      "epoch 90 | loss: 0.0365  |  0:00:10s\n",
      "epoch 91 | loss: 0.03795 |  0:00:10s\n",
      "epoch 92 | loss: 0.03993 |  0:00:10s\n",
      "epoch 93 | loss: 0.03662 |  0:00:10s\n",
      "epoch 94 | loss: 0.03768 |  0:00:10s\n",
      "epoch 95 | loss: 0.03924 |  0:00:10s\n",
      "epoch 96 | loss: 0.03614 |  0:00:10s\n",
      "epoch 97 | loss: 0.04126 |  0:00:11s\n",
      "epoch 98 | loss: 0.03821 |  0:00:11s\n",
      "epoch 99 | loss: 0.03797 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.02867 |  0:00:00s\n",
      "epoch 1  | loss: 0.78841 |  0:00:00s\n",
      "epoch 2  | loss: 0.58631 |  0:00:00s\n",
      "epoch 3  | loss: 0.45736 |  0:00:00s\n",
      "epoch 4  | loss: 0.40179 |  0:00:00s\n",
      "epoch 5  | loss: 0.33923 |  0:00:00s\n",
      "epoch 6  | loss: 0.35529 |  0:00:00s\n",
      "epoch 7  | loss: 0.34408 |  0:00:00s\n",
      "epoch 8  | loss: 0.32709 |  0:00:00s\n",
      "epoch 9  | loss: 0.31118 |  0:00:01s\n",
      "epoch 10 | loss: 0.30759 |  0:00:01s\n",
      "epoch 11 | loss: 0.29689 |  0:00:01s\n",
      "epoch 12 | loss: 0.30865 |  0:00:01s\n",
      "epoch 13 | loss: 0.29629 |  0:00:01s\n",
      "epoch 14 | loss: 0.28606 |  0:00:01s\n",
      "epoch 15 | loss: 0.29596 |  0:00:01s\n",
      "epoch 16 | loss: 0.2692  |  0:00:01s\n",
      "epoch 17 | loss: 0.26616 |  0:00:01s\n",
      "epoch 18 | loss: 0.25263 |  0:00:02s\n",
      "epoch 19 | loss: 0.23967 |  0:00:02s\n",
      "epoch 20 | loss: 0.23684 |  0:00:02s\n",
      "epoch 21 | loss: 0.23726 |  0:00:02s\n",
      "epoch 22 | loss: 0.23344 |  0:00:02s\n",
      "epoch 23 | loss: 0.2416  |  0:00:02s\n",
      "epoch 24 | loss: 0.22509 |  0:00:02s\n",
      "epoch 25 | loss: 0.21139 |  0:00:02s\n",
      "epoch 26 | loss: 0.19553 |  0:00:02s\n",
      "epoch 27 | loss: 0.18732 |  0:00:03s\n",
      "epoch 28 | loss: 0.17184 |  0:00:03s\n",
      "epoch 29 | loss: 0.16263 |  0:00:03s\n",
      "epoch 30 | loss: 0.15246 |  0:00:03s\n",
      "epoch 31 | loss: 0.13944 |  0:00:03s\n",
      "epoch 32 | loss: 0.13129 |  0:00:03s\n",
      "epoch 33 | loss: 0.11756 |  0:00:03s\n",
      "epoch 34 | loss: 0.10899 |  0:00:03s\n",
      "epoch 35 | loss: 0.10631 |  0:00:03s\n",
      "epoch 36 | loss: 0.09533 |  0:00:04s\n",
      "epoch 37 | loss: 0.09892 |  0:00:04s\n",
      "epoch 38 | loss: 0.08638 |  0:00:04s\n",
      "epoch 39 | loss: 0.08537 |  0:00:04s\n",
      "epoch 40 | loss: 0.08836 |  0:00:04s\n",
      "epoch 41 | loss: 0.08945 |  0:00:04s\n",
      "epoch 42 | loss: 0.08215 |  0:00:04s\n",
      "epoch 43 | loss: 0.08162 |  0:00:04s\n",
      "epoch 44 | loss: 0.07557 |  0:00:04s\n",
      "epoch 45 | loss: 0.07407 |  0:00:05s\n",
      "epoch 46 | loss: 0.0686  |  0:00:05s\n",
      "epoch 47 | loss: 0.07916 |  0:00:05s\n",
      "epoch 48 | loss: 0.07366 |  0:00:05s\n",
      "epoch 49 | loss: 0.06758 |  0:00:05s\n",
      "epoch 50 | loss: 0.0721  |  0:00:05s\n",
      "epoch 51 | loss: 0.06656 |  0:00:05s\n",
      "epoch 52 | loss: 0.06469 |  0:00:05s\n",
      "epoch 53 | loss: 0.06855 |  0:00:05s\n",
      "epoch 54 | loss: 0.06861 |  0:00:06s\n",
      "epoch 55 | loss: 0.06578 |  0:00:06s\n",
      "epoch 56 | loss: 0.06308 |  0:00:06s\n",
      "epoch 57 | loss: 0.06124 |  0:00:06s\n",
      "epoch 58 | loss: 0.06125 |  0:00:06s\n",
      "epoch 59 | loss: 0.05919 |  0:00:06s\n",
      "epoch 60 | loss: 0.05901 |  0:00:06s\n",
      "epoch 61 | loss: 0.0621  |  0:00:06s\n",
      "epoch 62 | loss: 0.06175 |  0:00:06s\n",
      "epoch 63 | loss: 0.06348 |  0:00:07s\n",
      "epoch 64 | loss: 0.05873 |  0:00:07s\n",
      "epoch 65 | loss: 0.0583  |  0:00:07s\n",
      "epoch 66 | loss: 0.06132 |  0:00:07s\n",
      "epoch 67 | loss: 0.05485 |  0:00:07s\n",
      "epoch 68 | loss: 0.05452 |  0:00:07s\n",
      "epoch 69 | loss: 0.05004 |  0:00:07s\n",
      "epoch 70 | loss: 0.05584 |  0:00:07s\n",
      "epoch 71 | loss: 0.05504 |  0:00:08s\n",
      "epoch 72 | loss: 0.05385 |  0:00:08s\n",
      "epoch 73 | loss: 0.04831 |  0:00:08s\n",
      "epoch 74 | loss: 0.04902 |  0:00:08s\n",
      "epoch 75 | loss: 0.05157 |  0:00:08s\n",
      "epoch 76 | loss: 0.05315 |  0:00:08s\n",
      "epoch 77 | loss: 0.05247 |  0:00:08s\n",
      "epoch 78 | loss: 0.04864 |  0:00:08s\n",
      "epoch 79 | loss: 0.05356 |  0:00:08s\n",
      "epoch 80 | loss: 0.05273 |  0:00:09s\n",
      "epoch 81 | loss: 0.04755 |  0:00:09s\n",
      "epoch 82 | loss: 0.04796 |  0:00:09s\n",
      "epoch 83 | loss: 0.04872 |  0:00:09s\n",
      "epoch 84 | loss: 0.04693 |  0:00:09s\n",
      "epoch 85 | loss: 0.04852 |  0:00:09s\n",
      "epoch 86 | loss: 0.04865 |  0:00:09s\n",
      "epoch 87 | loss: 0.04823 |  0:00:09s\n",
      "epoch 88 | loss: 0.04537 |  0:00:09s\n",
      "epoch 89 | loss: 0.04662 |  0:00:10s\n",
      "epoch 90 | loss: 0.04559 |  0:00:10s\n",
      "epoch 91 | loss: 0.04878 |  0:00:10s\n",
      "epoch 92 | loss: 0.04876 |  0:00:10s\n",
      "epoch 93 | loss: 0.0496  |  0:00:10s\n",
      "epoch 94 | loss: 0.0499  |  0:00:10s\n",
      "epoch 95 | loss: 0.04643 |  0:00:10s\n",
      "epoch 96 | loss: 0.04897 |  0:00:10s\n",
      "epoch 97 | loss: 0.04741 |  0:00:10s\n",
      "epoch 98 | loss: 0.04911 |  0:00:11s\n",
      "epoch 99 | loss: 0.04522 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.05826 |  0:00:00s\n",
      "epoch 1  | loss: 0.75295 |  0:00:00s\n",
      "epoch 2  | loss: 0.57384 |  0:00:00s\n",
      "epoch 3  | loss: 0.47813 |  0:00:00s\n",
      "epoch 4  | loss: 0.39073 |  0:00:00s\n",
      "epoch 5  | loss: 0.34817 |  0:00:00s\n",
      "epoch 6  | loss: 0.34192 |  0:00:00s\n",
      "epoch 7  | loss: 0.34992 |  0:00:00s\n",
      "epoch 8  | loss: 0.30978 |  0:00:01s\n",
      "epoch 9  | loss: 0.30851 |  0:00:01s\n",
      "epoch 10 | loss: 0.31148 |  0:00:01s\n",
      "epoch 11 | loss: 0.31191 |  0:00:01s\n",
      "epoch 12 | loss: 0.28892 |  0:00:01s\n",
      "epoch 13 | loss: 0.27857 |  0:00:01s\n",
      "epoch 14 | loss: 0.27421 |  0:00:01s\n",
      "epoch 15 | loss: 0.27886 |  0:00:01s\n",
      "epoch 16 | loss: 0.25844 |  0:00:01s\n",
      "epoch 17 | loss: 0.25239 |  0:00:02s\n",
      "epoch 18 | loss: 0.23851 |  0:00:02s\n",
      "epoch 19 | loss: 0.23022 |  0:00:02s\n",
      "epoch 20 | loss: 0.21462 |  0:00:02s\n",
      "epoch 21 | loss: 0.21146 |  0:00:02s\n",
      "epoch 22 | loss: 0.20049 |  0:00:02s\n",
      "epoch 23 | loss: 0.18698 |  0:00:02s\n",
      "epoch 24 | loss: 0.17878 |  0:00:02s\n",
      "epoch 25 | loss: 0.18262 |  0:00:03s\n",
      "epoch 26 | loss: 0.16232 |  0:00:03s\n",
      "epoch 27 | loss: 0.15535 |  0:00:03s\n",
      "epoch 28 | loss: 0.14716 |  0:00:03s\n",
      "epoch 29 | loss: 0.12998 |  0:00:03s\n",
      "epoch 30 | loss: 0.12081 |  0:00:03s\n",
      "epoch 31 | loss: 0.10191 |  0:00:03s\n",
      "epoch 32 | loss: 0.10342 |  0:00:03s\n",
      "epoch 33 | loss: 0.08766 |  0:00:03s\n",
      "epoch 34 | loss: 0.08416 |  0:00:04s\n",
      "epoch 35 | loss: 0.07969 |  0:00:04s\n",
      "epoch 36 | loss: 0.07736 |  0:00:04s\n",
      "epoch 37 | loss: 0.07414 |  0:00:04s\n",
      "epoch 38 | loss: 0.06407 |  0:00:04s\n",
      "epoch 39 | loss: 0.05767 |  0:00:04s\n",
      "epoch 40 | loss: 0.05807 |  0:00:04s\n",
      "epoch 41 | loss: 0.0612  |  0:00:04s\n",
      "epoch 42 | loss: 0.05942 |  0:00:04s\n",
      "epoch 43 | loss: 0.04738 |  0:00:05s\n",
      "epoch 44 | loss: 0.04784 |  0:00:05s\n",
      "epoch 45 | loss: 0.0504  |  0:00:05s\n",
      "epoch 46 | loss: 0.04739 |  0:00:05s\n",
      "epoch 47 | loss: 0.0531  |  0:00:05s\n",
      "epoch 48 | loss: 0.05263 |  0:00:05s\n",
      "epoch 49 | loss: 0.05109 |  0:00:05s\n",
      "epoch 50 | loss: 0.04761 |  0:00:05s\n",
      "epoch 51 | loss: 0.05662 |  0:00:05s\n",
      "epoch 52 | loss: 0.04541 |  0:00:06s\n",
      "epoch 53 | loss: 0.0415  |  0:00:06s\n",
      "epoch 54 | loss: 0.04021 |  0:00:06s\n",
      "epoch 55 | loss: 0.04385 |  0:00:06s\n",
      "epoch 56 | loss: 0.04214 |  0:00:06s\n",
      "epoch 57 | loss: 0.04229 |  0:00:06s\n",
      "epoch 58 | loss: 0.04176 |  0:00:06s\n",
      "epoch 59 | loss: 0.0419  |  0:00:06s\n",
      "epoch 60 | loss: 0.0413  |  0:00:07s\n",
      "epoch 61 | loss: 0.04123 |  0:00:07s\n",
      "epoch 62 | loss: 0.04106 |  0:00:07s\n",
      "epoch 63 | loss: 0.03853 |  0:00:07s\n",
      "epoch 64 | loss: 0.04029 |  0:00:07s\n",
      "epoch 65 | loss: 0.03884 |  0:00:07s\n",
      "epoch 66 | loss: 0.04034 |  0:00:07s\n",
      "epoch 67 | loss: 0.03844 |  0:00:07s\n",
      "epoch 68 | loss: 0.04229 |  0:00:07s\n",
      "epoch 69 | loss: 0.0367  |  0:00:08s\n",
      "epoch 70 | loss: 0.03968 |  0:00:08s\n",
      "epoch 71 | loss: 0.04218 |  0:00:08s\n",
      "epoch 72 | loss: 0.03967 |  0:00:08s\n",
      "epoch 73 | loss: 0.03939 |  0:00:08s\n",
      "epoch 74 | loss: 0.03697 |  0:00:08s\n",
      "epoch 75 | loss: 0.03623 |  0:00:08s\n",
      "epoch 76 | loss: 0.0387  |  0:00:08s\n",
      "epoch 77 | loss: 0.03792 |  0:00:08s\n",
      "epoch 78 | loss: 0.03677 |  0:00:09s\n",
      "epoch 79 | loss: 0.03874 |  0:00:09s\n",
      "epoch 80 | loss: 0.04512 |  0:00:09s\n",
      "epoch 81 | loss: 0.03767 |  0:00:09s\n",
      "epoch 82 | loss: 0.03632 |  0:00:09s\n",
      "epoch 83 | loss: 0.0358  |  0:00:09s\n",
      "epoch 84 | loss: 0.03517 |  0:00:09s\n",
      "epoch 85 | loss: 0.03567 |  0:00:09s\n",
      "epoch 86 | loss: 0.03566 |  0:00:09s\n",
      "epoch 87 | loss: 0.03438 |  0:00:10s\n",
      "epoch 88 | loss: 0.03818 |  0:00:10s\n",
      "epoch 89 | loss: 0.03518 |  0:00:10s\n",
      "epoch 90 | loss: 0.0358  |  0:00:10s\n",
      "epoch 91 | loss: 0.03579 |  0:00:10s\n",
      "epoch 92 | loss: 0.03465 |  0:00:10s\n",
      "epoch 93 | loss: 0.03641 |  0:00:10s\n",
      "epoch 94 | loss: 0.03525 |  0:00:10s\n",
      "epoch 95 | loss: 0.03343 |  0:00:10s\n",
      "epoch 96 | loss: 0.03462 |  0:00:11s\n",
      "epoch 97 | loss: 0.03341 |  0:00:11s\n",
      "epoch 98 | loss: 0.0347  |  0:00:11s\n",
      "epoch 99 | loss: 0.03293 |  0:00:11s\n"
     ]
    }
   ],
   "source": [
    "# example of evaluating a decision tree with random undersampling\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# define pipeline\n",
    "steps = [('under', RandomUnderSampler(sampling_strategy=0.1)), ('model', TabNetClassifier())]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# evaluate pipeline\n",
    "scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'roc_auc']\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "scores = cross_validate(pipeline, df_std, malwareColTrain, scoring=scoring, cv=cv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T02:21:33.685121Z",
     "iopub.status.busy": "2023-03-04T02:21:33.684355Z",
     "iopub.status.idle": "2023-03-04T02:21:33.695346Z",
     "shell.execute_reply": "2023-03-04T02:21:33.693805Z",
     "shell.execute_reply.started": "2023-03-04T02:21:33.685121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7839 accuracy\n",
      "0.9364 recall\n",
      "0.8427 f1 score\n",
      "0.9893 roc auc\n",
      "117.77390 training time\n",
      "0.97138 predition time\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.4f accuracy\" % (scores['test_precision_macro'].mean()))\n",
    "print(\"%0.4f recall\" % (scores['test_recall_macro'].mean()))\n",
    "print(\"%0.4f f1 score\" % (scores['test_f1_macro'].mean()))\n",
    "print(\"%0.4f roc auc\" % (scores['test_roc_auc'].mean()))\n",
    "\n",
    "print(\"%0.5f training time\" % (scores['fit_time'].sum()))\n",
    "print(\"%0.5f predition time\" % (scores['score_time'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7755 accuracy\n",
    "0.8022 recall\n",
    "0.7716 f1 score\n",
    "0.9434 roc auc\n",
    "116.01409 training time\n",
    "0.93435 predition time\n",
    "senza zscore"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
