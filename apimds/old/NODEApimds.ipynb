{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T02:18:02.234513Z",
     "iopub.status.busy": "2023-03-04T02:18:02.233784Z",
     "iopub.status.idle": "2023-03-04T02:18:03.477602Z",
     "shell.execute_reply": "2023-03-04T02:18:03.475896Z",
     "shell.execute_reply.started": "2023-03-04T02:18:02.234448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas is used for data manipulation\n",
    "# Read in data and display first 5 rows\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_df = pd.read_pickle(\"../dataset/apimdsFeuturesExtracted.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('../dataset/apimdsFinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "malwareColTrain = df3['malware'].copy()\n",
    "df3 = df3.drop('malware', axis = 1)\n",
    "\n",
    "cols= df3.columns\n",
    "\n",
    "for c in cols:\n",
    "   fh = FeatureHasher(n_features=114, input_type='string')\n",
    "   hash_train = fh.transform(df3[[c]].astype(str).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "malwareColTrainPCA = PCA_df['malware'].copy()\n",
    "PCA_df = PCA_df.drop('malware', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_train = hash_train.toarray()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create a scaler object\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "df_std = std_scaler.fit_transform(hash_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytorch_tabular\n",
      "  Obtaining dependency information for pytorch_tabular from https://files.pythonhosted.org/packages/46/c4/142c38ea4f568f8554078976e63484872baff530e54f68db9f6237352cbc/pytorch_tabular-1.0.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytorch_tabular-1.0.2-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: torch>=1.9.0 in /home/alessio/.local/lib/python3.11/site-packages (from pytorch_tabular) (2.0.1)\n",
      "Collecting category-encoders<2.7.0,>=2.6.0 (from pytorch_tabular)\n",
      "  Obtaining dependency information for category-encoders<2.7.0,>=2.6.0 from https://files.pythonhosted.org/packages/1f/e2/495811f12b2e90753fff0e42a07adb0370a725de17cc23a579ac9d3ca67c/category_encoders-2.6.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading category_encoders-2.6.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/alessio/.local/lib/python3.11/site-packages (from pytorch_tabular) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/alessio/.local/lib/python3.11/site-packages (from pytorch_tabular) (2.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/lib/python3.11/site-packages (from pytorch_tabular) (1.3.0)\n",
      "Collecting pytorch-lightning<2.0.0,>=1.8.0 (from pytorch_tabular)\n",
      "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting omegaconf>=2.1.0 (from pytorch_tabular)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchmetrics<0.12.0,>=0.10.0 (from pytorch_tabular)\n",
      "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorboard!=2.5.0,>2.2.0 in /home/alessio/.local/lib/python3.11/site-packages (from pytorch_tabular) (2.13.0)\n",
      "Collecting protobuf<4.23.0,>=3.20.0 (from pytorch_tabular)\n",
      "  Downloading protobuf-4.22.5-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytorch-tabnet==4.0 (from pytorch_tabular)\n",
      "  Downloading pytorch_tabnet-4.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML<6.1.0,>=5.4 in /usr/lib/python3.11/site-packages (from pytorch_tabular) (6.0.1)\n",
      "Collecting matplotlib>3.1 (from pytorch_tabular)\n",
      "  Obtaining dependency information for matplotlib>3.1 from https://files.pythonhosted.org/packages/4f/d7/3303f11188122f66c940056f162d030992e7fbc9c702869bab163e85156b/matplotlib-3.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading matplotlib-3.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: ipywidgets in /home/alessio/.local/lib/python3.11/site-packages (from pytorch_tabular) (8.1.0)\n",
      "Collecting einops<0.7.0,>=0.6.0 (from pytorch_tabular)\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich>=11.0.0 in /usr/lib/python3.11/site-packages (from pytorch_tabular) (13.5.2)\n",
      "Requirement already satisfied: scipy>1.4 in /usr/lib/python3.11/site-packages (from pytorch-tabnet==4.0->pytorch_tabular) (1.11.2)\n",
      "Collecting torch>=1.9.0 (from pytorch_tabular)\n",
      "  Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl (887.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0,>=4.36 in /home/alessio/.local/lib/python3.11/site-packages (from pytorch-tabnet==4.0->pytorch_tabular) (4.66.1)\n",
      "Collecting statsmodels>=0.9.0 (from category-encoders<2.7.0,>=2.6.0->pytorch_tabular)\n",
      "  Downloading statsmodels-0.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting patsy>=0.5.1 (from category-encoders<2.7.0,>=2.6.0->pytorch_tabular)\n",
      "  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib>3.1->pytorch_tabular)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/d8/23/8d968922459b1c8a2c6ffca28fac00324b06b3a0633be2a39b0b1c3f84ab/contourpy-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading contourpy-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>3.1->pytorch_tabular)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>3.1->pytorch_tabular)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/07/fb/c507a09ab93642224417c31a3acd2806bfa53f4d723cf5d6cbdf62f2f337/fonttools-4.42.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading fonttools-4.42.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.0/151.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib>3.1->pytorch_tabular)\n",
      "  Obtaining dependency information for kiwisolver>=1.0.1 from https://files.pythonhosted.org/packages/17/ba/17a706b232308e65f57deeccae503c268292e6a091313f6ce833a23093ea/kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular) (2.8.2)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1.0->pytorch_tabular)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/lib/python3.11/site-packages (from pandas>=1.1.5->pytorch_tabular) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/alessio/.local/lib/python3.11/site-packages (from pandas>=1.1.5->pytorch_tabular) (2023.3)\n",
      "Collecting fsspec[http]>2021.06.0 (from pytorch-lightning<2.0.0,>=1.8.0->pytorch_tabular)\n",
      "  Obtaining dependency information for fsspec[http]>2021.06.0 from https://files.pythonhosted.org/packages/3a/9f/b40e8e5be886143379000af5fc0c675352d59e82fd869d24bf784161dc77/fsspec-2023.9.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.9.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/alessio/.local/lib/python3.11/site-packages (from pytorch-lightning<2.0.0,>=1.8.0->pytorch_tabular) (4.5.0)\n",
      "Collecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<2.0.0,>=1.8.0->pytorch_tabular)\n",
      "  Obtaining dependency information for lightning-utilities>=0.6.0.post0 from https://files.pythonhosted.org/packages/46/ee/8641eeb6a062f383b7d6875604e1f3f83bd2c93a0b4dbcabd3150b32de6e/lightning_utilities-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/lib/python3.11/site-packages (from rich>=11.0.0->pytorch_tabular) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/lib/python3.11/site-packages (from rich>=11.0.0->pytorch_tabular) (2.16.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pytorch_tabular) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pytorch_tabular) (3.1.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/alessio/.local/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (1.56.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (2.18.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/alessio/.local/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (2.28.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (68.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/alessio/.local/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (2.3.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (0.40.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/alessio/.local/lib/python3.11/site-packages (from torch>=1.9.0->pytorch_tabular) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/alessio/.local/lib/python3.11/site-packages (from torch>=1.9.0->pytorch_tabular) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/alessio/.local/lib/python3.11/site-packages (from torch>=1.9.0->pytorch_tabular) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/alessio/.local/lib/python3.11/site-packages (from torch>=1.9.0->pytorch_tabular) (11.7.99)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/lib/python3.11/site-packages (from ipywidgets->pytorch_tabular) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/lib/python3.11/site-packages (from ipywidgets->pytorch_tabular) (8.14.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/lib/python3.11/site-packages (from ipywidgets->pytorch_tabular) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /home/alessio/.local/lib/python3.11/site-packages (from ipywidgets->pytorch_tabular) (4.0.8)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /home/alessio/.local/lib/python3.11/site-packages (from ipywidgets->pytorch_tabular) (3.0.8)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.8.0->pytorch_tabular)\n",
      "  Obtaining dependency information for aiohttp!=4.0.0a0,!=4.0.0a1 from https://files.pythonhosted.org/packages/4c/b8/5c5efbb1d3cb1da3612b8e309e8e31b602ee9c5cca8e41961db385fc9d00/aiohttp-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (1.26.15)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/alessio/.local/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (1.3.1)\n",
      "Requirement already satisfied: backcall in /usr/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /usr/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular) (3.0.39)\n",
      "Requirement already satisfied: stack-data in /usr/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular) (4.8.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.0.0->pytorch_tabular) (0.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (3.4)\n",
      "Requirement already satisfied: platformdirs in /usr/lib/python3.11/site-packages (from setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (3.10.0)\n",
      "Requirement already satisfied: jaraco.text in /usr/lib/python3.11/site-packages (from setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (3.11.1)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3.11/site-packages (from setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (10.1.0)\n",
      "Requirement already satisfied: ordered-set in /usr/lib/python3.11/site-packages (from setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (4.1.0)\n",
      "Requirement already satisfied: tomli in /usr/lib/python3.11/site-packages (from setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (2.0.1)\n",
      "Requirement already satisfied: validate-pyproject in /usr/lib/python3.11/site-packages (from setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (0.13.post1.dev0+gb752273.d20230520)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (2.1.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.8.0->pytorch_tabular) (22.2.0)\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.8.0->pytorch_tabular)\n",
      "  Obtaining dependency information for charset-normalizer<4.0,>=2.0 from https://files.pythonhosted.org/packages/bc/85/ef25d4ba14c7653c3020a1c6e1a7413e6791ef36a0ac177efa605fc2c737/charset_normalizer-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading charset_normalizer-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.8.0->pytorch_tabular)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.8.0->pytorch_tabular) (4.0.2)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.8.0->pytorch_tabular)\n",
      "  Downloading yarl-1.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.8/282.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.8.0->pytorch_tabular)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/e6/7e/74b176a5580e1a41da326d07cf47a0032923fb3eeec9afbd92bb5c6457df/frozenlist-1.4.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.8.0->pytorch_tabular)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->pytorch_tabular) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /usr/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets->pytorch_tabular) (0.2.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/alessio/.local/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (3.2.2)\n",
      "Requirement already satisfied: jaraco.functools in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (3.8.1)\n",
      "Requirement already satisfied: jaraco.context>=4.1 in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (4.3.0)\n",
      "Requirement already satisfied: autocommand in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (2.2.2)\n",
      "Requirement already satisfied: inflect in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (7.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch_tabular) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch_tabular) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /usr/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch_tabular) (0.2.2)\n",
      "Requirement already satisfied: fastjsonschema<=3,>=2.16.2 in /usr/lib/python3.11/site-packages (from validate-pyproject->setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (2.18.0)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /usr/lib/python3.11/site-packages (from inflect->jaraco.text->setuptools>=41.0.0->tensorboard!=2.5.0,>2.2.0->pytorch_tabular) (1.10.9)\n",
      "Downloading pytorch_tabular-1.0.2-py2.py3-none-any.whl (122 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.5/122.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading category_encoders-2.6.2-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fonttools-4.42.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Downloading aiohttp-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.9.0-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.6/199.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.5/250.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=7b9fcbccc05de36c82f6cc9e70a0ca3c774f354c80a735d41444034d45ba274e\n",
      "  Stored in directory: /home/alessio/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, protobuf, patsy, omegaconf, multidict, lightning-utilities, kiwisolver, fsspec, frozenlist, fonttools, einops, cycler, contourpy, charset-normalizer, yarl, matplotlib, aiosignal, statsmodels, aiohttp, category-encoders, torch, torchmetrics, pytorch-tabnet, pytorch-lightning, pytorch_tabular\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "  Attempting uninstall: pytorch-tabnet\n",
      "    Found existing installation: pytorch-tabnet 4.1.0\n",
      "    Uninstalling pytorch-tabnet-4.1.0:\n",
      "      Successfully uninstalled pytorch-tabnet-4.1.0\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 category-encoders-2.6.2 charset-normalizer-3.2.0 contourpy-1.1.0 cycler-0.11.0 einops-0.6.1 fonttools-4.42.1 frozenlist-1.4.0 fsspec-2023.9.0 kiwisolver-1.4.5 lightning-utilities-0.9.0 matplotlib-3.7.2 multidict-6.0.4 omegaconf-2.3.0 patsy-0.5.3 protobuf-4.22.5 pytorch-lightning-1.9.5 pytorch-tabnet-4.0 pytorch_tabular-1.0.2 statsmodels-0.14.0 torch-1.13.1 torchmetrics-0.11.4 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_tabular --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mutable default <class 'torch.distributed._shard.sharded_tensor.metadata.TensorProperties'> for field tensor_properties is not allowed: use default_factory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_tabular\u001b[39;00m \u001b[39mimport\u001b[39;00m Nod\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_tabular/__init__.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m __email__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmanujosephv@gmail.com\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1.0.2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m models, ssl_models\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcategorical_encoders\u001b[39;00m \u001b[39mimport\u001b[39;00m CategoricalEmbeddingTransformer\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfeature_extractor\u001b[39;00m \u001b[39mimport\u001b[39;00m DeepFeatureExtractor\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_tabular/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m autoint, category_embedding, gate, mixture_density, node, tabnet\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mautoint\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoIntConfig, AutoIntModel\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase_model\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseModel\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_tabular/models/autoint/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mautoint\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoIntBackbone, AutoIntModel\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoIntConfig\n\u001b[1;32m      4\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mAutoIntModel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAutoIntBackbone\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAutoIntConfig\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_tabular/models/autoint/autoint.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39momegaconf\u001b[39;00m \u001b[39mimport\u001b[39;00m DictConfig\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_tabular\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Embedding2dLayer\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_tabular\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _initialize_layers, _linear_dropout_bn\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_model\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseModel\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_tabular/models/common/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_tabular\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m activations, heads, layers, utils\n\u001b[1;32m      3\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mactivations\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlayers\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mutils\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mheads\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_tabular/models/common/heads/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mblocks\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearHead, MixtureDensityHead\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearHeadConfig, MixtureDensityHeadConfig\n\u001b[1;32m      4\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mLinearHead\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMixtureDensityHead\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLinearHeadConfig\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMixtureDensityHeadConfig\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_tabular/models/common/heads/blocks.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m Categorical\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_tabular\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mheads\u001b[39;00m \u001b[39mimport\u001b[39;00m config \u001b[39mas\u001b[39;00m head_config\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_tabular\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _initialize_layers, _linear_dropout_bn\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfig_link\u001b[39m(r):\n\u001b[1;32m     14\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This is a helper function decorator to link the config to the head.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_tabular/utils.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:  \u001b[39m# for 1.8\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud_io\u001b[39;00m \u001b[39mimport\u001b[39;00m get_filesystem\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:  \u001b[39m# for 1.9\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m \u001b[39mimport\u001b[39;00m get_filesystem\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/__init__.py:35\u001b[0m\n\u001b[1;32m     32\u001b[0m     _logger\u001b[39m.\u001b[39mpropagate \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_fabric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseed\u001b[39;00m \u001b[39mimport\u001b[39;00m seed_everything  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m Callback  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer  \u001b[39m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/callbacks/__init__.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprediction_writer\u001b[39;00m \u001b[39mimport\u001b[39;00m BasePredictionWriter\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprogress\u001b[39;00m \u001b[39mimport\u001b[39;00m ProgressBarBase, RichProgressBar, TQDMProgressBar\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpruning\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelPruning\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m \u001b[39mimport\u001b[39;00m QuantizationAwareTraining\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrich_model_summary\u001b[39;00m \u001b[39mimport\u001b[39;00m RichModelSummary\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/callbacks/pruning.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallback\u001b[39;00m \u001b[39mimport\u001b[39;00m Callback\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodule\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningModule\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m MisconfigurationException\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrank_zero\u001b[39;00m \u001b[39mimport\u001b[39;00m rank_zero_debug, rank_zero_only\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/core/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatamodule\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningDataModule\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodule\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningModule\n\u001b[1;32m     18\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mLightningDataModule\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLightningModule\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/core/module.py:53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelIO\n\u001b[1;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloggers\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger\n\u001b[0;32m---> 53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconnectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogger_connector\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfx_validator\u001b[39;00m \u001b[39mimport\u001b[39;00m _FxValidator\n\u001b[1;32m     54\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m GradClipAlgorithmType\n\u001b[1;32m     55\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m MisconfigurationException\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/__init__.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_fabric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseed\u001b[39;00m \u001b[39mimport\u001b[39;00m seed_everything\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m     19\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mTrainer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mseed_everything\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloggers\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger\n\u001b[1;32m     57\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloggers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensorboard\u001b[39;00m \u001b[39mimport\u001b[39;00m TensorBoardLogger\n\u001b[0;32m---> 58\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m \u001b[39mimport\u001b[39;00m PredictionLoop, TrainingEpochLoop\n\u001b[1;32m     59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataloader\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluation_loop\u001b[39;00m \u001b[39mimport\u001b[39;00m EvaluationLoop\n\u001b[1;32m     60\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfit_loop\u001b[39;00m \u001b[39mimport\u001b[39;00m FitLoop\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/loops/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloop\u001b[39;00m \u001b[39mimport\u001b[39;00m Loop  \u001b[39m# noqa: F401 isort: skip (avoids circular imports)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatch\u001b[39;00m \u001b[39mimport\u001b[39;00m TrainingBatchLoop  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataloader\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoaderLoop, EvaluationLoop, PredictionLoop  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mepoch\u001b[39;00m \u001b[39mimport\u001b[39;00m EvaluationEpochLoop, PredictionEpochLoop, TrainingEpochLoop  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/loops/batch/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright The PyTorch Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining_batch_loop\u001b[39;00m \u001b[39mimport\u001b[39;00m TrainingBatchLoop  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanual_loop\u001b[39;00m \u001b[39mimport\u001b[39;00m ManualOptimization  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping_extensions\u001b[39;00m \u001b[39mimport\u001b[39;00m OrderedDict\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloop\u001b[39;00m \u001b[39mimport\u001b[39;00m Loop\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanual_loop\u001b[39;00m \u001b[39mimport\u001b[39;00m _OUTPUTS_TYPE \u001b[39mas\u001b[39;00m _MANUAL_LOOP_OUTPUTS_TYPE\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanual_loop\u001b[39;00m \u001b[39mimport\u001b[39;00m ManualOptimization\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer_loop\u001b[39;00m \u001b[39mimport\u001b[39;00m _OUTPUTS_TYPE \u001b[39mas\u001b[39;00m _OPTIMIZER_LOOP_OUTPUTS_TYPE\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanual_loop\u001b[39;00m \u001b[39mimport\u001b[39;00m ManualOptimization  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer_loop\u001b[39;00m \u001b[39mimport\u001b[39;00m OptimizerLoop  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/manual_loop.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m \u001b[39mimport\u001b[39;00m Loop\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclosure\u001b[39;00m \u001b[39mimport\u001b[39;00m OutputResult\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m _build_training_step_kwargs, _extract_hiddens\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprogress\u001b[39;00m \u001b[39mimport\u001b[39;00m Progress, ReadyCompletedTracker\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m MisconfigurationException\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtimer\u001b[39;00m \u001b[39mimport\u001b[39;00m Timer\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloops\u001b[39;00m \u001b[39mimport\u001b[39;00m Loop\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstrategies\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m ParallelStrategy\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstrategies\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstrategy\u001b[39;00m \u001b[39mimport\u001b[39;00m Strategy\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprogress\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseProgress\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/strategies/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright The Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_fabric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstrategies\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m \u001b[39mimport\u001b[39;00m _StrategyRegistry\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstrategies\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbagua\u001b[39;00m \u001b[39mimport\u001b[39;00m BaguaStrategy  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstrategies\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolossalai\u001b[39;00m \u001b[39mimport\u001b[39;00m ColossalAIStrategy  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstrategies\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mddp\u001b[39;00m \u001b[39mimport\u001b[39;00m DDPStrategy  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/strategies/bagua.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_fabric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m ReduceOp\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverrides\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m _LightningModuleWrapperBase, _LightningPrecisionModuleWrapperBase\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprecision\u001b[39;00m \u001b[39mimport\u001b[39;00m PrecisionPlugin\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstrategies\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mddp\u001b[39;00m \u001b[39mimport\u001b[39;00m DDPStrategy\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstrategies\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstrategy\u001b[39;00m \u001b[39mimport\u001b[39;00m TBroadcast\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/plugins/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhpu_plugin\u001b[39;00m \u001b[39mimport\u001b[39;00m HPUCheckpointIO\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayer_sync\u001b[39;00m \u001b[39mimport\u001b[39;00m LayerSync, NativeSyncBatchNorm\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprecision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapex_amp\u001b[39;00m \u001b[39mimport\u001b[39;00m ApexMixedPrecisionPlugin\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprecision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolossalai\u001b[39;00m \u001b[39mimport\u001b[39;00m ColossalAIPrecisionPlugin\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprecision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeepspeed\u001b[39;00m \u001b[39mimport\u001b[39;00m DeepSpeedPrecisionPlugin\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprecision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeepspeed\u001b[39;00m \u001b[39mimport\u001b[39;00m DeepSpeedPrecisionPlugin\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprecision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdouble\u001b[39;00m \u001b[39mimport\u001b[39;00m DoublePrecisionPlugin\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprecision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfsdp_native_native_amp\u001b[39;00m \u001b[39mimport\u001b[39;00m FullyShardedNativeNativeMixedPrecisionPlugin\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprecision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfully_sharded_native_amp\u001b[39;00m \u001b[39mimport\u001b[39;00m FullyShardedNativeMixedPrecisionPlugin\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprecision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhpu\u001b[39;00m \u001b[39mimport\u001b[39;00m HPUPrecisionPlugin\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/fsdp_native_native_amp.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m MisconfigurationException\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m _TORCH_GREATER_EQUAL_1_12 \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mdistributed\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfsdp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfully_sharded_data_parallel\u001b[39;00m \u001b[39mimport\u001b[39;00m MixedPrecision\n\u001b[1;32m     25\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfsdp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msharded_grad_scaler\u001b[39;00m \u001b[39mimport\u001b[39;00m ShardedGradScaler\n\u001b[1;32m     26\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/distributed/fsdp/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mflat_param\u001b[39;00m \u001b[39mimport\u001b[39;00m FlatParameter\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfully_sharded_data_parallel\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     BackwardPrefetch,\n\u001b[1;32m      4\u001b[0m     CPUOffload,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     StateDictType,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mwrap\u001b[39;00m \u001b[39mimport\u001b[39;00m ParamExecOrderWrapPolicy\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/distributed/fsdp/flat_param.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_fsdp_extensions\u001b[39;00m \u001b[39mimport\u001b[39;00m _ext_post_unflatten_transform, _ext_pre_flatten_transform\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m _alloc_storage, _free_storage, _set_fsdp_flattened, p_assert\n\u001b[1;32m     29\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     30\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFlatParameter\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFlatParamHandle\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHandleTrainingState\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/distributed/fsdp/_fsdp_extensions.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdist\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfsdp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shard_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m _create_chunk_sharded_tensor\n\u001b[1;32m     10\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mFSDPExtensions\u001b[39;00m(ABC):\n\u001b[1;32m     11\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m    This enables some customizable hooks to enable composability with tensor\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m    parallelism. To activate these hooks, use :func:`_set_fsdp_extensions` to\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m    set a custom :class:`FSDPExtensions` that implements the hooks.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/distributed/fsdp/_shard_utils.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m distributed_c10d\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msharded_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     Shard,\n\u001b[1;32m     12\u001b[0m     ShardedTensor,\n\u001b[1;32m     13\u001b[0m     ShardedTensorMetadata,\n\u001b[1;32m     14\u001b[0m     TensorProperties,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msharding_spec\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     ChunkShardingSpec,\n\u001b[1;32m     18\u001b[0m     EnumerableShardingSpec,\n\u001b[1;32m     19\u001b[0m     ShardingSpec,\n\u001b[1;32m     20\u001b[0m     ShardMetadata,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sharding_spec_to_offsets\u001b[39m(\n\u001b[1;32m     25\u001b[0m     sharding_spec: ShardingSpec, tensor_numel: \u001b[39mint\u001b[39m, world_size: \u001b[39mint\u001b[39m\n\u001b[1;32m     26\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/distributed/_shard/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     _replicate_tensor,\n\u001b[1;32m      3\u001b[0m     _shard_tensor,\n\u001b[1;32m      4\u001b[0m     load_with_process_group,\n\u001b[1;32m      5\u001b[0m     shard_module,\n\u001b[1;32m      6\u001b[0m     shard_parameter,\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/distributed/_shard/api.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m distributed_c10d\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msharded_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     ShardedTensor,\n\u001b[1;32m      8\u001b[0m     _PartialTensor\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mreplicated_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m ReplicatedTensor\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msharding_spec\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     ShardingSpec,\n\u001b[1;32m     13\u001b[0m     ChunkShardingSpec\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/distributed/_shard/sharded_tensor/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msharding_spec\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mshard_spec\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpartial_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m _PartialTensor\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     _CUSTOM_SHARDED_OPS,\n\u001b[1;32m     13\u001b[0m     _SHARDED_OPS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     TensorProperties,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/distributed/_shard/sharding_spec/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     DevicePlacementSpec,\n\u001b[1;32m      3\u001b[0m     EnumerableShardingSpec,\n\u001b[1;32m      4\u001b[0m     PlacementSpec,\n\u001b[1;32m      5\u001b[0m     ShardingSpec,\n\u001b[1;32m      6\u001b[0m     _infer_sharding_spec_from_shards_metadata,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mchunk_sharding_spec\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     ChunkShardingSpec,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetadata\u001b[39;00m \u001b[39mimport\u001b[39;00m ShardMetadata\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/distributed/_shard/sharding_spec/api.py:16\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_internals\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     check_tensor,\n\u001b[1;32m     10\u001b[0m     get_chunked_dim_size,\n\u001b[1;32m     11\u001b[0m     get_split_size,\n\u001b[1;32m     12\u001b[0m     validate_non_overlapping_shards_metadata\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetadata\u001b[39;00m \u001b[39mimport\u001b[39;00m ShardMetadata\n\u001b[0;32m---> 16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msharded_tensor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetadata\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msharded_tensor_meta\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mop_registry_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m _decorator_func\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     20\u001b[0m     \u001b[39m# Only include ShardedTensor when do type checking, exclude it\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39m# from run-time to resolve circular dependency.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/distributed/_shard/sharded_tensor/metadata.py:70\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     62\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcreate_from_tensor\u001b[39m(tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTensorProperties\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     63\u001b[0m         \u001b[39mreturn\u001b[39;00m TensorProperties(\n\u001b[1;32m     64\u001b[0m             dtype\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdtype,\n\u001b[1;32m     65\u001b[0m             layout\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mlayout,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m             pin_memory\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mis_pinned()\n\u001b[1;32m     69\u001b[0m         )\n\u001b[0;32m---> 70\u001b[0m \u001b[39m@dataclass\u001b[39;49m\n\u001b[1;32m     71\u001b[0m \u001b[39mclass\u001b[39;49;00m \u001b[39mShardedTensorMetadata\u001b[39;49;00m(\u001b[39mobject\u001b[39;49m):\n\u001b[1;32m     72\u001b[0m \u001b[39m    \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m    Represents metadata for :class:`ShardedTensor`\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     76\u001b[0m     \u001b[39m# Metadata about each shard of the Tensor\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:1223\u001b[0m, in \u001b[0;36mdataclass\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[39mreturn\u001b[39;00m wrap\n\u001b[1;32m   1222\u001b[0m \u001b[39m# We're called as @dataclass without parens.\u001b[39;00m\n\u001b[0;32m-> 1223\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(\u001b[39mcls\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:1213\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[0;32m-> 1213\u001b[0m     \u001b[39mreturn\u001b[39;00m _process_class(\u001b[39mcls\u001b[39;49m, init, \u001b[39mrepr\u001b[39;49m, eq, order, unsafe_hash,\n\u001b[1;32m   1214\u001b[0m                           frozen, match_args, kw_only, slots,\n\u001b[1;32m   1215\u001b[0m                           weakref_slot)\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:958\u001b[0m, in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m    955\u001b[0m         kw_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m         \u001b[39m# Otherwise it's a field of some type.\u001b[39;00m\n\u001b[0;32m--> 958\u001b[0m         cls_fields\u001b[39m.\u001b[39mappend(_get_field(\u001b[39mcls\u001b[39;49m, name, \u001b[39mtype\u001b[39;49m, kw_only))\n\u001b[1;32m    960\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m cls_fields:\n\u001b[1;32m    961\u001b[0m     fields[f\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m f\n",
      "File \u001b[0;32m/usr/lib/python3.11/dataclasses.py:815\u001b[0m, in \u001b[0;36m_get_field\u001b[0;34m(cls, a_name, a_type, default_kw_only)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39m# For real fields, disallow mutable defaults.  Use unhashable as a proxy\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[39m# indicator for mutability.  Read the __hash__ attribute from the class,\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39m# not the instance.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39m_field_type \u001b[39mis\u001b[39;00m _FIELD \u001b[39mand\u001b[39;00m f\u001b[39m.\u001b[39mdefault\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__hash__\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmutable default \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(f\u001b[39m.\u001b[39mdefault)\u001b[39m}\u001b[39;00m\u001b[39m for field \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    816\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m is not allowed: use default_factory\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m f\n",
      "\u001b[0;31mValueError\u001b[0m: mutable default <class 'torch.distributed._shard.sharded_tensor.metadata.TensorProperties'> for field tensor_properties is not allowed: use default_factory"
     ]
    }
   ],
   "source": [
    "from pytorch_tabular.models import node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'lib' has no attribute 'DenseBlock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/alessio/code/python/malware-detection/apimds/NODEApimds.ipynb Cell 8\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alessio/code/python/malware-detection/apimds/NODEApimds.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alessio/code/python/malware-detection/apimds/NODEApimds.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m in_features \u001b[39m=\u001b[39m df_std\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alessio/code/python/malware-detection/apimds/NODEApimds.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alessio/code/python/malware-detection/apimds/NODEApimds.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     lib\u001b[39m.\u001b[39;49mDenseBlock(in_features, \u001b[39m128\u001b[39m, num_layers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, tree_dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, depth\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m, flatten_output\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alessio/code/python/malware-detection/apimds/NODEApimds.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                    choice_function\u001b[39m=\u001b[39mlib\u001b[39m.\u001b[39mentmax15, bin_function\u001b[39m=\u001b[39mlib\u001b[39m.\u001b[39mentmoid15),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alessio/code/python/malware-detection/apimds/NODEApimds.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     lib\u001b[39m.\u001b[39mLambda(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)),  \u001b[39m# average first channels of every tree\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alessio/code/python/malware-detection/apimds/NODEApimds.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alessio/code/python/malware-detection/apimds/NODEApimds.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'lib' has no attribute 'DenseBlock'"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lib\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "in_features = df_std.shape[1]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    lib.DenseBlock(in_features, 128, num_layers=8, tree_dim=3, depth=6, flatten_output=False,\n",
    "                   choice_function=lib.entmax15, bin_function=lib.entmoid15),\n",
    "    lib.Lambda(lambda x: x[..., 0].mean(dim=-1)),  # average first channels of every tree\n",
    "    \n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T02:21:26.648062Z",
     "iopub.status.busy": "2023-03-04T02:21:26.646055Z",
     "iopub.status.idle": "2023-03-04T02:21:29.161687Z",
     "shell.execute_reply": "2023-03-04T02:21:29.160325Z",
     "shell.execute_reply.started": "2023-03-04T02:21:26.647956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.09567 |  0:00:00s\n",
      "epoch 1  | loss: 0.74645 |  0:00:00s\n",
      "epoch 2  | loss: 0.53509 |  0:00:00s\n",
      "epoch 3  | loss: 0.41713 |  0:00:00s\n",
      "epoch 4  | loss: 0.35879 |  0:00:00s\n",
      "epoch 5  | loss: 0.31935 |  0:00:00s\n",
      "epoch 6  | loss: 0.35533 |  0:00:00s\n",
      "epoch 7  | loss: 0.33466 |  0:00:00s\n",
      "epoch 8  | loss: 0.29986 |  0:00:01s\n",
      "epoch 9  | loss: 0.30012 |  0:00:01s\n",
      "epoch 10 | loss: 0.2912  |  0:00:01s\n",
      "epoch 11 | loss: 0.28942 |  0:00:01s\n",
      "epoch 12 | loss: 0.2804  |  0:00:01s\n",
      "epoch 13 | loss: 0.27622 |  0:00:01s\n",
      "epoch 14 | loss: 0.29043 |  0:00:01s\n",
      "epoch 15 | loss: 0.26753 |  0:00:01s\n",
      "epoch 16 | loss: 0.26909 |  0:00:01s\n",
      "epoch 17 | loss: 0.24305 |  0:00:02s\n",
      "epoch 18 | loss: 0.23611 |  0:00:02s\n",
      "epoch 19 | loss: 0.24096 |  0:00:02s\n",
      "epoch 20 | loss: 0.22133 |  0:00:02s\n",
      "epoch 21 | loss: 0.20742 |  0:00:02s\n",
      "epoch 22 | loss: 0.2096  |  0:00:02s\n",
      "epoch 23 | loss: 0.204   |  0:00:02s\n",
      "epoch 24 | loss: 0.18209 |  0:00:02s\n",
      "epoch 25 | loss: 0.17379 |  0:00:02s\n",
      "epoch 26 | loss: 0.17136 |  0:00:03s\n",
      "epoch 27 | loss: 0.15749 |  0:00:03s\n",
      "epoch 28 | loss: 0.16157 |  0:00:03s\n",
      "epoch 29 | loss: 0.14156 |  0:00:03s\n",
      "epoch 30 | loss: 0.13783 |  0:00:03s\n",
      "epoch 31 | loss: 0.13523 |  0:00:03s\n",
      "epoch 32 | loss: 0.1284  |  0:00:03s\n",
      "epoch 33 | loss: 0.12357 |  0:00:03s\n",
      "epoch 34 | loss: 0.11753 |  0:00:04s\n",
      "epoch 35 | loss: 0.11339 |  0:00:04s\n",
      "epoch 36 | loss: 0.10115 |  0:00:04s\n",
      "epoch 37 | loss: 0.10139 |  0:00:04s\n",
      "epoch 38 | loss: 0.10191 |  0:00:04s\n",
      "epoch 39 | loss: 0.09409 |  0:00:04s\n",
      "epoch 40 | loss: 0.09147 |  0:00:04s\n",
      "epoch 41 | loss: 0.08409 |  0:00:04s\n",
      "epoch 42 | loss: 0.08032 |  0:00:05s\n",
      "epoch 43 | loss: 0.07849 |  0:00:05s\n",
      "epoch 44 | loss: 0.07764 |  0:00:05s\n",
      "epoch 45 | loss: 0.07505 |  0:00:05s\n",
      "epoch 46 | loss: 0.06814 |  0:00:05s\n",
      "epoch 47 | loss: 0.06835 |  0:00:05s\n",
      "epoch 48 | loss: 0.06953 |  0:00:05s\n",
      "epoch 49 | loss: 0.06497 |  0:00:05s\n",
      "epoch 50 | loss: 0.06247 |  0:00:05s\n",
      "epoch 51 | loss: 0.0608  |  0:00:06s\n",
      "epoch 52 | loss: 0.05767 |  0:00:06s\n",
      "epoch 53 | loss: 0.05711 |  0:00:06s\n",
      "epoch 54 | loss: 0.05571 |  0:00:06s\n",
      "epoch 55 | loss: 0.05505 |  0:00:06s\n",
      "epoch 56 | loss: 0.05336 |  0:00:06s\n",
      "epoch 57 | loss: 0.05815 |  0:00:06s\n",
      "epoch 58 | loss: 0.05189 |  0:00:06s\n",
      "epoch 59 | loss: 0.06212 |  0:00:06s\n",
      "epoch 60 | loss: 0.0541  |  0:00:07s\n",
      "epoch 61 | loss: 0.0607  |  0:00:07s\n",
      "epoch 62 | loss: 0.04522 |  0:00:07s\n",
      "epoch 63 | loss: 0.04715 |  0:00:07s\n",
      "epoch 64 | loss: 0.05108 |  0:00:07s\n",
      "epoch 65 | loss: 0.05265 |  0:00:07s\n",
      "epoch 66 | loss: 0.04772 |  0:00:07s\n",
      "epoch 67 | loss: 0.04393 |  0:00:07s\n",
      "epoch 68 | loss: 0.05083 |  0:00:07s\n",
      "epoch 69 | loss: 0.04508 |  0:00:08s\n",
      "epoch 70 | loss: 0.04607 |  0:00:08s\n",
      "epoch 71 | loss: 0.04276 |  0:00:08s\n",
      "epoch 72 | loss: 0.04655 |  0:00:08s\n",
      "epoch 73 | loss: 0.04791 |  0:00:08s\n",
      "epoch 74 | loss: 0.04274 |  0:00:08s\n",
      "epoch 75 | loss: 0.05077 |  0:00:08s\n",
      "epoch 76 | loss: 0.04259 |  0:00:08s\n",
      "epoch 77 | loss: 0.04039 |  0:00:08s\n",
      "epoch 78 | loss: 0.04186 |  0:00:09s\n",
      "epoch 79 | loss: 0.03876 |  0:00:09s\n",
      "epoch 80 | loss: 0.03736 |  0:00:09s\n",
      "epoch 81 | loss: 0.03917 |  0:00:09s\n",
      "epoch 82 | loss: 0.04168 |  0:00:09s\n",
      "epoch 83 | loss: 0.03849 |  0:00:09s\n",
      "epoch 84 | loss: 0.0402  |  0:00:09s\n",
      "epoch 85 | loss: 0.03851 |  0:00:09s\n",
      "epoch 86 | loss: 0.03762 |  0:00:09s\n",
      "epoch 87 | loss: 0.03978 |  0:00:10s\n",
      "epoch 88 | loss: 0.03795 |  0:00:10s\n",
      "epoch 89 | loss: 0.03913 |  0:00:10s\n",
      "epoch 90 | loss: 0.03712 |  0:00:10s\n",
      "epoch 91 | loss: 0.03976 |  0:00:10s\n",
      "epoch 92 | loss: 0.03847 |  0:00:10s\n",
      "epoch 93 | loss: 0.03956 |  0:00:10s\n",
      "epoch 94 | loss: 0.04024 |  0:00:10s\n",
      "epoch 95 | loss: 0.03712 |  0:00:10s\n",
      "epoch 96 | loss: 0.03636 |  0:00:11s\n",
      "epoch 97 | loss: 0.03913 |  0:00:11s\n",
      "epoch 98 | loss: 0.03704 |  0:00:11s\n",
      "epoch 99 | loss: 0.03503 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.07083 |  0:00:00s\n",
      "epoch 1  | loss: 0.77121 |  0:00:00s\n",
      "epoch 2  | loss: 0.55931 |  0:00:00s\n",
      "epoch 3  | loss: 0.43284 |  0:00:00s\n",
      "epoch 4  | loss: 0.351   |  0:00:00s\n",
      "epoch 5  | loss: 0.32031 |  0:00:00s\n",
      "epoch 6  | loss: 0.3445  |  0:00:00s\n",
      "epoch 7  | loss: 0.32695 |  0:00:00s\n",
      "epoch 8  | loss: 0.30712 |  0:00:01s\n",
      "epoch 9  | loss: 0.29528 |  0:00:01s\n",
      "epoch 10 | loss: 0.29477 |  0:00:01s\n",
      "epoch 11 | loss: 0.28908 |  0:00:01s\n",
      "epoch 12 | loss: 0.28702 |  0:00:01s\n",
      "epoch 13 | loss: 0.28722 |  0:00:01s\n",
      "epoch 14 | loss: 0.2701  |  0:00:01s\n",
      "epoch 15 | loss: 0.26612 |  0:00:01s\n",
      "epoch 16 | loss: 0.25892 |  0:00:01s\n",
      "epoch 17 | loss: 0.24753 |  0:00:02s\n",
      "epoch 18 | loss: 0.23785 |  0:00:02s\n",
      "epoch 19 | loss: 0.22    |  0:00:02s\n",
      "epoch 20 | loss: 0.21375 |  0:00:02s\n",
      "epoch 21 | loss: 0.19727 |  0:00:02s\n",
      "epoch 22 | loss: 0.17465 |  0:00:02s\n",
      "epoch 23 | loss: 0.1564  |  0:00:02s\n",
      "epoch 24 | loss: 0.14144 |  0:00:02s\n",
      "epoch 25 | loss: 0.13677 |  0:00:02s\n",
      "epoch 26 | loss: 0.12017 |  0:00:03s\n",
      "epoch 27 | loss: 0.11446 |  0:00:03s\n",
      "epoch 28 | loss: 0.10115 |  0:00:03s\n",
      "epoch 29 | loss: 0.09561 |  0:00:03s\n",
      "epoch 30 | loss: 0.08547 |  0:00:03s\n",
      "epoch 31 | loss: 0.07648 |  0:00:03s\n",
      "epoch 32 | loss: 0.08716 |  0:00:03s\n",
      "epoch 33 | loss: 0.07337 |  0:00:03s\n",
      "epoch 34 | loss: 0.07083 |  0:00:03s\n",
      "epoch 35 | loss: 0.06858 |  0:00:04s\n",
      "epoch 36 | loss: 0.0681  |  0:00:04s\n",
      "epoch 37 | loss: 0.06235 |  0:00:04s\n",
      "epoch 38 | loss: 0.0613  |  0:00:04s\n",
      "epoch 39 | loss: 0.05843 |  0:00:04s\n",
      "epoch 40 | loss: 0.05691 |  0:00:04s\n",
      "epoch 41 | loss: 0.05657 |  0:00:04s\n",
      "epoch 42 | loss: 0.05571 |  0:00:04s\n",
      "epoch 43 | loss: 0.05253 |  0:00:05s\n",
      "epoch 44 | loss: 0.05512 |  0:00:05s\n",
      "epoch 45 | loss: 0.05027 |  0:00:05s\n",
      "epoch 46 | loss: 0.0497  |  0:00:05s\n",
      "epoch 47 | loss: 0.05113 |  0:00:05s\n",
      "epoch 48 | loss: 0.05262 |  0:00:05s\n",
      "epoch 49 | loss: 0.05075 |  0:00:05s\n",
      "epoch 50 | loss: 0.04703 |  0:00:05s\n",
      "epoch 51 | loss: 0.04815 |  0:00:05s\n",
      "epoch 52 | loss: 0.04574 |  0:00:06s\n",
      "epoch 53 | loss: 0.04466 |  0:00:06s\n",
      "epoch 54 | loss: 0.04649 |  0:00:06s\n",
      "epoch 55 | loss: 0.05064 |  0:00:06s\n",
      "epoch 56 | loss: 0.05604 |  0:00:06s\n",
      "epoch 57 | loss: 0.05196 |  0:00:06s\n",
      "epoch 58 | loss: 0.04757 |  0:00:06s\n",
      "epoch 59 | loss: 0.04727 |  0:00:06s\n",
      "epoch 60 | loss: 0.04882 |  0:00:06s\n",
      "epoch 61 | loss: 0.04427 |  0:00:07s\n",
      "epoch 62 | loss: 0.04699 |  0:00:07s\n",
      "epoch 63 | loss: 0.04394 |  0:00:07s\n",
      "epoch 64 | loss: 0.04392 |  0:00:07s\n",
      "epoch 65 | loss: 0.04364 |  0:00:07s\n",
      "epoch 66 | loss: 0.0454  |  0:00:07s\n",
      "epoch 67 | loss: 0.04253 |  0:00:07s\n",
      "epoch 68 | loss: 0.0459  |  0:00:07s\n",
      "epoch 69 | loss: 0.05883 |  0:00:07s\n",
      "epoch 70 | loss: 0.04956 |  0:00:08s\n",
      "epoch 71 | loss: 0.05148 |  0:00:08s\n",
      "epoch 72 | loss: 0.05024 |  0:00:08s\n",
      "epoch 73 | loss: 0.05221 |  0:00:08s\n",
      "epoch 74 | loss: 0.0474  |  0:00:08s\n",
      "epoch 75 | loss: 0.04891 |  0:00:08s\n",
      "epoch 76 | loss: 0.05135 |  0:00:08s\n",
      "epoch 77 | loss: 0.0699  |  0:00:08s\n",
      "epoch 78 | loss: 0.06127 |  0:00:09s\n",
      "epoch 79 | loss: 0.05917 |  0:00:09s\n",
      "epoch 80 | loss: 0.05245 |  0:00:09s\n",
      "epoch 81 | loss: 0.05346 |  0:00:09s\n",
      "epoch 82 | loss: 0.05473 |  0:00:09s\n",
      "epoch 83 | loss: 0.05259 |  0:00:09s\n",
      "epoch 84 | loss: 0.06184 |  0:00:09s\n",
      "epoch 85 | loss: 0.04976 |  0:00:09s\n",
      "epoch 86 | loss: 0.04883 |  0:00:09s\n",
      "epoch 87 | loss: 0.05137 |  0:00:10s\n",
      "epoch 88 | loss: 0.05012 |  0:00:10s\n",
      "epoch 89 | loss: 0.04565 |  0:00:10s\n",
      "epoch 90 | loss: 0.0467  |  0:00:10s\n",
      "epoch 91 | loss: 0.04391 |  0:00:10s\n",
      "epoch 92 | loss: 0.04745 |  0:00:10s\n",
      "epoch 93 | loss: 0.04503 |  0:00:10s\n",
      "epoch 94 | loss: 0.04571 |  0:00:10s\n",
      "epoch 95 | loss: 0.04278 |  0:00:10s\n",
      "epoch 96 | loss: 0.04068 |  0:00:11s\n",
      "epoch 97 | loss: 0.04127 |  0:00:11s\n",
      "epoch 98 | loss: 0.04166 |  0:00:11s\n",
      "epoch 99 | loss: 0.04105 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.01451 |  0:00:00s\n",
      "epoch 1  | loss: 0.7555  |  0:00:00s\n",
      "epoch 2  | loss: 0.58652 |  0:00:00s\n",
      "epoch 3  | loss: 0.45946 |  0:00:00s\n",
      "epoch 4  | loss: 0.38182 |  0:00:00s\n",
      "epoch 5  | loss: 0.32724 |  0:00:00s\n",
      "epoch 6  | loss: 0.32274 |  0:00:00s\n",
      "epoch 7  | loss: 0.33457 |  0:00:00s\n",
      "epoch 8  | loss: 0.32634 |  0:00:01s\n",
      "epoch 9  | loss: 0.31624 |  0:00:01s\n",
      "epoch 10 | loss: 0.30286 |  0:00:01s\n",
      "epoch 11 | loss: 0.30196 |  0:00:01s\n",
      "epoch 12 | loss: 0.2951  |  0:00:01s\n",
      "epoch 13 | loss: 0.2931  |  0:00:01s\n",
      "epoch 14 | loss: 0.28602 |  0:00:01s\n",
      "epoch 15 | loss: 0.26965 |  0:00:01s\n",
      "epoch 16 | loss: 0.26515 |  0:00:01s\n",
      "epoch 17 | loss: 0.25935 |  0:00:02s\n",
      "epoch 18 | loss: 0.24922 |  0:00:02s\n",
      "epoch 19 | loss: 0.2464  |  0:00:02s\n",
      "epoch 20 | loss: 0.2195  |  0:00:02s\n",
      "epoch 21 | loss: 0.20877 |  0:00:02s\n",
      "epoch 22 | loss: 0.19201 |  0:00:02s\n",
      "epoch 23 | loss: 0.1838  |  0:00:02s\n",
      "epoch 24 | loss: 0.16967 |  0:00:02s\n",
      "epoch 25 | loss: 0.15635 |  0:00:02s\n",
      "epoch 26 | loss: 0.13844 |  0:00:03s\n",
      "epoch 27 | loss: 0.12947 |  0:00:03s\n",
      "epoch 28 | loss: 0.13051 |  0:00:03s\n",
      "epoch 29 | loss: 0.11565 |  0:00:03s\n",
      "epoch 30 | loss: 0.10609 |  0:00:03s\n",
      "epoch 31 | loss: 0.09274 |  0:00:03s\n",
      "epoch 32 | loss: 0.08733 |  0:00:03s\n",
      "epoch 33 | loss: 0.09026 |  0:00:03s\n",
      "epoch 34 | loss: 0.09005 |  0:00:03s\n",
      "epoch 35 | loss: 0.09829 |  0:00:04s\n",
      "epoch 36 | loss: 0.08534 |  0:00:04s\n",
      "epoch 37 | loss: 0.08271 |  0:00:04s\n",
      "epoch 38 | loss: 0.08235 |  0:00:04s\n",
      "epoch 39 | loss: 0.08805 |  0:00:04s\n",
      "epoch 40 | loss: 0.07638 |  0:00:04s\n",
      "epoch 41 | loss: 0.08    |  0:00:04s\n",
      "epoch 42 | loss: 0.08082 |  0:00:04s\n",
      "epoch 43 | loss: 0.0751  |  0:00:04s\n",
      "epoch 44 | loss: 0.07544 |  0:00:05s\n",
      "epoch 45 | loss: 0.07701 |  0:00:05s\n",
      "epoch 46 | loss: 0.07971 |  0:00:05s\n",
      "epoch 47 | loss: 0.07801 |  0:00:05s\n",
      "epoch 48 | loss: 0.06715 |  0:00:05s\n",
      "epoch 49 | loss: 0.06766 |  0:00:05s\n",
      "epoch 50 | loss: 0.06345 |  0:00:05s\n",
      "epoch 51 | loss: 0.06117 |  0:00:05s\n",
      "epoch 52 | loss: 0.06288 |  0:00:05s\n",
      "epoch 53 | loss: 0.05857 |  0:00:06s\n",
      "epoch 54 | loss: 0.05515 |  0:00:06s\n",
      "epoch 55 | loss: 0.05938 |  0:00:06s\n",
      "epoch 56 | loss: 0.05294 |  0:00:06s\n",
      "epoch 57 | loss: 0.05761 |  0:00:06s\n",
      "epoch 58 | loss: 0.0606  |  0:00:06s\n",
      "epoch 59 | loss: 0.05272 |  0:00:06s\n",
      "epoch 60 | loss: 0.0567  |  0:00:06s\n",
      "epoch 61 | loss: 0.06155 |  0:00:06s\n",
      "epoch 62 | loss: 0.06056 |  0:00:07s\n",
      "epoch 63 | loss: 0.06401 |  0:00:07s\n",
      "epoch 64 | loss: 0.05143 |  0:00:07s\n",
      "epoch 65 | loss: 0.05481 |  0:00:07s\n",
      "epoch 66 | loss: 0.05274 |  0:00:07s\n",
      "epoch 67 | loss: 0.04926 |  0:00:07s\n",
      "epoch 68 | loss: 0.04918 |  0:00:07s\n",
      "epoch 69 | loss: 0.04823 |  0:00:07s\n",
      "epoch 70 | loss: 0.04959 |  0:00:07s\n",
      "epoch 71 | loss: 0.04893 |  0:00:08s\n",
      "epoch 72 | loss: 0.04579 |  0:00:08s\n",
      "epoch 73 | loss: 0.04298 |  0:00:08s\n",
      "epoch 74 | loss: 0.04233 |  0:00:08s\n",
      "epoch 75 | loss: 0.04845 |  0:00:08s\n",
      "epoch 76 | loss: 0.04378 |  0:00:08s\n",
      "epoch 77 | loss: 0.04429 |  0:00:08s\n",
      "epoch 78 | loss: 0.04485 |  0:00:08s\n",
      "epoch 79 | loss: 0.04259 |  0:00:08s\n",
      "epoch 80 | loss: 0.04514 |  0:00:09s\n",
      "epoch 81 | loss: 0.04181 |  0:00:09s\n",
      "epoch 82 | loss: 0.04218 |  0:00:09s\n",
      "epoch 83 | loss: 0.04056 |  0:00:09s\n",
      "epoch 84 | loss: 0.04073 |  0:00:09s\n",
      "epoch 85 | loss: 0.04347 |  0:00:09s\n",
      "epoch 86 | loss: 0.04602 |  0:00:09s\n",
      "epoch 87 | loss: 0.04343 |  0:00:09s\n",
      "epoch 88 | loss: 0.04375 |  0:00:09s\n",
      "epoch 89 | loss: 0.04082 |  0:00:10s\n",
      "epoch 90 | loss: 0.04823 |  0:00:10s\n",
      "epoch 91 | loss: 0.04217 |  0:00:10s\n",
      "epoch 92 | loss: 0.04264 |  0:00:10s\n",
      "epoch 93 | loss: 0.04838 |  0:00:10s\n",
      "epoch 94 | loss: 0.04431 |  0:00:10s\n",
      "epoch 95 | loss: 0.04374 |  0:00:10s\n",
      "epoch 96 | loss: 0.04349 |  0:00:11s\n",
      "epoch 97 | loss: 0.04289 |  0:00:11s\n",
      "epoch 98 | loss: 0.04339 |  0:00:11s\n",
      "epoch 99 | loss: 0.04119 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.06952 |  0:00:00s\n",
      "epoch 1  | loss: 0.72375 |  0:00:00s\n",
      "epoch 2  | loss: 0.59057 |  0:00:00s\n",
      "epoch 3  | loss: 0.476   |  0:00:00s\n",
      "epoch 4  | loss: 0.3765  |  0:00:00s\n",
      "epoch 5  | loss: 0.34484 |  0:00:00s\n",
      "epoch 6  | loss: 0.33531 |  0:00:00s\n",
      "epoch 7  | loss: 0.34269 |  0:00:00s\n",
      "epoch 8  | loss: 0.34432 |  0:00:01s\n",
      "epoch 9  | loss: 0.32741 |  0:00:01s\n",
      "epoch 10 | loss: 0.29217 |  0:00:01s\n",
      "epoch 11 | loss: 0.28184 |  0:00:01s\n",
      "epoch 12 | loss: 0.27411 |  0:00:01s\n",
      "epoch 13 | loss: 0.276   |  0:00:01s\n",
      "epoch 14 | loss: 0.28513 |  0:00:01s\n",
      "epoch 15 | loss: 0.2732  |  0:00:01s\n",
      "epoch 16 | loss: 0.28379 |  0:00:02s\n",
      "epoch 17 | loss: 0.2585  |  0:00:02s\n",
      "epoch 18 | loss: 0.25113 |  0:00:02s\n",
      "epoch 19 | loss: 0.24244 |  0:00:02s\n",
      "epoch 20 | loss: 0.21592 |  0:00:02s\n",
      "epoch 21 | loss: 0.20268 |  0:00:02s\n",
      "epoch 22 | loss: 0.20304 |  0:00:02s\n",
      "epoch 23 | loss: 0.17793 |  0:00:02s\n",
      "epoch 24 | loss: 0.16225 |  0:00:02s\n",
      "epoch 25 | loss: 0.16552 |  0:00:03s\n",
      "epoch 26 | loss: 0.15191 |  0:00:03s\n",
      "epoch 27 | loss: 0.15268 |  0:00:03s\n",
      "epoch 28 | loss: 0.13814 |  0:00:03s\n",
      "epoch 29 | loss: 0.13483 |  0:00:03s\n",
      "epoch 30 | loss: 0.12499 |  0:00:03s\n",
      "epoch 31 | loss: 0.11766 |  0:00:03s\n",
      "epoch 32 | loss: 0.1106  |  0:00:03s\n",
      "epoch 33 | loss: 0.11507 |  0:00:03s\n",
      "epoch 34 | loss: 0.09856 |  0:00:04s\n",
      "epoch 35 | loss: 0.10266 |  0:00:04s\n",
      "epoch 36 | loss: 0.09769 |  0:00:04s\n",
      "epoch 37 | loss: 0.08883 |  0:00:04s\n",
      "epoch 38 | loss: 0.08125 |  0:00:04s\n",
      "epoch 39 | loss: 0.06944 |  0:00:04s\n",
      "epoch 40 | loss: 0.07357 |  0:00:04s\n",
      "epoch 41 | loss: 0.06993 |  0:00:04s\n",
      "epoch 42 | loss: 0.07107 |  0:00:04s\n",
      "epoch 43 | loss: 0.06786 |  0:00:05s\n",
      "epoch 44 | loss: 0.07137 |  0:00:05s\n",
      "epoch 45 | loss: 0.06596 |  0:00:05s\n",
      "epoch 46 | loss: 0.06463 |  0:00:05s\n",
      "epoch 47 | loss: 0.06977 |  0:00:05s\n",
      "epoch 48 | loss: 0.05902 |  0:00:05s\n",
      "epoch 49 | loss: 0.06156 |  0:00:05s\n",
      "epoch 50 | loss: 0.05837 |  0:00:05s\n",
      "epoch 51 | loss: 0.05649 |  0:00:05s\n",
      "epoch 52 | loss: 0.06966 |  0:00:06s\n",
      "epoch 53 | loss: 0.06216 |  0:00:06s\n",
      "epoch 54 | loss: 0.05597 |  0:00:06s\n",
      "epoch 55 | loss: 0.05915 |  0:00:06s\n",
      "epoch 56 | loss: 0.05283 |  0:00:06s\n",
      "epoch 57 | loss: 0.05586 |  0:00:06s\n",
      "epoch 58 | loss: 0.054   |  0:00:06s\n",
      "epoch 59 | loss: 0.04939 |  0:00:06s\n",
      "epoch 60 | loss: 0.04873 |  0:00:06s\n",
      "epoch 61 | loss: 0.05093 |  0:00:07s\n",
      "epoch 62 | loss: 0.04527 |  0:00:07s\n",
      "epoch 63 | loss: 0.04106 |  0:00:07s\n",
      "epoch 64 | loss: 0.04877 |  0:00:07s\n",
      "epoch 65 | loss: 0.04217 |  0:00:07s\n",
      "epoch 66 | loss: 0.04446 |  0:00:07s\n",
      "epoch 67 | loss: 0.04322 |  0:00:07s\n",
      "epoch 68 | loss: 0.04658 |  0:00:07s\n",
      "epoch 69 | loss: 0.03905 |  0:00:08s\n",
      "epoch 70 | loss: 0.04311 |  0:00:08s\n",
      "epoch 71 | loss: 0.04178 |  0:00:08s\n",
      "epoch 72 | loss: 0.03943 |  0:00:08s\n",
      "epoch 73 | loss: 0.0388  |  0:00:08s\n",
      "epoch 74 | loss: 0.04426 |  0:00:08s\n",
      "epoch 75 | loss: 0.03903 |  0:00:08s\n",
      "epoch 76 | loss: 0.04259 |  0:00:08s\n",
      "epoch 77 | loss: 0.03764 |  0:00:09s\n",
      "epoch 78 | loss: 0.03402 |  0:00:09s\n",
      "epoch 79 | loss: 0.03719 |  0:00:09s\n",
      "epoch 80 | loss: 0.039   |  0:00:09s\n",
      "epoch 81 | loss: 0.04035 |  0:00:09s\n",
      "epoch 82 | loss: 0.0363  |  0:00:09s\n",
      "epoch 83 | loss: 0.03535 |  0:00:09s\n",
      "epoch 84 | loss: 0.04319 |  0:00:09s\n",
      "epoch 85 | loss: 0.0375  |  0:00:09s\n",
      "epoch 86 | loss: 0.03688 |  0:00:10s\n",
      "epoch 87 | loss: 0.03546 |  0:00:10s\n",
      "epoch 88 | loss: 0.03875 |  0:00:10s\n",
      "epoch 89 | loss: 0.03514 |  0:00:10s\n",
      "epoch 90 | loss: 0.03427 |  0:00:10s\n",
      "epoch 91 | loss: 0.05033 |  0:00:10s\n",
      "epoch 92 | loss: 0.03497 |  0:00:10s\n",
      "epoch 93 | loss: 0.03821 |  0:00:10s\n",
      "epoch 94 | loss: 0.03951 |  0:00:10s\n",
      "epoch 95 | loss: 0.03189 |  0:00:11s\n",
      "epoch 96 | loss: 0.0348  |  0:00:11s\n",
      "epoch 97 | loss: 0.03817 |  0:00:11s\n",
      "epoch 98 | loss: 0.03319 |  0:00:11s\n",
      "epoch 99 | loss: 0.03463 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.09747 |  0:00:00s\n",
      "epoch 1  | loss: 0.7476  |  0:00:00s\n",
      "epoch 2  | loss: 0.57023 |  0:00:00s\n",
      "epoch 3  | loss: 0.47031 |  0:00:00s\n",
      "epoch 4  | loss: 0.38972 |  0:00:00s\n",
      "epoch 5  | loss: 0.38673 |  0:00:00s\n",
      "epoch 6  | loss: 0.37913 |  0:00:00s\n",
      "epoch 7  | loss: 0.33856 |  0:00:00s\n",
      "epoch 8  | loss: 0.33308 |  0:00:01s\n",
      "epoch 9  | loss: 0.3166  |  0:00:01s\n",
      "epoch 10 | loss: 0.30659 |  0:00:01s\n",
      "epoch 11 | loss: 0.30591 |  0:00:01s\n",
      "epoch 12 | loss: 0.31922 |  0:00:01s\n",
      "epoch 13 | loss: 0.30113 |  0:00:01s\n",
      "epoch 14 | loss: 0.309   |  0:00:01s\n",
      "epoch 15 | loss: 0.29491 |  0:00:01s\n",
      "epoch 16 | loss: 0.28805 |  0:00:01s\n",
      "epoch 17 | loss: 0.27991 |  0:00:02s\n",
      "epoch 18 | loss: 0.27408 |  0:00:02s\n",
      "epoch 19 | loss: 0.25711 |  0:00:02s\n",
      "epoch 20 | loss: 0.24488 |  0:00:02s\n",
      "epoch 21 | loss: 0.22732 |  0:00:02s\n",
      "epoch 22 | loss: 0.20361 |  0:00:02s\n",
      "epoch 23 | loss: 0.19053 |  0:00:02s\n",
      "epoch 24 | loss: 0.1941  |  0:00:02s\n",
      "epoch 25 | loss: 0.17962 |  0:00:02s\n",
      "epoch 26 | loss: 0.17802 |  0:00:02s\n",
      "epoch 27 | loss: 0.16246 |  0:00:03s\n",
      "epoch 28 | loss: 0.15    |  0:00:03s\n",
      "epoch 29 | loss: 0.1345  |  0:00:03s\n",
      "epoch 30 | loss: 0.1241  |  0:00:03s\n",
      "epoch 31 | loss: 0.11345 |  0:00:03s\n",
      "epoch 32 | loss: 0.11021 |  0:00:03s\n",
      "epoch 33 | loss: 0.10383 |  0:00:03s\n",
      "epoch 34 | loss: 0.09638 |  0:00:03s\n",
      "epoch 35 | loss: 0.0977  |  0:00:04s\n",
      "epoch 36 | loss: 0.09059 |  0:00:04s\n",
      "epoch 37 | loss: 0.08704 |  0:00:04s\n",
      "epoch 38 | loss: 0.07406 |  0:00:04s\n",
      "epoch 39 | loss: 0.07432 |  0:00:04s\n",
      "epoch 40 | loss: 0.07988 |  0:00:04s\n",
      "epoch 41 | loss: 0.07224 |  0:00:04s\n",
      "epoch 42 | loss: 0.07071 |  0:00:04s\n",
      "epoch 43 | loss: 0.06795 |  0:00:05s\n",
      "epoch 44 | loss: 0.05954 |  0:00:05s\n",
      "epoch 45 | loss: 0.06621 |  0:00:05s\n",
      "epoch 46 | loss: 0.05973 |  0:00:05s\n",
      "epoch 47 | loss: 0.06231 |  0:00:05s\n",
      "epoch 48 | loss: 0.06391 |  0:00:05s\n",
      "epoch 49 | loss: 0.05526 |  0:00:05s\n",
      "epoch 50 | loss: 0.05276 |  0:00:05s\n",
      "epoch 51 | loss: 0.05306 |  0:00:06s\n",
      "epoch 52 | loss: 0.05237 |  0:00:06s\n",
      "epoch 53 | loss: 0.05499 |  0:00:06s\n",
      "epoch 54 | loss: 0.05305 |  0:00:06s\n",
      "epoch 55 | loss: 0.05379 |  0:00:06s\n",
      "epoch 56 | loss: 0.0502  |  0:00:06s\n",
      "epoch 57 | loss: 0.04935 |  0:00:06s\n",
      "epoch 58 | loss: 0.04987 |  0:00:06s\n",
      "epoch 59 | loss: 0.04706 |  0:00:06s\n",
      "epoch 60 | loss: 0.04791 |  0:00:07s\n",
      "epoch 61 | loss: 0.0464  |  0:00:07s\n",
      "epoch 62 | loss: 0.04622 |  0:00:07s\n",
      "epoch 63 | loss: 0.05532 |  0:00:07s\n",
      "epoch 64 | loss: 0.04876 |  0:00:07s\n",
      "epoch 65 | loss: 0.04674 |  0:00:07s\n",
      "epoch 66 | loss: 0.04319 |  0:00:07s\n",
      "epoch 67 | loss: 0.04677 |  0:00:07s\n",
      "epoch 68 | loss: 0.04502 |  0:00:07s\n",
      "epoch 69 | loss: 0.04376 |  0:00:08s\n",
      "epoch 70 | loss: 0.04717 |  0:00:08s\n",
      "epoch 71 | loss: 0.04296 |  0:00:08s\n",
      "epoch 72 | loss: 0.04454 |  0:00:08s\n",
      "epoch 73 | loss: 0.0433  |  0:00:08s\n",
      "epoch 74 | loss: 0.0463  |  0:00:08s\n",
      "epoch 75 | loss: 0.04511 |  0:00:08s\n",
      "epoch 76 | loss: 0.04193 |  0:00:08s\n",
      "epoch 77 | loss: 0.04361 |  0:00:08s\n",
      "epoch 78 | loss: 0.04434 |  0:00:09s\n",
      "epoch 79 | loss: 0.04313 |  0:00:09s\n",
      "epoch 80 | loss: 0.04317 |  0:00:09s\n",
      "epoch 81 | loss: 0.04241 |  0:00:09s\n",
      "epoch 82 | loss: 0.0414  |  0:00:09s\n",
      "epoch 83 | loss: 0.04089 |  0:00:09s\n",
      "epoch 84 | loss: 0.04226 |  0:00:09s\n",
      "epoch 85 | loss: 0.04232 |  0:00:09s\n",
      "epoch 86 | loss: 0.04201 |  0:00:09s\n",
      "epoch 87 | loss: 0.04229 |  0:00:10s\n",
      "epoch 88 | loss: 0.04407 |  0:00:10s\n",
      "epoch 89 | loss: 0.04185 |  0:00:10s\n",
      "epoch 90 | loss: 0.04184 |  0:00:10s\n",
      "epoch 91 | loss: 0.04267 |  0:00:10s\n",
      "epoch 92 | loss: 0.04091 |  0:00:10s\n",
      "epoch 93 | loss: 0.04288 |  0:00:10s\n",
      "epoch 94 | loss: 0.0454  |  0:00:10s\n",
      "epoch 95 | loss: 0.04098 |  0:00:10s\n",
      "epoch 96 | loss: 0.04151 |  0:00:11s\n",
      "epoch 97 | loss: 0.03991 |  0:00:11s\n",
      "epoch 98 | loss: 0.04263 |  0:00:11s\n",
      "epoch 99 | loss: 0.03909 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.00355 |  0:00:00s\n",
      "epoch 1  | loss: 0.71602 |  0:00:00s\n",
      "epoch 2  | loss: 0.5532  |  0:00:00s\n",
      "epoch 3  | loss: 0.43454 |  0:00:00s\n",
      "epoch 4  | loss: 0.35547 |  0:00:00s\n",
      "epoch 5  | loss: 0.33496 |  0:00:00s\n",
      "epoch 6  | loss: 0.32259 |  0:00:00s\n",
      "epoch 7  | loss: 0.31773 |  0:00:00s\n",
      "epoch 8  | loss: 0.30591 |  0:00:01s\n",
      "epoch 9  | loss: 0.27674 |  0:00:01s\n",
      "epoch 10 | loss: 0.27099 |  0:00:01s\n",
      "epoch 11 | loss: 0.27772 |  0:00:01s\n",
      "epoch 12 | loss: 0.25311 |  0:00:01s\n",
      "epoch 13 | loss: 0.25699 |  0:00:01s\n",
      "epoch 14 | loss: 0.26915 |  0:00:01s\n",
      "epoch 15 | loss: 0.23653 |  0:00:01s\n",
      "epoch 16 | loss: 0.21366 |  0:00:01s\n",
      "epoch 17 | loss: 0.20343 |  0:00:02s\n",
      "epoch 18 | loss: 0.18471 |  0:00:02s\n",
      "epoch 19 | loss: 0.18038 |  0:00:02s\n",
      "epoch 20 | loss: 0.21035 |  0:00:02s\n",
      "epoch 21 | loss: 0.20617 |  0:00:02s\n",
      "epoch 22 | loss: 0.20779 |  0:00:02s\n",
      "epoch 23 | loss: 0.19801 |  0:00:02s\n",
      "epoch 24 | loss: 0.19075 |  0:00:02s\n",
      "epoch 25 | loss: 0.18347 |  0:00:02s\n",
      "epoch 26 | loss: 0.19092 |  0:00:03s\n",
      "epoch 27 | loss: 0.15964 |  0:00:03s\n",
      "epoch 28 | loss: 0.17754 |  0:00:03s\n",
      "epoch 29 | loss: 0.16003 |  0:00:03s\n",
      "epoch 30 | loss: 0.13926 |  0:00:03s\n",
      "epoch 31 | loss: 0.13749 |  0:00:03s\n",
      "epoch 32 | loss: 0.1198  |  0:00:03s\n",
      "epoch 33 | loss: 0.12775 |  0:00:03s\n",
      "epoch 34 | loss: 0.11371 |  0:00:03s\n",
      "epoch 35 | loss: 0.11079 |  0:00:04s\n",
      "epoch 36 | loss: 0.08592 |  0:00:04s\n",
      "epoch 37 | loss: 0.08741 |  0:00:04s\n",
      "epoch 38 | loss: 0.09881 |  0:00:04s\n",
      "epoch 39 | loss: 0.08439 |  0:00:04s\n",
      "epoch 40 | loss: 0.08446 |  0:00:04s\n",
      "epoch 41 | loss: 0.09128 |  0:00:04s\n",
      "epoch 42 | loss: 0.07381 |  0:00:04s\n",
      "epoch 43 | loss: 0.07728 |  0:00:04s\n",
      "epoch 44 | loss: 0.07054 |  0:00:05s\n",
      "epoch 45 | loss: 0.07504 |  0:00:05s\n",
      "epoch 46 | loss: 0.06882 |  0:00:05s\n",
      "epoch 47 | loss: 0.0686  |  0:00:05s\n",
      "epoch 48 | loss: 0.06562 |  0:00:05s\n",
      "epoch 49 | loss: 0.0682  |  0:00:05s\n",
      "epoch 50 | loss: 0.0702  |  0:00:05s\n",
      "epoch 51 | loss: 0.06918 |  0:00:05s\n",
      "epoch 52 | loss: 0.06429 |  0:00:05s\n",
      "epoch 53 | loss: 0.06223 |  0:00:06s\n",
      "epoch 54 | loss: 0.06466 |  0:00:06s\n",
      "epoch 55 | loss: 0.06155 |  0:00:06s\n",
      "epoch 56 | loss: 0.05932 |  0:00:06s\n",
      "epoch 57 | loss: 0.06162 |  0:00:06s\n",
      "epoch 58 | loss: 0.06394 |  0:00:06s\n",
      "epoch 59 | loss: 0.06556 |  0:00:06s\n",
      "epoch 60 | loss: 0.06068 |  0:00:06s\n",
      "epoch 61 | loss: 0.0532  |  0:00:06s\n",
      "epoch 62 | loss: 0.05964 |  0:00:07s\n",
      "epoch 63 | loss: 0.05614 |  0:00:07s\n",
      "epoch 64 | loss: 0.06907 |  0:00:07s\n",
      "epoch 65 | loss: 0.06144 |  0:00:07s\n",
      "epoch 66 | loss: 0.05467 |  0:00:07s\n",
      "epoch 67 | loss: 0.06276 |  0:00:07s\n",
      "epoch 68 | loss: 0.05951 |  0:00:07s\n",
      "epoch 69 | loss: 0.05645 |  0:00:07s\n",
      "epoch 70 | loss: 0.05954 |  0:00:07s\n",
      "epoch 71 | loss: 0.05618 |  0:00:08s\n",
      "epoch 72 | loss: 0.05401 |  0:00:08s\n",
      "epoch 73 | loss: 0.05506 |  0:00:08s\n",
      "epoch 74 | loss: 0.05471 |  0:00:08s\n",
      "epoch 75 | loss: 0.05269 |  0:00:08s\n",
      "epoch 76 | loss: 0.05017 |  0:00:08s\n",
      "epoch 77 | loss: 0.05215 |  0:00:08s\n",
      "epoch 78 | loss: 0.05162 |  0:00:08s\n",
      "epoch 79 | loss: 0.05251 |  0:00:08s\n",
      "epoch 80 | loss: 0.04896 |  0:00:09s\n",
      "epoch 81 | loss: 0.05123 |  0:00:09s\n",
      "epoch 82 | loss: 0.0511  |  0:00:09s\n",
      "epoch 83 | loss: 0.04963 |  0:00:09s\n",
      "epoch 84 | loss: 0.05274 |  0:00:09s\n",
      "epoch 85 | loss: 0.05072 |  0:00:09s\n",
      "epoch 86 | loss: 0.05483 |  0:00:09s\n",
      "epoch 87 | loss: 0.05018 |  0:00:09s\n",
      "epoch 88 | loss: 0.05154 |  0:00:09s\n",
      "epoch 89 | loss: 0.05032 |  0:00:10s\n",
      "epoch 90 | loss: 0.04879 |  0:00:10s\n",
      "epoch 91 | loss: 0.0486  |  0:00:10s\n",
      "epoch 92 | loss: 0.04964 |  0:00:10s\n",
      "epoch 93 | loss: 0.0491  |  0:00:10s\n",
      "epoch 94 | loss: 0.04949 |  0:00:10s\n",
      "epoch 95 | loss: 0.04941 |  0:00:10s\n",
      "epoch 96 | loss: 0.04782 |  0:00:10s\n",
      "epoch 97 | loss: 0.04879 |  0:00:10s\n",
      "epoch 98 | loss: 0.05166 |  0:00:11s\n",
      "epoch 99 | loss: 0.0489  |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.07515 |  0:00:00s\n",
      "epoch 1  | loss: 0.72398 |  0:00:00s\n",
      "epoch 2  | loss: 0.52986 |  0:00:00s\n",
      "epoch 3  | loss: 0.42436 |  0:00:00s\n",
      "epoch 4  | loss: 0.37454 |  0:00:00s\n",
      "epoch 5  | loss: 0.35796 |  0:00:00s\n",
      "epoch 6  | loss: 0.35627 |  0:00:00s\n",
      "epoch 7  | loss: 0.34418 |  0:00:00s\n",
      "epoch 8  | loss: 0.32824 |  0:00:00s\n",
      "epoch 9  | loss: 0.32014 |  0:00:01s\n",
      "epoch 10 | loss: 0.3293  |  0:00:01s\n",
      "epoch 11 | loss: 0.30931 |  0:00:01s\n",
      "epoch 12 | loss: 0.30769 |  0:00:01s\n",
      "epoch 13 | loss: 0.31237 |  0:00:01s\n",
      "epoch 14 | loss: 0.29699 |  0:00:01s\n",
      "epoch 15 | loss: 0.30162 |  0:00:01s\n",
      "epoch 16 | loss: 0.29937 |  0:00:01s\n",
      "epoch 17 | loss: 0.2972  |  0:00:02s\n",
      "epoch 18 | loss: 0.29573 |  0:00:02s\n",
      "epoch 19 | loss: 0.2976  |  0:00:02s\n",
      "epoch 20 | loss: 0.29053 |  0:00:02s\n",
      "epoch 21 | loss: 0.28504 |  0:00:02s\n",
      "epoch 22 | loss: 0.27428 |  0:00:02s\n",
      "epoch 23 | loss: 0.26385 |  0:00:02s\n",
      "epoch 24 | loss: 0.25064 |  0:00:02s\n",
      "epoch 25 | loss: 0.23243 |  0:00:02s\n",
      "epoch 26 | loss: 0.21553 |  0:00:03s\n",
      "epoch 27 | loss: 0.1981  |  0:00:03s\n",
      "epoch 28 | loss: 0.16986 |  0:00:03s\n",
      "epoch 29 | loss: 0.14958 |  0:00:03s\n",
      "epoch 30 | loss: 0.13195 |  0:00:03s\n",
      "epoch 31 | loss: 0.11957 |  0:00:03s\n",
      "epoch 32 | loss: 0.10534 |  0:00:03s\n",
      "epoch 33 | loss: 0.08979 |  0:00:03s\n",
      "epoch 34 | loss: 0.08564 |  0:00:03s\n",
      "epoch 35 | loss: 0.07757 |  0:00:04s\n",
      "epoch 36 | loss: 0.06432 |  0:00:04s\n",
      "epoch 37 | loss: 0.05719 |  0:00:04s\n",
      "epoch 38 | loss: 0.05706 |  0:00:04s\n",
      "epoch 39 | loss: 0.05258 |  0:00:04s\n",
      "epoch 40 | loss: 0.05358 |  0:00:04s\n",
      "epoch 41 | loss: 0.05416 |  0:00:04s\n",
      "epoch 42 | loss: 0.05318 |  0:00:04s\n",
      "epoch 43 | loss: 0.05029 |  0:00:04s\n",
      "epoch 44 | loss: 0.05467 |  0:00:05s\n",
      "epoch 45 | loss: 0.04728 |  0:00:05s\n",
      "epoch 46 | loss: 0.04989 |  0:00:05s\n",
      "epoch 47 | loss: 0.04623 |  0:00:05s\n",
      "epoch 48 | loss: 0.04761 |  0:00:05s\n",
      "epoch 49 | loss: 0.04831 |  0:00:05s\n",
      "epoch 50 | loss: 0.04397 |  0:00:05s\n",
      "epoch 51 | loss: 0.03996 |  0:00:05s\n",
      "epoch 52 | loss: 0.04231 |  0:00:05s\n",
      "epoch 53 | loss: 0.04256 |  0:00:06s\n",
      "epoch 54 | loss: 0.03995 |  0:00:06s\n",
      "epoch 55 | loss: 0.0428  |  0:00:06s\n",
      "epoch 56 | loss: 0.03817 |  0:00:06s\n",
      "epoch 57 | loss: 0.03817 |  0:00:06s\n",
      "epoch 58 | loss: 0.04276 |  0:00:06s\n",
      "epoch 59 | loss: 0.03833 |  0:00:06s\n",
      "epoch 60 | loss: 0.03887 |  0:00:06s\n",
      "epoch 61 | loss: 0.03819 |  0:00:06s\n",
      "epoch 62 | loss: 0.03812 |  0:00:07s\n",
      "epoch 63 | loss: 0.03698 |  0:00:07s\n",
      "epoch 64 | loss: 0.04064 |  0:00:07s\n",
      "epoch 65 | loss: 0.03825 |  0:00:07s\n",
      "epoch 66 | loss: 0.03528 |  0:00:07s\n",
      "epoch 67 | loss: 0.04467 |  0:00:07s\n",
      "epoch 68 | loss: 0.04095 |  0:00:07s\n",
      "epoch 69 | loss: 0.03915 |  0:00:07s\n",
      "epoch 70 | loss: 0.03853 |  0:00:07s\n",
      "epoch 71 | loss: 0.03586 |  0:00:08s\n",
      "epoch 72 | loss: 0.03972 |  0:00:08s\n",
      "epoch 73 | loss: 0.03861 |  0:00:08s\n",
      "epoch 74 | loss: 0.03843 |  0:00:08s\n",
      "epoch 75 | loss: 0.04166 |  0:00:08s\n",
      "epoch 76 | loss: 0.038   |  0:00:08s\n",
      "epoch 77 | loss: 0.0562  |  0:00:08s\n",
      "epoch 78 | loss: 0.03677 |  0:00:08s\n",
      "epoch 79 | loss: 0.04115 |  0:00:08s\n",
      "epoch 80 | loss: 0.04228 |  0:00:09s\n",
      "epoch 81 | loss: 0.04013 |  0:00:09s\n",
      "epoch 82 | loss: 0.03645 |  0:00:09s\n",
      "epoch 83 | loss: 0.03683 |  0:00:09s\n",
      "epoch 84 | loss: 0.03726 |  0:00:09s\n",
      "epoch 85 | loss: 0.03749 |  0:00:09s\n",
      "epoch 86 | loss: 0.03811 |  0:00:09s\n",
      "epoch 87 | loss: 0.03467 |  0:00:09s\n",
      "epoch 88 | loss: 0.03539 |  0:00:09s\n",
      "epoch 89 | loss: 0.03394 |  0:00:10s\n",
      "epoch 90 | loss: 0.03627 |  0:00:10s\n",
      "epoch 91 | loss: 0.03574 |  0:00:10s\n",
      "epoch 92 | loss: 0.03316 |  0:00:10s\n",
      "epoch 93 | loss: 0.03331 |  0:00:10s\n",
      "epoch 94 | loss: 0.03544 |  0:00:10s\n",
      "epoch 95 | loss: 0.03598 |  0:00:10s\n",
      "epoch 96 | loss: 0.03194 |  0:00:10s\n",
      "epoch 97 | loss: 0.03387 |  0:00:10s\n",
      "epoch 98 | loss: 0.03452 |  0:00:11s\n",
      "epoch 99 | loss: 0.03569 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.0916  |  0:00:00s\n",
      "epoch 1  | loss: 0.76445 |  0:00:00s\n",
      "epoch 2  | loss: 0.5835  |  0:00:00s\n",
      "epoch 3  | loss: 0.44559 |  0:00:00s\n",
      "epoch 4  | loss: 0.39369 |  0:00:00s\n",
      "epoch 5  | loss: 0.3565  |  0:00:00s\n",
      "epoch 6  | loss: 0.37857 |  0:00:00s\n",
      "epoch 7  | loss: 0.35244 |  0:00:00s\n",
      "epoch 8  | loss: 0.32216 |  0:00:01s\n",
      "epoch 9  | loss: 0.31545 |  0:00:01s\n",
      "epoch 10 | loss: 0.28907 |  0:00:01s\n",
      "epoch 11 | loss: 0.30492 |  0:00:01s\n",
      "epoch 12 | loss: 0.29393 |  0:00:01s\n",
      "epoch 13 | loss: 0.28289 |  0:00:01s\n",
      "epoch 14 | loss: 0.2711  |  0:00:01s\n",
      "epoch 15 | loss: 0.26571 |  0:00:01s\n",
      "epoch 16 | loss: 0.25814 |  0:00:01s\n",
      "epoch 17 | loss: 0.24222 |  0:00:01s\n",
      "epoch 18 | loss: 0.24188 |  0:00:02s\n",
      "epoch 19 | loss: 0.22471 |  0:00:02s\n",
      "epoch 20 | loss: 0.20311 |  0:00:02s\n",
      "epoch 21 | loss: 0.18453 |  0:00:02s\n",
      "epoch 22 | loss: 0.1716  |  0:00:02s\n",
      "epoch 23 | loss: 0.13971 |  0:00:02s\n",
      "epoch 24 | loss: 0.11786 |  0:00:02s\n",
      "epoch 25 | loss: 0.11898 |  0:00:02s\n",
      "epoch 26 | loss: 0.10997 |  0:00:02s\n",
      "epoch 27 | loss: 0.11296 |  0:00:03s\n",
      "epoch 28 | loss: 0.10136 |  0:00:03s\n",
      "epoch 29 | loss: 0.09174 |  0:00:03s\n",
      "epoch 30 | loss: 0.08738 |  0:00:03s\n",
      "epoch 31 | loss: 0.08436 |  0:00:03s\n",
      "epoch 32 | loss: 0.09071 |  0:00:03s\n",
      "epoch 33 | loss: 0.08537 |  0:00:03s\n",
      "epoch 34 | loss: 0.08095 |  0:00:03s\n",
      "epoch 35 | loss: 0.0831  |  0:00:03s\n",
      "epoch 36 | loss: 0.07732 |  0:00:04s\n",
      "epoch 37 | loss: 0.07493 |  0:00:04s\n",
      "epoch 38 | loss: 0.07442 |  0:00:04s\n",
      "epoch 39 | loss: 0.07771 |  0:00:04s\n",
      "epoch 40 | loss: 0.07487 |  0:00:04s\n",
      "epoch 41 | loss: 0.0754  |  0:00:04s\n",
      "epoch 42 | loss: 0.07131 |  0:00:04s\n",
      "epoch 43 | loss: 0.06778 |  0:00:04s\n",
      "epoch 44 | loss: 0.06456 |  0:00:04s\n",
      "epoch 45 | loss: 0.066   |  0:00:05s\n",
      "epoch 46 | loss: 0.05851 |  0:00:05s\n",
      "epoch 47 | loss: 0.06225 |  0:00:05s\n",
      "epoch 48 | loss: 0.06074 |  0:00:05s\n",
      "epoch 49 | loss: 0.05215 |  0:00:05s\n",
      "epoch 50 | loss: 0.0596  |  0:00:05s\n",
      "epoch 51 | loss: 0.04997 |  0:00:05s\n",
      "epoch 52 | loss: 0.0563  |  0:00:05s\n",
      "epoch 53 | loss: 0.05153 |  0:00:05s\n",
      "epoch 54 | loss: 0.05667 |  0:00:06s\n",
      "epoch 55 | loss: 0.0506  |  0:00:06s\n",
      "epoch 56 | loss: 0.05151 |  0:00:06s\n",
      "epoch 57 | loss: 0.05011 |  0:00:06s\n",
      "epoch 58 | loss: 0.05294 |  0:00:06s\n",
      "epoch 59 | loss: 0.05937 |  0:00:06s\n",
      "epoch 60 | loss: 0.05405 |  0:00:06s\n",
      "epoch 61 | loss: 0.052   |  0:00:06s\n",
      "epoch 62 | loss: 0.0506  |  0:00:06s\n",
      "epoch 63 | loss: 0.05466 |  0:00:07s\n",
      "epoch 64 | loss: 0.06056 |  0:00:07s\n",
      "epoch 65 | loss: 0.05839 |  0:00:07s\n",
      "epoch 66 | loss: 0.05585 |  0:00:07s\n",
      "epoch 67 | loss: 0.05885 |  0:00:07s\n",
      "epoch 68 | loss: 0.05838 |  0:00:07s\n",
      "epoch 69 | loss: 0.04604 |  0:00:07s\n",
      "epoch 70 | loss: 0.05516 |  0:00:07s\n",
      "epoch 71 | loss: 0.05091 |  0:00:07s\n",
      "epoch 72 | loss: 0.04974 |  0:00:08s\n",
      "epoch 73 | loss: 0.0466  |  0:00:08s\n",
      "epoch 74 | loss: 0.04889 |  0:00:08s\n",
      "epoch 75 | loss: 0.04894 |  0:00:08s\n",
      "epoch 76 | loss: 0.0443  |  0:00:08s\n",
      "epoch 77 | loss: 0.05539 |  0:00:08s\n",
      "epoch 78 | loss: 0.04887 |  0:00:08s\n",
      "epoch 79 | loss: 0.05258 |  0:00:08s\n",
      "epoch 80 | loss: 0.05737 |  0:00:08s\n",
      "epoch 81 | loss: 0.05587 |  0:00:09s\n",
      "epoch 82 | loss: 0.05072 |  0:00:09s\n",
      "epoch 83 | loss: 0.05263 |  0:00:09s\n",
      "epoch 84 | loss: 0.05051 |  0:00:09s\n",
      "epoch 85 | loss: 0.06239 |  0:00:09s\n",
      "epoch 86 | loss: 0.05281 |  0:00:09s\n",
      "epoch 87 | loss: 0.05163 |  0:00:09s\n",
      "epoch 88 | loss: 0.04979 |  0:00:09s\n",
      "epoch 89 | loss: 0.04402 |  0:00:09s\n",
      "epoch 90 | loss: 0.04928 |  0:00:10s\n",
      "epoch 91 | loss: 0.04781 |  0:00:10s\n",
      "epoch 92 | loss: 0.04824 |  0:00:10s\n",
      "epoch 93 | loss: 0.04464 |  0:00:10s\n",
      "epoch 94 | loss: 0.05065 |  0:00:10s\n",
      "epoch 95 | loss: 0.04856 |  0:00:10s\n",
      "epoch 96 | loss: 0.04789 |  0:00:10s\n",
      "epoch 97 | loss: 0.05169 |  0:00:10s\n",
      "epoch 98 | loss: 0.04708 |  0:00:10s\n",
      "epoch 99 | loss: 0.04964 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.04206 |  0:00:00s\n",
      "epoch 1  | loss: 0.82361 |  0:00:00s\n",
      "epoch 2  | loss: 0.61918 |  0:00:00s\n",
      "epoch 3  | loss: 0.47768 |  0:00:00s\n",
      "epoch 4  | loss: 0.37387 |  0:00:00s\n",
      "epoch 5  | loss: 0.35544 |  0:00:00s\n",
      "epoch 6  | loss: 0.3458  |  0:00:00s\n",
      "epoch 7  | loss: 0.35079 |  0:00:00s\n",
      "epoch 8  | loss: 0.33898 |  0:00:00s\n",
      "epoch 9  | loss: 0.32691 |  0:00:01s\n",
      "epoch 10 | loss: 0.31488 |  0:00:01s\n",
      "epoch 11 | loss: 0.30241 |  0:00:01s\n",
      "epoch 12 | loss: 0.30181 |  0:00:01s\n",
      "epoch 13 | loss: 0.28407 |  0:00:01s\n",
      "epoch 14 | loss: 0.28566 |  0:00:01s\n",
      "epoch 15 | loss: 0.27677 |  0:00:01s\n",
      "epoch 16 | loss: 0.26977 |  0:00:01s\n",
      "epoch 17 | loss: 0.24893 |  0:00:01s\n",
      "epoch 18 | loss: 0.23866 |  0:00:02s\n",
      "epoch 19 | loss: 0.23617 |  0:00:02s\n",
      "epoch 20 | loss: 0.21726 |  0:00:02s\n",
      "epoch 21 | loss: 0.19974 |  0:00:02s\n",
      "epoch 22 | loss: 0.18995 |  0:00:02s\n",
      "epoch 23 | loss: 0.17206 |  0:00:02s\n",
      "epoch 24 | loss: 0.15697 |  0:00:02s\n",
      "epoch 25 | loss: 0.15486 |  0:00:02s\n",
      "epoch 26 | loss: 0.12395 |  0:00:02s\n",
      "epoch 27 | loss: 0.13126 |  0:00:03s\n",
      "epoch 28 | loss: 0.11336 |  0:00:03s\n",
      "epoch 29 | loss: 0.10467 |  0:00:03s\n",
      "epoch 30 | loss: 0.09009 |  0:00:03s\n",
      "epoch 31 | loss: 0.0785  |  0:00:03s\n",
      "epoch 32 | loss: 0.06666 |  0:00:03s\n",
      "epoch 33 | loss: 0.06426 |  0:00:03s\n",
      "epoch 34 | loss: 0.06111 |  0:00:03s\n",
      "epoch 35 | loss: 0.06268 |  0:00:03s\n",
      "epoch 36 | loss: 0.04684 |  0:00:04s\n",
      "epoch 37 | loss: 0.04891 |  0:00:04s\n",
      "epoch 38 | loss: 0.04316 |  0:00:04s\n",
      "epoch 39 | loss: 0.05138 |  0:00:04s\n",
      "epoch 40 | loss: 0.04077 |  0:00:04s\n",
      "epoch 41 | loss: 0.05079 |  0:00:04s\n",
      "epoch 42 | loss: 0.04482 |  0:00:04s\n",
      "epoch 43 | loss: 0.04045 |  0:00:04s\n",
      "epoch 44 | loss: 0.04055 |  0:00:04s\n",
      "epoch 45 | loss: 0.0414  |  0:00:05s\n",
      "epoch 46 | loss: 0.04231 |  0:00:05s\n",
      "epoch 47 | loss: 0.04153 |  0:00:05s\n",
      "epoch 48 | loss: 0.04242 |  0:00:05s\n",
      "epoch 49 | loss: 0.04043 |  0:00:05s\n",
      "epoch 50 | loss: 0.04504 |  0:00:05s\n",
      "epoch 51 | loss: 0.03815 |  0:00:05s\n",
      "epoch 52 | loss: 0.04293 |  0:00:05s\n",
      "epoch 53 | loss: 0.04189 |  0:00:05s\n",
      "epoch 54 | loss: 0.04467 |  0:00:06s\n",
      "epoch 55 | loss: 0.03841 |  0:00:06s\n",
      "epoch 56 | loss: 0.03512 |  0:00:06s\n",
      "epoch 57 | loss: 0.04261 |  0:00:06s\n",
      "epoch 58 | loss: 0.04001 |  0:00:06s\n",
      "epoch 59 | loss: 0.03938 |  0:00:06s\n",
      "epoch 60 | loss: 0.03792 |  0:00:06s\n",
      "epoch 61 | loss: 0.03718 |  0:00:06s\n",
      "epoch 62 | loss: 0.03448 |  0:00:06s\n",
      "epoch 63 | loss: 0.04019 |  0:00:07s\n",
      "epoch 64 | loss: 0.03877 |  0:00:07s\n",
      "epoch 65 | loss: 0.03738 |  0:00:07s\n",
      "epoch 66 | loss: 0.03671 |  0:00:07s\n",
      "epoch 67 | loss: 0.03798 |  0:00:07s\n",
      "epoch 68 | loss: 0.03982 |  0:00:07s\n",
      "epoch 69 | loss: 0.03387 |  0:00:07s\n",
      "epoch 70 | loss: 0.03853 |  0:00:07s\n",
      "epoch 71 | loss: 0.03732 |  0:00:07s\n",
      "epoch 72 | loss: 0.03541 |  0:00:08s\n",
      "epoch 73 | loss: 0.03862 |  0:00:08s\n",
      "epoch 74 | loss: 0.0361  |  0:00:08s\n",
      "epoch 75 | loss: 0.03567 |  0:00:08s\n",
      "epoch 76 | loss: 0.03678 |  0:00:08s\n",
      "epoch 77 | loss: 0.0362  |  0:00:08s\n",
      "epoch 78 | loss: 0.03807 |  0:00:08s\n",
      "epoch 79 | loss: 0.03377 |  0:00:08s\n",
      "epoch 80 | loss: 0.03613 |  0:00:09s\n",
      "epoch 81 | loss: 0.03776 |  0:00:09s\n",
      "epoch 82 | loss: 0.03755 |  0:00:09s\n",
      "epoch 83 | loss: 0.0362  |  0:00:09s\n",
      "epoch 84 | loss: 0.03639 |  0:00:09s\n",
      "epoch 85 | loss: 0.03422 |  0:00:09s\n",
      "epoch 86 | loss: 0.039   |  0:00:09s\n",
      "epoch 87 | loss: 0.03665 |  0:00:09s\n",
      "epoch 88 | loss: 0.03317 |  0:00:09s\n",
      "epoch 89 | loss: 0.0353  |  0:00:10s\n",
      "epoch 90 | loss: 0.03395 |  0:00:10s\n",
      "epoch 91 | loss: 0.03581 |  0:00:10s\n",
      "epoch 92 | loss: 0.03693 |  0:00:10s\n",
      "epoch 93 | loss: 0.03599 |  0:00:10s\n",
      "epoch 94 | loss: 0.03266 |  0:00:10s\n",
      "epoch 95 | loss: 0.03184 |  0:00:10s\n",
      "epoch 96 | loss: 0.03244 |  0:00:10s\n",
      "epoch 97 | loss: 0.03407 |  0:00:10s\n",
      "epoch 98 | loss: 0.03814 |  0:00:11s\n",
      "epoch 99 | loss: 0.03024 |  0:00:11s\n",
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.06368 |  0:00:00s\n",
      "epoch 1  | loss: 0.72459 |  0:00:00s\n",
      "epoch 2  | loss: 0.55533 |  0:00:00s\n",
      "epoch 3  | loss: 0.43225 |  0:00:00s\n",
      "epoch 4  | loss: 0.36409 |  0:00:00s\n",
      "epoch 5  | loss: 0.31976 |  0:00:00s\n",
      "epoch 6  | loss: 0.29391 |  0:00:00s\n",
      "epoch 7  | loss: 0.28506 |  0:00:00s\n",
      "epoch 8  | loss: 0.27836 |  0:00:01s\n",
      "epoch 9  | loss: 0.26045 |  0:00:01s\n",
      "epoch 10 | loss: 0.26235 |  0:00:01s\n",
      "epoch 11 | loss: 0.25723 |  0:00:01s\n",
      "epoch 12 | loss: 0.26211 |  0:00:01s\n",
      "epoch 13 | loss: 0.2496  |  0:00:01s\n",
      "epoch 14 | loss: 0.24878 |  0:00:01s\n",
      "epoch 15 | loss: 0.2415  |  0:00:01s\n",
      "epoch 16 | loss: 0.23256 |  0:00:01s\n",
      "epoch 17 | loss: 0.22516 |  0:00:02s\n",
      "epoch 18 | loss: 0.22253 |  0:00:02s\n",
      "epoch 19 | loss: 0.22056 |  0:00:02s\n",
      "epoch 20 | loss: 0.21512 |  0:00:02s\n",
      "epoch 21 | loss: 0.19035 |  0:00:02s\n",
      "epoch 22 | loss: 0.1874  |  0:00:02s\n",
      "epoch 23 | loss: 0.17011 |  0:00:02s\n",
      "epoch 24 | loss: 0.15704 |  0:00:02s\n",
      "epoch 25 | loss: 0.14762 |  0:00:02s\n",
      "epoch 26 | loss: 0.13496 |  0:00:03s\n",
      "epoch 27 | loss: 0.13047 |  0:00:03s\n",
      "epoch 28 | loss: 0.11357 |  0:00:03s\n",
      "epoch 29 | loss: 0.10606 |  0:00:03s\n",
      "epoch 30 | loss: 0.09616 |  0:00:03s\n",
      "epoch 31 | loss: 0.07941 |  0:00:03s\n",
      "epoch 32 | loss: 0.07649 |  0:00:03s\n",
      "epoch 33 | loss: 0.06943 |  0:00:03s\n",
      "epoch 34 | loss: 0.0658  |  0:00:03s\n",
      "epoch 35 | loss: 0.09523 |  0:00:04s\n",
      "epoch 36 | loss: 0.08582 |  0:00:04s\n",
      "epoch 37 | loss: 0.05632 |  0:00:04s\n",
      "epoch 38 | loss: 0.05457 |  0:00:04s\n",
      "epoch 39 | loss: 0.0516  |  0:00:04s\n",
      "epoch 40 | loss: 0.05337 |  0:00:04s\n",
      "epoch 41 | loss: 0.0517  |  0:00:04s\n",
      "epoch 42 | loss: 0.04523 |  0:00:04s\n",
      "epoch 43 | loss: 0.0559  |  0:00:04s\n",
      "epoch 44 | loss: 0.04317 |  0:00:05s\n",
      "epoch 45 | loss: 0.04448 |  0:00:05s\n",
      "epoch 46 | loss: 0.0461  |  0:00:05s\n",
      "epoch 47 | loss: 0.0427  |  0:00:05s\n",
      "epoch 48 | loss: 0.04656 |  0:00:05s\n",
      "epoch 49 | loss: 0.04306 |  0:00:05s\n",
      "epoch 50 | loss: 0.04349 |  0:00:05s\n",
      "epoch 51 | loss: 0.0475  |  0:00:05s\n",
      "epoch 52 | loss: 0.04952 |  0:00:05s\n",
      "epoch 53 | loss: 0.04306 |  0:00:06s\n",
      "epoch 54 | loss: 0.04567 |  0:00:06s\n",
      "epoch 55 | loss: 0.03888 |  0:00:06s\n",
      "epoch 56 | loss: 0.03814 |  0:00:06s\n",
      "epoch 57 | loss: 0.04439 |  0:00:06s\n",
      "epoch 58 | loss: 0.04283 |  0:00:06s\n",
      "epoch 59 | loss: 0.03968 |  0:00:06s\n",
      "epoch 60 | loss: 0.03777 |  0:00:06s\n",
      "epoch 61 | loss: 0.03853 |  0:00:06s\n",
      "epoch 62 | loss: 0.03595 |  0:00:07s\n",
      "epoch 63 | loss: 0.03466 |  0:00:07s\n",
      "epoch 64 | loss: 0.03584 |  0:00:07s\n",
      "epoch 65 | loss: 0.03552 |  0:00:07s\n",
      "epoch 66 | loss: 0.03436 |  0:00:07s\n",
      "epoch 67 | loss: 0.03543 |  0:00:07s\n",
      "epoch 68 | loss: 0.03631 |  0:00:07s\n",
      "epoch 69 | loss: 0.03575 |  0:00:07s\n",
      "epoch 70 | loss: 0.03531 |  0:00:07s\n",
      "epoch 71 | loss: 0.03436 |  0:00:08s\n",
      "epoch 72 | loss: 0.03568 |  0:00:08s\n",
      "epoch 73 | loss: 0.03386 |  0:00:08s\n",
      "epoch 74 | loss: 0.03645 |  0:00:08s\n",
      "epoch 75 | loss: 0.03335 |  0:00:08s\n",
      "epoch 76 | loss: 0.03368 |  0:00:08s\n",
      "epoch 77 | loss: 0.03416 |  0:00:08s\n",
      "epoch 78 | loss: 0.0331  |  0:00:08s\n",
      "epoch 79 | loss: 0.03249 |  0:00:08s\n",
      "epoch 80 | loss: 0.03209 |  0:00:09s\n",
      "epoch 81 | loss: 0.03621 |  0:00:09s\n",
      "epoch 82 | loss: 0.03249 |  0:00:09s\n",
      "epoch 83 | loss: 0.03355 |  0:00:09s\n",
      "epoch 84 | loss: 0.03413 |  0:00:09s\n",
      "epoch 85 | loss: 0.03151 |  0:00:09s\n",
      "epoch 86 | loss: 0.03352 |  0:00:09s\n",
      "epoch 87 | loss: 0.03445 |  0:00:09s\n",
      "epoch 88 | loss: 0.0341  |  0:00:10s\n",
      "epoch 89 | loss: 0.03203 |  0:00:10s\n",
      "epoch 90 | loss: 0.03423 |  0:00:10s\n",
      "epoch 91 | loss: 0.03295 |  0:00:10s\n",
      "epoch 92 | loss: 0.03255 |  0:00:10s\n",
      "epoch 93 | loss: 0.03203 |  0:00:10s\n",
      "epoch 94 | loss: 0.03179 |  0:00:10s\n",
      "epoch 95 | loss: 0.0318  |  0:00:10s\n",
      "epoch 96 | loss: 0.03257 |  0:00:10s\n",
      "epoch 97 | loss: 0.03112 |  0:00:10s\n",
      "epoch 98 | loss: 0.03165 |  0:00:11s\n",
      "epoch 99 | loss: 0.03308 |  0:00:11s\n"
     ]
    }
   ],
   "source": [
    "# example of evaluating a decision tree with random undersampling\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "# define dataset\n",
    "#X, y = make_classification(n_samples=10000, weights=[0.99], flip_y=0)\n",
    "# define pipeline\n",
    "steps = [('under', RandomUnderSampler(sampling_strategy=0.1)), ('model', model)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# evaluate pipeline\n",
    "scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'roc_auc']\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "scores = cross_validate(pipeline, df_std, malwareColTrain, scoring=scoring, cv=cv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T02:21:33.685121Z",
     "iopub.status.busy": "2023-03-04T02:21:33.684355Z",
     "iopub.status.idle": "2023-03-04T02:21:33.695346Z",
     "shell.execute_reply": "2023-03-04T02:21:33.693805Z",
     "shell.execute_reply.started": "2023-03-04T02:21:33.685121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7755 accuracy\n",
      "0.8022 recall\n",
      "0.7716 f1 score\n",
      "0.9434 roc auc\n",
      "116.01409 training time\n",
      "0.93435 predition time\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.4f accuracy\" % (scores['test_precision_macro'].mean()))\n",
    "print(\"%0.4f recall\" % (scores['test_recall_macro'].mean()))\n",
    "print(\"%0.4f f1 score\" % (scores['test_f1_macro'].mean()))\n",
    "print(\"%0.4f roc auc\" % (scores['test_roc_auc'].mean()))\n",
    "\n",
    "print(\"%0.5f training time\" % (scores['fit_time'].sum()))\n",
    "print(\"%0.5f predition time\" % (scores['score_time'].sum()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
